{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7266c47e",
   "metadata": {},
   "source": [
    "# Classificazione di Aspect e Opinion con ModernBERT su Laptop-ACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie caricate.\n"
     ]
    }
   ],
   "source": [
    "# Import delle librerie necessarie\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from torch.amp import autocast, GradScaler # Per Mixed Precision\n",
    "\n",
    "print(\"Librerie caricate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70bed1",
   "metadata": {},
   "source": [
    "### Impostazioni per la riproducibilit√† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds impostati su 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Imposto i seed per la riproducibilit√†.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        # Imposto anche i seed per la GPU, se disponibile\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Esegui l'impostazione del seed\n",
    "set_seed(42) \n",
    "print(\"Random seeds impostati su 42.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb63eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260219_181741-sc66wk2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/sc66wk2x' target=\"_blank\">run_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/sc66wk2x' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/sc66wk2x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B inizializzato per il progetto: BigData-TextMining-ACOS\n"
     ]
    }
   ],
   "source": [
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\",\n",
    "    \"seed\": 42,\n",
    "    'patience': 2  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config,\n",
    "    name=f\"run_{config['model_name']}_{config['dataset']}\"\n",
    ")\n",
    "\n",
    "print(f\"W&B inizializzato per il progetto: {wandb.run.project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84319964",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bba6",
   "metadata": {},
   "source": [
    "### Creazione di PyTorch Dataset e DataLoader\n",
    "In questa fase, trasformiamo i nostri DataFrame Pandas (strutture dati tabellari) in oggetti Dataset e DataLoader di PyTorch. Questo passaggio √® il \"ponte\" necessario per alimentare il modello ModernBERT durante l'addestramento.\n",
    "\n",
    "#### Obiettivi di questa sezione:\n",
    "\n",
    "  1. Standardizzazione dei Dati (ACOSDataset):\n",
    "\n",
    "       * I modelli basati su Transformer non possono leggere direttamente i DataFrame. La classe ACOSDataset estrae le liste di input_ids, attention_mask e labels e le converte in Tensori PyTorch (torch.tensor).\n",
    "\n",
    "       * Viene utilizzato il tipo di dato torch.long, richiesto dai layer di embedding e dalle funzioni di calcolo della Loss per task di classificazione.\n",
    "\n",
    "  2. Gestione del Caricamento (DataLoader):\n",
    "\n",
    "      * Batching: Invece di caricare l'intero dataset in memoria (rischioso per la GPU), i dati vengono divisi in piccoli blocchi chiamati Batch (nel nostro caso di dimensione 16).\n",
    "\n",
    "      * Shuffling (Solo Training): Utilizziamo shuffle=True nel train_loader per rimescolare l'ordine delle frasi a ogni epoca. Questo impedisce al modello di imparare l'ordine sequenziale dei dati, costringendolo invece a focalizzarsi sui pattern linguistici reali.\n",
    "\n",
    "      * Efficienza: I DataLoader gestiscono il caricamento dei dati in parallelo, ottimizzando i tempi di addestramento sulla GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7f7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento dei dataset pre-processati...\n",
      "Dataset e DataLoaders creati con successo!\n",
      "Esempi nel set di Training LAPTOP: 2934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. CARICAMENTO DEI DATASET SALVATI  ---\n",
    "cartella_dati = \"data_allineati\"\n",
    "\n",
    "print(\"üìÇ Caricamento dei dataset pre-processati...\")\n",
    "# Carichiamo i Laptop\n",
    "df_train_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"train_laptop_aligned.pkl\"))\n",
    "df_dev_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"dev_laptop_aligned.pkl\"))\n",
    "df_test_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"test_laptop_aligned.pkl\"))\n",
    "\n",
    "class ACOSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Estraiamo le colonne che abbiamo generato nella fase di allineamento\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convertiamo le liste in Tensori di PyTorch (LongTensor per ID e Label)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- CREAZIONE DELLE ISTANZE ---\n",
    "\n",
    "# Creiamo i dataset per il dominio Laptop\n",
    "train_dataset_laptop = ACOSDataset(df_train_align_laptop)\n",
    "dev_dataset_laptop = ACOSDataset(df_dev_align_laptop)\n",
    "test_dataset_laptop = ACOSDataset(df_test_align_laptop)\n",
    "\n",
    "\n",
    "# --- CONFIGURAZIONE DATALOADERS ---\n",
    "\n",
    "BATCH_SIZE = 16 # Numero di frasi analizzate contemporaneamente\n",
    "\n",
    "train_loader_laptop = DataLoader(train_dataset_laptop, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_laptop = DataLoader(dev_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "test_loader_laptop = DataLoader(test_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dataset e DataLoaders creati con successo!\")\n",
    "print(f\"Esempi nel set di Training LAPTOP: {len(train_dataset_laptop)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70053",
   "metadata": {},
   "source": [
    "### Definizione e l'Inizializzazione del Modello di Token Classification.\n",
    "\n",
    "1. Caricare il \"Cervello\" (ModernBERT Pre-trained)\n",
    "Dobbiamo scaricare il modello ModernBERT-base dal repository di Hugging Face. In questa fase, il modello sa gi√† \"leggere\" e \"capire\" la lingua inglese perch√© √® stato addestrato su miliardi di testi, ma non sa ancora nulla del tuo task specifico (ACOS). √à come un laureato in lingue che per√≤ non ha mai lavorato in un ristorante o in un negozio di computer.\n",
    "\n",
    "2. Aggiungere la \"Testa\" di Classificazione\n",
    "ModernBERT normalmente restituisce dei vettori numerici (embedding) per ogni parola. Noi dobbiamo aggiungere sopra questi vettori uno strato finale chiamato Linear Layer (o testa di classificazione).\n",
    "\n",
    "   * Questo strato prender√† l'output di ModernBERT e lo \"schiaccer√†\" su 5 classi possibili: 0 (O), 1 (B-ASP), 2 (I-ASP), 3 (B-OPI), 4 (I-OPI).\n",
    "\n",
    "   * Il modello dovr√† imparare a mappare ogni pezzetto di frase a una di queste cinque etichette.\n",
    "\n",
    "3. Configurare la Strategia di Apprendimento (Optimizer & Loss)\n",
    "Dobbiamo dare al modello gli strumenti per imparare dai suoi errori:\n",
    "\n",
    "  * Loss Function (Funzione di Perdita): Useremo la CrossEntropyLoss. √à il \"voto\" che diamo al modello. Se il modello dice che \"pizza\" √® un'opinione (B-OPI) ma il tuo dataset dice che √® un aspetto (B-ASP), la Loss sar√† alta. Il modello cercher√† di abbassarla il pi√π possibile.\n",
    "\n",
    "   * Optimizer (Ottimizzatore): Di solito si usa AdamW. √à l'algoritmo che decide \"come\" e \"quanto\" cambiare i pesi interni del modello per correggere gli errori.\n",
    "\n",
    "   * Learning Rate: La velocit√† con cui il modello impara. Se √® troppo alta, il modello √® \"frettoloso\" e sbaglia; se √® troppo bassa, non imparer√† mai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU Trovata: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Scaricamento e configurazione di ModernBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e577dd4e9f7e41a9b81db0d5d4f396d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertForTokenClassification LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     | \n",
      "------------------+------------+-\n",
      "decoder.bias      | UNEXPECTED | \n",
      "classifier.weight | MISSING    | \n",
      "classifier.bias   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODELLO PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base\n",
      "Task: Token Classification (Estrazione Aspetti & Opinioni)\n",
      "Numero di Classi: 5\n",
      "Optimizer: AdamW (lr=5e-5)\n",
      "Loss Function: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. CONFIGURAZIONE DEL DEVICE ---\n",
    "# Se hai una GPU NVIDIA, user√† 'cuda'. Se hai un Mac M1/M2, user√† 'mps'. Altrimenti 'cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\" GPU Trovata: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Acceleratore Apple Metal (MPS) Trovato\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Nessuna GPU trovata. L'addestramento sar√† lento.\")\n",
    "\n",
    "# --- 2. CARICAMENTO DI MODERNBERT (IL \"CERVELLO\") + TESTA DI CLASSIFICAZIONE ---\n",
    "# Definiamo le 5 etichette: 0=O, 1=B-ASP, 2=I-ASP, 3=B-OPI, 4=I-OPI\n",
    "NUM_LABELS = 5 \n",
    "\n",
    "print(\"Scaricamento e configurazione di ModernBERT...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\",\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# Spostiamo il modello sul dispositivo di calcolo (GPU/MPS/CPU)\n",
    "model.to(device)\n",
    "\n",
    "# --- 3. CONFIGURAZIONE DELL'OTTIMIZZATORE E DELLA LOSS ---\n",
    "\n",
    "# A. Optimizer (AdamW 8-bit)\n",
    "# Usiamo il Learning Rate standard di 5e-5 come definito nei parametri sperimentali \n",
    "# e la versione a 8-bit per non saturare la memoria\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=5e-5)\n",
    "\n",
    "# B. Loss Function (CrossEntropyLoss)\n",
    "# La funzione che calcola l'errore tra la predizione del modello e le label reali.\n",
    "# Nota: 'ignore_index=-100' √® lo standard di PyTorch per ignorare i token di padding nel calcolo dell'errore.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELLO PRONTO PER IL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architettura: ModernBERT-base\")\n",
    "print(f\"Task: Token Classification (Estrazione Aspetti & Opinioni)\")\n",
    "print(f\"Numero di Classi: {NUM_LABELS}\")\n",
    "print(f\"Optimizer: AdamW (lr=5e-5)\")\n",
    "print(f\"Loss Function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c723566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training su LAPTOP: 10 epoche | Device: cuda\n",
      "üì¶ Accumulo Gradienti ogni 4 step | FP16 Attivato\n",
      "\n",
      "--- Epoca 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:38<00:00,  4.80it/s, loss=0.0534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.1472 | üîç Valid Loss: 0.0526\n",
      "üíæ Miglior modello trovato (Loss: 0.0526)! Salvataggio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de04402356f540f3be1ae8e1de57446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoca 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:37<00:00,  4.89it/s, loss=0.0597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0416 | üîç Valid Loss: 0.0356\n",
      "üíæ Miglior modello trovato (Loss: 0.0356)! Salvataggio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3133161e32f440f18e2d28dee805e233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoca 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:37<00:00,  4.90it/s, loss=0.0361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0278 | üîç Valid Loss: 0.0311\n",
      "üíæ Miglior modello trovato (Loss: 0.0311)! Salvataggio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb3ffd6560c4804ae71c2d59ff14aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoca 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:37<00:00,  4.92it/s, loss=0.00291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0192 | üîç Valid Loss: 0.0320\n",
      "‚ö†Ô∏è Nessun miglioramento. Patience: 1/2\n",
      "\n",
      "--- Epoca 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:37<00:00,  4.91it/s, loss=0.03]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0124 | üîç Valid Loss: 0.0340\n",
      "‚ö†Ô∏è Nessun miglioramento. Patience: 2/2\n",
      "\n",
      "üõë EARLY STOPPING ATTIVATO! Interruzione all'epoca 5.\n",
      "\n",
      "‚úÖ Fine Addestramento.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>‚ñà‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>valid_loss_epoch</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.02999</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss_epoch</td><td>0.01237</td></tr><tr><td>valid_loss_epoch</td><td>0.034</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/sc66wk2x' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/sc66wk2x</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260219_181741-sc66wk2x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "# Gradient Checkpointing: risparmia tantissima VRAM ricalcolando i passaggi intermedi\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config.get('accumulation_steps', 4) \n",
    "patience = config.get('patience', 2)\n",
    "patience_counter = 0\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config['learning_rate']\n",
    ")\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "total_steps = (len(train_loader_laptop) // accumulation_steps) * config['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE ---\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast anche in valutazione\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        wandb.log({\"batch_loss\": loss.item() * accumulation_steps})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"üöÄ Training su LAPTOP: {config['epochs']} epoche | Device: {device}\")\n",
    "print(f\"üì¶ Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch(model, train_loader_laptop, optimizer, scheduler, device, epoch, scaler, accumulation_steps)\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model(model, dev_loader_laptop, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"üìâ Train Loss: {train_loss_laptop:.4f} | üîç Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    # Se il modello migliora (la valid loss scende)\n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  # ### NUOVO: Resettiamo la pazienza ###\n",
    "        \n",
    "        print(f\"üíæ Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})! Salvataggio...\")\n",
    "        \n",
    "        import os\n",
    "        # Creiamo la cartella se non esiste (sicurezza aggiuntiva)\n",
    "        if not os.path.exists(\"./best_model_laptop\"):\n",
    "            os.makedirs(\"./best_model_laptop\")\n",
    "            \n",
    "        model.save_pretrained(\"./best_model_laptop\")\n",
    "        \n",
    "    # Se il modello NON migliora\n",
    "    else:\n",
    "        patience_counter += 1  # ### NUOVO: Incrementiamo il contatore ###\n",
    "        print(f\"‚ö†Ô∏è Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Se abbiamo esaurito la pazienza\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nüõë EARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break # Esce dal ciclo for\n",
    "\n",
    "print(\"\\n‚úÖ Fine Addestramento.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6584a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento del modello migliore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee4d4cf63934936a5ce520364115b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Inizio Test sul Dataset Laptop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test LAPTOP: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:09<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä RISULTATI FINALI (TEST SET - LAPTOP)\n",
      "==================================================\n",
      "Overall Precision: 0.6661\n",
      "Overall Recall:    0.7654\n",
      "Overall F1-Score:  0.7123\n",
      "Overall Accuracy:  0.9884\n",
      "\n",
      "üîç Dettaglio per Classe (Quello che conta per il paper):\n",
      "--------------------------------------------------\n",
      "üîπ ASP:\n",
      "   Precision: 0.6104\n",
      "   Recall:    0.7307\n",
      "   F1-Score:  0.6652\n",
      "   Support:   802\n",
      "üîπ OPI:\n",
      "   Precision: 0.7289\n",
      "   Recall:    0.8013\n",
      "   F1-Score:  0.7634\n",
      "   Support:   775\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# --- A. CARICAMENTO DEL \"CAMPIONE\" ---\n",
    "# Carichiamo i pesi migliori salvati durante il training (Epoca 3)\n",
    "print(\"üìÇ Caricamento del modello migliore...\")\n",
    "model_path = \"./best_model_laptop\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval() # Modalit√† esame (spegne dropout)\n",
    "\n",
    "# --- B. PREPARAZIONE METRICHE ---\n",
    "# Carichiamo la metrica seqeval (standard per NER/ABSA)\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Mappa per decodificare i numeri in etichette\n",
    "# (Assicurati che corrisponda al tuo training!)\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "label_list = list(id2label.values())\n",
    "\n",
    "print(\"üöÄ Inizio Test sul Dataset Laptop...\")\n",
    "\n",
    "# --- C. CICLO DI PREVISIONE ---\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_laptop, desc=\"Test LAPTOP\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 1. Il modello predice\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # 2. Prendiamo la classe con probabilit√† pi√π alta (argmax)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # 3. Convertiamo i numeri in etichette (pulendo i -100)\n",
    "        # Dobbiamo ignorare i token speciali (-100) usati per il padding/subwords\n",
    "        for i in range(len(labels)):\n",
    "            true_label_row = []\n",
    "            pred_label_row = []\n",
    "            \n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] != -100: # Ignoriamo i token di padding/speciali\n",
    "                    true_label_row.append(id2label[labels[i][j].item()])\n",
    "                    pred_label_row.append(id2label[preds[i][j].item()])\n",
    "            \n",
    "            true_labels.append(true_label_row)\n",
    "            predictions.append(pred_label_row)\n",
    "\n",
    "# --- D. CALCOLO E STAMPA RISULTATI ---\n",
    "results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä RISULTATI FINALI (TEST SET - LAPTOP)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Stampiamo le metriche generali\n",
    "print(f\"Overall Precision: {results['overall_precision']:.4f}\")\n",
    "print(f\"Overall Recall:    {results['overall_recall']:.4f}\")\n",
    "print(f\"Overall F1-Score:  {results['overall_f1']:.4f}\")\n",
    "print(f\"Overall Accuracy:  {results['overall_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Dettaglio per Classe (Quello che conta per il paper):\")\n",
    "print(\"-\" * 50)\n",
    "# Estraiamo le metriche specifiche per ASP e OPI\n",
    "for key in results.keys():\n",
    "    if key in ['ASP', 'OPI']: # Filtriamo solo le nostre classi di interesse\n",
    "        print(f\"üîπ {key}:\")\n",
    "        print(f\"   Precision: {results[key]['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {results[key]['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {results[key]['f1']:.4f}\")\n",
    "        print(f\"   Support:   {results[key]['number']}\") # Quanti ce n'erano davvero\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9758056",
   "metadata": {},
   "source": [
    "## Classificatore Category-Sentiment (Extract-Classify-ACOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b648b",
   "metadata": {},
   "source": [
    "Implementiamo il **secondo stadio** dell'architettura proposta nel paper. Dopo aver estratto gli Aspetti e le Opinioni nello Step 1, ora dobbiamo capire a quale Categoria appartengono e qual √® il loro Sentiment.\n",
    "\n",
    "Il codice di preparazione √® diviso in tre componenti fondamentali:\n",
    "\n",
    " 1. Il Dataset PyTorch (`ACOSPairDataset`)\n",
    "\n",
    " 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "\n",
    " 3. Inizializzazione e DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4462143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACOSPairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        \n",
    "        # Gli span sono gi√† corretti con il +1 per il [CLS]!\n",
    "        a_span = row['aspect_span']\n",
    "        o_span = row['opinion_span']\n",
    "\n",
    "        # Tokenizzazione (ModernBERT usa il token [CLS] in automatico)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'aspect_span': torch.tensor(a_span),\n",
    "            'opinion_span': torch.tensor(o_span),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364034ae",
   "metadata": {},
   "source": [
    "### 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "Questa √® la vera \"magia\" matematica del paper, tradotta in codice:\n",
    "* **Il Corpo (Backbone):** Invece di partire da zero, carichiamo il *corpo* del modello che hai gi√† addestrato nello Step 1 (`best_model_laptop`). In questo modo, la rete conosce gi√† il dominio tecnico dei computer!\n",
    "* **Span Pooling:** Il modello estrae i vettori (hidden states) corrispondenti alle parole dell'Aspetto e dell'Opinione e ne calcola la media. Se un elemento √® implicito (`-1`), pesca automaticamente il vettore globale del token `[CLS]`.\n",
    "* **Feature Fusion:** Concatena il vettore dell'aspetto ($u_a$) e dell'opinione ($u_o$) in un unico grande vettore di dimensione 1536.\n",
    "* **Le 121 Teste (Multiple Multi-class):** Passa questo vettore in 121 classificatori lineari paralleli. Ognuno di essi decider√† se per la *sua* categoria la coppia √® `Positive (0)`, `Negative (1)`, `Neutral (2)` o `Invalid (3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "843e426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class ModernBertACOSClassifier(nn.Module):\n",
    "    def __init__(self, path_to_best_model, num_categories):\n",
    "        super(ModernBertACOSClassifier, self).__init__()\n",
    "        \n",
    "        # Carichiamo SOLO IL CORPO dal tuo modello dello Step 1\n",
    "        self.modernbert = AutoModel.from_pretrained(path_to_best_model)\n",
    "        hidden_size = self.modernbert.config.hidden_size # 768\n",
    "        \n",
    "        # Le 121 teste (ognuna prende il vettore concatenato 1536 e sputa 4 classi)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size * 2, 4) for _ in range(num_categories)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, aspect_spans, opinion_spans):\n",
    "        outputs = self.modernbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state # [Batch, Seq_Len, 768]\n",
    "        \n",
    "        batch_size = last_hidden_state.size(0)\n",
    "        u_a_list, u_o_list = [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Pooling Aspetto\n",
    "            a_start, a_end = aspect_spans[i]\n",
    "            if a_start == -1: \n",
    "                u_a = last_hidden_state[i, 0, :] # Token [CLS]\n",
    "            else:\n",
    "                u_a = last_hidden_state[i, a_start:a_end, :].mean(dim=0)\n",
    "            \n",
    "            # Pooling Opinione\n",
    "            o_start, o_end = opinion_spans[i]\n",
    "            if o_start == -1: \n",
    "                u_o = last_hidden_state[i, 0, :] # Token [CLS]\n",
    "            else:\n",
    "                u_o = last_hidden_state[i, o_start:o_end, :].mean(dim=0)\n",
    "            \n",
    "            u_a_list.append(u_a)\n",
    "            u_o_list.append(u_o)\n",
    "\n",
    "        # Concatenazione: [u_a ; u_o]\n",
    "        combined_features = torch.cat((torch.stack(u_a_list), torch.stack(u_o_list)), dim=-1)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "\n",
    "        # Passiamo il vettore nelle teste lineari\n",
    "        logits = [head(combined_features) for head in self.heads]\n",
    "        \n",
    "        # Output: [Batch, 121, 4]\n",
    "        return torch.stack(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d70d3e",
   "metadata": {},
   "source": [
    "### 3. Inizializzazione e DataLoaders\n",
    "L'ultimo blocco carica fisicamente i file salvati dalla nostra \"Fabbrica dei Dati\", istanzia i `Dataset`, e crea i `DataLoader` (con batch size = 16) per \"nutrire\" la GPU in modo efficiente durante l'addestramento. Infine, sposta il modello sulla scheda video (CUDA) pronto per il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103aad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849abe809d5f46be835a8b8900dc974f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_laptop\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello inizializzato! Categorie: 121 | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Carichiamo la lista delle categorie salvata prima\n",
    "with open(\"data_coppie/laptop_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list) # 121\n",
    "\n",
    "# 2. Carichiamo i DataFrame di Train e Dev\n",
    "df_train = pd.read_pickle(\"data_coppie/train_laptop_pairs.pkl\")\n",
    "df_dev = pd.read_pickle(\"data_coppie/dev_laptop_pairs.pkl\")\n",
    "df_test = pd.read_pickle(\"data_coppie/test_laptop_pairs.pkl\") # <-- AGGIUNTO!\n",
    "\n",
    "# 3. Inizializziamo il Tokenizer e i Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "train_dataset = ACOSPairDataset(df_train, tokenizer)\n",
    "dev_dataset = ACOSPairDataset(df_dev, tokenizer)\n",
    "test_dataset = ACOSPairDataset(df_test, tokenizer) # <-- AGGIUNTO!\n",
    "\n",
    "# 4. Creiamo i DataLoader (Batch size 16 √® un buon compromesso tra velocit√† e VRAM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dev_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16) # <-- AGGIUNTO!\n",
    "\n",
    "# 5. Inizializziamo il Modello usando i pesi dello Step 1!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modello inizializzato! Categorie: {num_categories} | Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca5086",
   "metadata": {},
   "source": [
    "## WANDB per il Monitoraggio dello Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95073d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/je5xy7qn' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/je5xy7qn</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260219_222615-je5xy7qn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cristinatomaciello/Desktop/UniversitaÃÄ/2anno/1semstre/big data/progetto-text-mining/wandb/run-20260219_222634-wpqj75af</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/wpqj75af' target=\"_blank\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/wpqj75af' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/wpqj75af</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS\n"
     ]
    }
   ],
   "source": [
    "# Chiudiamo per sicurezza qualsiasi run precedente rimasta aperta nello stesso notebook\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters per lo STEP 2\n",
    "config_step2 = {\n",
    "    \"learning_rate\": 2e-5, # Solitamente per lo Step 2 un LR leggermente pi√π basso √® meglio (es. 2e-5)\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 4, # Aggiunto per il tuo training loop ottimizzato!\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\", \n",
    "    \"seed\": 42,\n",
    "    \"patience\": 2  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run per lo Step 2\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config_step2,\n",
    "    # Aggiungiamo \"Step2_Class\" al nome per distinguerlo dallo Step 1\n",
    "    name=f\"Step2_Class_{config_step2['model_name']}_{config_step2['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\" W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6178aa",
   "metadata": {},
   "source": [
    "## Training e valutazione del modello su Sentiment e Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf680e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "\n",
    "# Gradient Checkpointing: abilitato SOLO sul corpo di ModernBERT\n",
    "model.modernbert.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config_step2.get('accumulation_steps', 4) \n",
    "patience = config_step2.get('patience', 2)\n",
    "patience_counter = 0\n",
    "\n",
    "# Ottimizzatore AdamW a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config_step2['learning_rate']\n",
    ")\n",
    "\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "# La Loss per le 121 teste\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calcolo degli step totali per lo scheduler\n",
    "total_steps = (len(train_loader) // accumulation_steps) * config_step2['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (STEP 2) ---\n",
    "\n",
    "def evaluate_model_step2(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            aspect_span = batch['aspect_span'].to(device)\n",
    "            opinion_span = batch['opinion_span'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast anche in valutazione\n",
    "            with autocast(device_type='cuda'):\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                               aspect_spans=aspect_span, opinion_spans=opinion_span)\n",
    "                \n",
    "                # Calcolo Loss su tutte le teste\n",
    "                loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_step2(model, data_loader, optimizer, scheduler, criterion, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        aspect_span = batch['aspect_span'].to(device)\n",
    "        opinion_span = batch['opinion_span'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass\n",
    "        with autocast(device_type='cuda'):\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                           aspect_spans=aspect_span, opinion_spans=opinion_span)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            # Dividiamo la loss per gli step di accumulo\n",
    "            loss = loss / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Ricalcoliamo il valore reale della loss per i log\n",
    "        real_loss = loss.item() * accumulation_steps\n",
    "        total_loss += real_loss\n",
    "        \n",
    "        wandb.log({\"batch_loss\": real_loss})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=real_loss)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"Training STEP 2 su LAPTOP: {config_step2['epochs']} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "\n",
    "for epoch in range(config_step2['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config_step2['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch_step2(\n",
    "        model, train_loader, optimizer, scheduler, criterion, \n",
    "        device, epoch, scaler, accumulation_steps\n",
    "    )\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model_step2(model, val_loader, criterion, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"üìâ Train Loss: {train_loss_laptop:.4f} | üîç Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"üíæ Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})! Salvataggio...\")\n",
    "        \n",
    "        # Salvataggio custom model (Solo i pesi)\n",
    "        save_dir = \"./best_classifier_laptop\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Step 2.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4c16a",
   "metadata": {},
   "source": [
    "## Test su sentiment e categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    # Chiamo la run con il prefisso \"TEST_\"\n",
    "    name=f\"TEST_Step2_{config_step2['model_name']}_{config_step2['dataset']}\",\n",
    "    job_type=\"test\"\n",
    ")\n",
    "\n",
    "# --- 1. PREPARAZIONE ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Avvio Test sul Device: {device}\")\n",
    "\n",
    "# Ricreiamo l'architettura del modello\n",
    "# (Assicurati che 'category_list' sia ancora in memoria, altrimenti ricaricala dal .pkl)\n",
    "num_categories = len(category_list)\n",
    "model_test = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories)\n",
    "\n",
    "# Carichiamo i pesi appena addestrati!\n",
    "model_path = \"./best_classifier_laptop/pytorch_model.bin\"\n",
    "model_test.load_state_dict(torch.load(model_path))\n",
    "model_test.to(device)\n",
    "model_test.eval()\n",
    "\n",
    "# --- 2. INFERENZA SUL TEST SET ---\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "print(\"Calcolo delle predizioni sul Test Set in corso...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader: \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        aspect_span = batch['aspect_span'].to(device)\n",
    "        opinion_span = batch['opinion_span'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        # Facciamo predire il modello (usiamo autocast se lo hai usato in training)\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            logits = model_test(input_ids, attention_mask, aspect_span, opinion_span)\n",
    "            \n",
    "        # I logits sono [Batch, 13, 4]. Prendiamo la classe con la probabilit√† pi√π alta\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Portiamo i risultati su CPU e li aggiungiamo alle liste\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        all_true.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "# --- 3. VALUTAZIONE DELLE PERFORMANCE ---\n",
    "all_preds = np.array(all_preds)\n",
    "all_true = np.array(all_true)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REPORT FINALE DEL CLASSIFICATORE (Step 2)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calcoliamo il report ignorando la classe 3 (Invalid) per vedere le performance reali\n",
    "target_names = ['Positive (0)', 'Negative (1)', 'Neutral (2)']\n",
    "labels_to_eval = [0, 1, 2]\n",
    "\n",
    "report = classification_report(\n",
    "    all_true, \n",
    "    all_preds, \n",
    "    labels=labels_to_eval, \n",
    "    target_names=target_names,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Calcoliamo il Micro F1-Score (che √® la metrica standard usata in questi paper)\n",
    "micro_f1 = f1_score(all_true, all_preds, labels=labels_to_eval, average='micro')\n",
    "macro_f1 = f1_score(all_true, all_preds, labels=labels_to_eval, average='macro')\n",
    "\n",
    "print(f\"MICRO F1-Score (Sentiment Corretto): {micro_f1:.4f}\")\n",
    "print(f\"MACRO F1-Score (Media delle classi): {macro_f1:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
