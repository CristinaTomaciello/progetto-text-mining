{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7266c47e",
   "metadata": {},
   "source": [
    "# Classificazione di Aspect e Opinion con ModernBERT su Laptop-ACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie caricate.\n"
     ]
    }
   ],
   "source": [
    "# Import delle librerie necessarie\n",
    "# Import delle librerie necessarie\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.amp import autocast, GradScaler # Per Mixed Precision\n",
    "\n",
    "print(\"Librerie caricate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70bed1",
   "metadata": {},
   "source": [
    "### Impostazioni per la riproducibilit√† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68aa33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds impostati su 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Imposto i seed per la riproducibilit√†.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        # Imposto anche i seed per la GPU, se disponibile\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Esegui l'impostazione del seed\n",
    "set_seed(42) \n",
    "print(\"Random seeds impostati su 42.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb63eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cristinatomaciello/Desktop/UniversitaÃÄ/2anno/1semstre/big data/progetto-text-mining/wandb/run-20260219_164246-8o3f12ih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/8o3f12ih' target=\"_blank\">run_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/8o3f12ih' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/8o3f12ih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B inizializzato per il progetto: BigData-TextMining-ACOS\n"
     ]
    }
   ],
   "source": [
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\",\n",
    "    \"seed\": 42,\n",
    "    'patience': 2  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config,\n",
    "    name=f\"run_{config['model_name']}_{config['dataset']}\"\n",
    ")\n",
    "\n",
    "print(f\"W&B inizializzato per il progetto: {wandb.run.project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84319964",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bba6",
   "metadata": {},
   "source": [
    "### Creazione di PyTorch Dataset e DataLoader\n",
    "In questa fase, trasformiamo i nostri DataFrame Pandas (strutture dati tabellari) in oggetti Dataset e DataLoader di PyTorch. Questo passaggio √® il \"ponte\" necessario per alimentare il modello ModernBERT durante l'addestramento.\n",
    "\n",
    "#### Obiettivi di questa sezione:\n",
    "\n",
    "  1. Standardizzazione dei Dati (ACOSDataset):\n",
    "\n",
    "       * I modelli basati su Transformer non possono leggere direttamente i DataFrame. La classe ACOSDataset estrae le liste di input_ids, attention_mask e labels e le converte in Tensori PyTorch (torch.tensor).\n",
    "\n",
    "       * Viene utilizzato il tipo di dato torch.long, richiesto dai layer di embedding e dalle funzioni di calcolo della Loss per task di classificazione.\n",
    "\n",
    "  2. Gestione del Caricamento (DataLoader):\n",
    "\n",
    "      * Batching: Invece di caricare l'intero dataset in memoria (rischioso per la GPU), i dati vengono divisi in piccoli blocchi chiamati Batch (nel nostro caso di dimensione 16).\n",
    "\n",
    "      * Shuffling (Solo Training): Utilizziamo shuffle=True nel train_loader per rimescolare l'ordine delle frasi a ogni epoca. Questo impedisce al modello di imparare l'ordine sequenziale dei dati, costringendolo invece a focalizzarsi sui pattern linguistici reali.\n",
    "\n",
    "      * Efficienza: I DataLoader gestiscono il caricamento dei dati in parallelo, ottimizzando i tempi di addestramento sulla GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7f7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento dei dataset pre-processati...\n",
      "Dataset e DataLoaders creati con successo!\n",
      "Esempi nel set di Training LAPTOP: 2934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. CARICAMENTO DEI DATASET SALVATI  ---\n",
    "cartella_dati = \"data_allineati\"\n",
    "\n",
    "print(\"üìÇ Caricamento dei dataset pre-processati...\")\n",
    "# Carichiamo i Laptop\n",
    "df_train_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"train_laptop_aligned.pkl\"))\n",
    "df_dev_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"dev_laptop_aligned.pkl\"))\n",
    "df_test_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"test_laptop_aligned.pkl\"))\n",
    "\n",
    "class ACOSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Estraiamo le colonne che abbiamo generato nella fase di allineamento\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convertiamo le liste in Tensori di PyTorch (LongTensor per ID e Label)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- CREAZIONE DELLE ISTANZE ---\n",
    "\n",
    "# Creiamo i dataset per il dominio Laptop\n",
    "train_dataset_laptop = ACOSDataset(df_train_align_laptop)\n",
    "dev_dataset_laptop = ACOSDataset(df_dev_align_laptop)\n",
    "test_dataset_laptop = ACOSDataset(df_test_align_laptop)\n",
    "\n",
    "\n",
    "# --- CONFIGURAZIONE DATALOADERS ---\n",
    "\n",
    "BATCH_SIZE = 16 # Numero di frasi analizzate contemporaneamente\n",
    "\n",
    "train_loader_laptop = DataLoader(train_dataset_laptop, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_laptop = DataLoader(dev_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "test_loader_laptop = DataLoader(test_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dataset e DataLoaders creati con successo!\")\n",
    "print(f\"Esempi nel set di Training LAPTOP: {len(train_dataset_laptop)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70053",
   "metadata": {},
   "source": [
    "### Definizione e l'Inizializzazione del Modello di Token Classification.\n",
    "\n",
    "1. Caricare il \"Cervello\" (ModernBERT Pre-trained)\n",
    "Dobbiamo scaricare il modello ModernBERT-base dal repository di Hugging Face. In questa fase, il modello sa gi√† \"leggere\" e \"capire\" la lingua inglese perch√© √® stato addestrato su miliardi di testi, ma non sa ancora nulla del tuo task specifico (ACOS). √à come un laureato in lingue che per√≤ non ha mai lavorato in un ristorante o in un negozio di computer.\n",
    "\n",
    "2. Aggiungere la \"Testa\" di Classificazione\n",
    "ModernBERT normalmente restituisce dei vettori numerici (embedding) per ogni parola. Noi dobbiamo aggiungere sopra questi vettori uno strato finale chiamato Linear Layer (o testa di classificazione).\n",
    "\n",
    "   * Questo strato prender√† l'output di ModernBERT e lo \"schiaccer√†\" su 5 classi possibili: 0 (O), 1 (B-ASP), 2 (I-ASP), 3 (B-OPI), 4 (I-OPI).\n",
    "\n",
    "   * Il modello dovr√† imparare a mappare ogni pezzetto di frase a una di queste cinque etichette.\n",
    "\n",
    "3. Configurare la Strategia di Apprendimento (Optimizer & Loss)\n",
    "Dobbiamo dare al modello gli strumenti per imparare dai suoi errori:\n",
    "\n",
    "  * Loss Function (Funzione di Perdita): Useremo la CrossEntropyLoss. √à il \"voto\" che diamo al modello. Se il modello dice che \"pizza\" √® un'opinione (B-OPI) ma il tuo dataset dice che √® un aspetto (B-ASP), la Loss sar√† alta. Il modello cercher√† di abbassarla il pi√π possibile.\n",
    "\n",
    "   * Optimizer (Ottimizzatore): Di solito si usa AdamW. √à l'algoritmo che decide \"come\" e \"quanto\" cambiare i pesi interni del modello per correggere gli errori.\n",
    "\n",
    "   * Learning Rate: La velocit√† con cui il modello impara. Se √® troppo alta, il modello √® \"frettoloso\" e sbaglia; se √® troppo bassa, non imparer√† mai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acceleratore Apple Metal (MPS) Trovato\n",
      "Scaricamento e configurazione di ModernBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd28330d4974b2089dcfdcab028e61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertForTokenClassification LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     | \n",
      "------------------+------------+-\n",
      "decoder.bias      | UNEXPECTED | \n",
      "classifier.weight | MISSING    | \n",
      "classifier.bias   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODELLO PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base\n",
      "Task: Token Classification (Estrazione Aspetti & Opinioni)\n",
      "Numero di Classi: 5\n",
      "Optimizer: AdamW (lr=5e-5)\n",
      "Loss Function: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. CONFIGURAZIONE DEL DEVICE ---\n",
    "# Se hai una GPU NVIDIA, user√† 'cuda'. Se hai un Mac M1/M2, user√† 'mps'. Altrimenti 'cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\" GPU Trovata: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Acceleratore Apple Metal (MPS) Trovato\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Nessuna GPU trovata. L'addestramento sar√† lento.\")\n",
    "\n",
    "# --- 2. CARICAMENTO DI MODERNBERT (IL \"CERVELLO\") + TESTA DI CLASSIFICAZIONE ---\n",
    "# Definiamo le 5 etichette: 0=O, 1=B-ASP, 2=I-ASP, 3=B-OPI, 4=I-OPI\n",
    "NUM_LABELS = 5 \n",
    "\n",
    "print(\"Scaricamento e configurazione di ModernBERT...\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\",\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# Spostiamo il modello sul dispositivo di calcolo (GPU/MPS/CPU)\n",
    "model.to(device)\n",
    "\n",
    "# --- 3. CONFIGURAZIONE DELL'OTTIMIZZATORE E DELLA LOSS ---\n",
    "\n",
    "# A. Optimizer (AdamW 8-bit)\n",
    "# Usiamo il Learning Rate standard di 5e-5 come definito nei parametri sperimentali \n",
    "# e la versione a 8-bit per non saturare la memoria\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=5e-5)\n",
    "\n",
    "# B. Loss Function (CrossEntropyLoss)\n",
    "# La funzione che calcola l'errore tra la predizione del modello e le label reali.\n",
    "# Nota: 'ignore_index=-100' √® lo standard di PyTorch per ignorare i token di padding nel calcolo dell'errore.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELLO PRONTO PER IL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architettura: ModernBERT-base\")\n",
    "print(f\"Task: Token Classification (Estrazione Aspetti & Opinioni)\")\n",
    "print(f\"Numero di Classi: {NUM_LABELS}\")\n",
    "print(f\"Optimizer: AdamW (lr=5e-5)\")\n",
    "print(f\"Loss Function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c723566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Inizio Training su LAPTOP: 5 epoche su mps\n",
      "üõë Early Stopping configurato con Patience = 2\n",
      "\n",
      "--- Epoca 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/184 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [1:01:52<00:00, 20.17s/it, loss=0.0488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0710 | üîç Valid Loss: 0.0343\n",
      "üíæ Miglior modello trovato (Loss: 0.0343)! Salvataggio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4c38c6349e4bdca28644d962b3718d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoca 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [57:56<00:00, 18.89s/it, loss=0.0479] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0280 | üîç Valid Loss: 0.0311\n",
      "üíæ Miglior modello trovato (Loss: 0.0311)! Salvataggio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd47a23c7da480ca72633f41b5b2954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoca 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [1:13:39<00:00, 24.02s/it, loss=0.00788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0178 | üîç Valid Loss: 0.0284\n",
      "üíæ Miglior modello trovato (Loss: 0.0284)! Salvataggio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3affc35bbdca40f298c3d038c37a5e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoca 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [1:08:28<00:00, 22.33s/it, loss=0.00218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0098 | üîç Valid Loss: 0.0330\n",
      "‚ö†Ô∏è Nessun miglioramento. Patience: 1/2\n",
      "\n",
      "--- Epoca 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [1:19:17<00:00, 25.86s/it, loss=0.0117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Train Loss: 0.0048 | üîç Valid Loss: 0.0340\n",
      "‚ö†Ô∏è Nessun miglioramento. Patience: 2/2\n",
      "\n",
      "üõë EARLY STOPPING ATTIVATO! Interruzione all'epoca 5.\n",
      "\n",
      "‚úÖ Fine Addestramento.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>valid_loss_epoch</td><td>‚ñà‚ñÑ‚ñÅ‚ñÜ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.01166</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss_epoch</td><td>0.00477</td></tr><tr><td>valid_loss_epoch</td><td>0.03401</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/w86iot8t' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/w86iot8t</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260214_164930-w86iot8t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "# Gradient Checkpointing: risparmia tantissima VRAM ricalcolando i passaggi intermedi\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config.get('accumulation_steps', 4) \n",
    "patience = config.get('patience', 2)\n",
    "patience_counter = 0\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config['learning_rate']\n",
    ")\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "total_steps = (len(train_loader_laptop) // accumulation_steps) * config['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE ---\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast anche in valutazione\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        wandb.log({\"batch_loss\": loss.item() * accumulation_steps})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"üöÄ Training su LAPTOP: {config['epochs']} epoche | Device: {device}\")\n",
    "print(f\"üì¶ Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch(model, train_loader_laptop, optimizer, scheduler, device, epoch, scaler, accumulation_steps)\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model(model, dev_loader_laptop, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"üìâ Train Loss: {train_loss_laptop:.4f} | üîç Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    # Se il modello migliora (la valid loss scende)\n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  # ### NUOVO: Resettiamo la pazienza ###\n",
    "        \n",
    "        print(f\"üíæ Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})! Salvataggio...\")\n",
    "        \n",
    "        import os\n",
    "        # Creiamo la cartella se non esiste (sicurezza aggiuntiva)\n",
    "        if not os.path.exists(\"./best_model_laptop\"):\n",
    "            os.makedirs(\"./best_model_laptop\")\n",
    "            \n",
    "        model.save_pretrained(\"./best_model_laptop\")\n",
    "        \n",
    "    # Se il modello NON migliora\n",
    "    else:\n",
    "        patience_counter += 1  # ### NUOVO: Incrementiamo il contatore ###\n",
    "        print(f\"‚ö†Ô∏è Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Se abbiamo esaurito la pazienza\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nüõë EARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break # Esce dal ciclo for\n",
    "\n",
    "print(\"\\n‚úÖ Fine Addestramento.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6584a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Caricamento del modello migliore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f700460a35474bb4b057e3f4a67ab287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a402be0a31634cc3a7ea58a0060d48f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Inizio Test sul Dataset Laptop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test LAPTOP: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [11:25<00:00, 13.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä RISULTATI FINALI (TEST SET - LAPTOP)\n",
      "==================================================\n",
      "Overall Precision: 0.7014\n",
      "Overall Recall:    0.7850\n",
      "Overall F1-Score:  0.7409\n",
      "Overall Accuracy:  0.9892\n",
      "\n",
      "üîç Dettaglio per Classe (Quello che conta per il paper):\n",
      "--------------------------------------------------\n",
      "üîπ ASP:\n",
      "   Precision: 0.6386\n",
      "   Recall:    0.7269\n",
      "   F1-Score:  0.6799\n",
      "   Support:   802\n",
      "üîπ OPI:\n",
      "   Precision: 0.7688\n",
      "   Recall:    0.8452\n",
      "   F1-Score:  0.8052\n",
      "   Support:   775\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# --- A. CARICAMENTO DEL \"CAMPIONE\" ---\n",
    "# Carichiamo i pesi migliori salvati durante il training (Epoca 3)\n",
    "print(\"üìÇ Caricamento del modello migliore...\")\n",
    "model_path = \"./best_model_laptop\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval() # Modalit√† esame (spegne dropout)\n",
    "\n",
    "# --- B. PREPARAZIONE METRICHE ---\n",
    "# Carichiamo la metrica seqeval (standard per NER/ABSA)\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Mappa per decodificare i numeri in etichette\n",
    "# (Assicurati che corrisponda al tuo training!)\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "label_list = list(id2label.values())\n",
    "\n",
    "print(\"üöÄ Inizio Test sul Dataset Laptop...\")\n",
    "\n",
    "# --- C. CICLO DI PREVISIONE ---\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_laptop, desc=\"Test LAPTOP\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 1. Il modello predice\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # 2. Prendiamo la classe con probabilit√† pi√π alta (argmax)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # 3. Convertiamo i numeri in etichette (pulendo i -100)\n",
    "        # Dobbiamo ignorare i token speciali (-100) usati per il padding/subwords\n",
    "        for i in range(len(labels)):\n",
    "            true_label_row = []\n",
    "            pred_label_row = []\n",
    "            \n",
    "            for j in range(len(labels[i])):\n",
    "                if labels[i][j] != -100: # Ignoriamo i token di padding/speciali\n",
    "                    true_label_row.append(id2label[labels[i][j].item()])\n",
    "                    pred_label_row.append(id2label[preds[i][j].item()])\n",
    "            \n",
    "            true_labels.append(true_label_row)\n",
    "            predictions.append(pred_label_row)\n",
    "\n",
    "# --- D. CALCOLO E STAMPA RISULTATI ---\n",
    "results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä RISULTATI FINALI (TEST SET - LAPTOP)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Stampiamo le metriche generali\n",
    "print(f\"Overall Precision: {results['overall_precision']:.4f}\")\n",
    "print(f\"Overall Recall:    {results['overall_recall']:.4f}\")\n",
    "print(f\"Overall F1-Score:  {results['overall_f1']:.4f}\")\n",
    "print(f\"Overall Accuracy:  {results['overall_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nüîç Dettaglio per Classe (Quello che conta per il paper):\")\n",
    "print(\"-\" * 50)\n",
    "# Estraiamo le metriche specifiche per ASP e OPI\n",
    "for key in results.keys():\n",
    "    if key in ['ASP', 'OPI']: # Filtriamo solo le nostre classi di interesse\n",
    "        print(f\"üîπ {key}:\")\n",
    "        print(f\"   Precision: {results[key]['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {results[key]['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {results[key]['f1']:.4f}\")\n",
    "        print(f\"   Support:   {results[key]['number']}\") # Quanti ce n'erano davvero\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
