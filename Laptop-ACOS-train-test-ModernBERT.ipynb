{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7266c47e",
   "metadata": {},
   "source": [
    "# Classificazione di Aspect e Opinion con ModernBERT su Laptop-ACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a40e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie caricate.\n",
      " GPU Trovata: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Import delle librerie necessarie\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup, AutoModel\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb #per gpu invidia\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.amp import autocast, GradScaler # Per Mixed Precision\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"Librerie caricate.\")\n",
    "\n",
    "# --- 1. CONFIGURAZIONE DEL DEVICE ---\n",
    "# Se hai una GPU NVIDIA, user√† 'cuda'. Se hai un Mac M1/M2, user√† 'mps'. Altrimenti 'cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\" GPU Trovata: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Acceleratore Apple Metal (MPS) Trovato\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Nessuna GPU trovata. L'addestramento sar√† lento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70bed1",
   "metadata": {},
   "source": [
    "### Impostazioni per la riproducibilit√† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds impostati su 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Imposto i seed per la riproducibilit√†.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        # Imposto anche i seed per la GPU, se disponibile\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Esegui l'impostazione del seed\n",
    "set_seed(42) \n",
    "print(\"Random seeds impostati su 42.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb63eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260222_170922-xwfj1yam</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xwfj1yam' target=\"_blank\">run_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xwfj1yam' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xwfj1yam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B inizializzato per il progetto: BigData-TextMining-ACOS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\",\n",
    "    \"seed\": 42,\n",
    "    'patience': 2  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config,\n",
    "    name=f\"run_{config['model_name']}_{config['dataset']}\"\n",
    ")\n",
    "\n",
    "print(f\"W&B inizializzato per il progetto: {wandb.run.project}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84319964",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bba6",
   "metadata": {},
   "source": [
    "### Creazione di PyTorch Dataset e DataLoader\n",
    "Questa cella si occupa di caricare i dati pre-processati e di \"impacchettarli\" nel formato esatto richiesto dalla nostra nuova architettura PyTorch personalizzata. Rappresenta un passaggio cruciale per replicare fedelmente il paper originale, permettendoci di gestire gli elementi impliciti.\n",
    "\n",
    "Nello specifico, il codice esegue tre operazioni fondamentali:\n",
    "\n",
    "1. **Caricamento dei DataFrame:** Legge i file `.pkl` (Train, Dev e Test per il dominio Laptop) che abbiamo precedentemente aggiornato. Questi file ora contengono le annotazioni binarie che indicano la presenza di aspetti o opinioni implicite.\n",
    "2. **Definizione della Classe Custom `ACOSDataset`:** Questa √® la modifica principale rispetto a una pipeline standard di HuggingFace. Invece di restituire solo i classici 3 tensori della Token Classification (`input_ids`, `attention_mask` e `labels` con i tag BIO), questa classe sovrascritta restituisce **5 tensori** per ogni frase. Vengono infatti estratti e passati al modello anche `implicit_aspect_labels` e `implicit_opinion_labels`. Questi tensori serviranno ad addestrare in parallelo le due teste di classificazione binaria sul token `[CLS]`.\n",
    "3. **Configurazione dei DataLoader:** Crea gli iteratori di PyTorch che alimenteranno il modello durante l'addestramento e il test, processando blocchi (batch) di 16 frasi alla volta. I dati di training vengono rimescolati (`shuffle=True`) per stabilizzare l'apprendimento della rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7f7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dei dataset pre-processati aggiornati...\n",
      "Dataset e DataLoaders creati con successo!\n",
      "Esempi nel set di Training LAPTOP: 2934\n"
     ]
    }
   ],
   "source": [
    "cartella_dati = \"data_allineati\"\n",
    "\n",
    "print(\"Caricamento dei dataset pre-processati aggiornati...\")\n",
    "df_train_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"train_laptop_aligned.pkl\"))\n",
    "df_dev_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"dev_laptop_aligned.pkl\"))\n",
    "df_test_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"test_laptop_aligned.pkl\"))\n",
    "\n",
    "class ACOSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "        # Estraiamo le colonne per gli impliciti!\n",
    "        self.implicit_aspect_label = df['implicit_aspect_label'].tolist()\n",
    "        self.implicit_opinion_label = df['implicit_opinion_label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            # Passiamo le etichette al Dataloader\n",
    "            'implicit_aspect_labels': torch.tensor(self.implicit_aspect_label[idx], dtype=torch.long),\n",
    "            'implicit_opinion_labels': torch.tensor(self.implicit_opinion_label[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- CREAZIONE DELLE ISTANZE ---\n",
    "\n",
    "# Creiamo i dataset per il dominio Laptop\n",
    "train_dataset_laptop = ACOSDataset(df_train_align_laptop)\n",
    "dev_dataset_laptop = ACOSDataset(df_dev_align_laptop)\n",
    "test_dataset_laptop = ACOSDataset(df_test_align_laptop)\n",
    "\n",
    "\n",
    "# --- CONFIGURAZIONE DATALOADERS ---\n",
    "\n",
    "BATCH_SIZE = 16 # Numero di frasi analizzate contemporaneamente\n",
    "\n",
    "train_loader_laptop = DataLoader(train_dataset_laptop, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_laptop = DataLoader(dev_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "test_loader_laptop = DataLoader(test_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dataset e DataLoaders creati con successo!\")\n",
    "print(f\"Esempi nel set di Training LAPTOP: {len(train_dataset_laptop)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70053",
   "metadata": {},
   "source": [
    "### Architettura Multi-Task: ModernBERT ACOS Extractor\n",
    "\n",
    "In questa cella abbandoniamo l'architettura standard di Token Classification per costruire un modello personalizzato in PyTorch, progettato per replicare fedelmente la logica di estrazione del paper ACOS originale.\n",
    "\n",
    "Il problema dei modelli tradizionali √® che falliscono quando un aspetto o un'opinione non sono scritti esplicitamente nel testo (elementi *Impliciti*). Per risolvere questa criticit√†, abbiamo progettato una rete **Multi-Task** composta da un \"cervello\" centrale (l'encoder ModernBERT) e **tre teste di classificazione indipendenti**:\n",
    "\n",
    "1. **Testa di Token Classification (Estrazione Esplicita):** Analizza ogni singola parola della frase per assegnare i tag BIO (B-ASP, I-ASP, B-OPI, I-OPI, O), estraendo gli span di testo espliciti.\n",
    "2. **Testa per Aspetti Impliciti (Classificazione Binaria):** Sfrutta il token speciale `[CLS]`, che racchiude il significato globale della frase, per prevedere matematicamente (S√¨/No) se la recensione contiene un aspetto sottinteso.\n",
    "3. **Testa per Opinioni Implicite (Classificazione Binaria):** Sfrutta sempre il token `[CLS]` per indovinare se c'√® un'opinione sottintesa.\n",
    "\n",
    "**La Loss Combinata (L'addestramento simultaneo)**\n",
    "Il vero \"motore\" di questa classe √® nella funzione `forward`. Durante l'addestramento, il modello calcola contemporaneamente tre errori separati (uno per l'estrazione e due per le previsioni binarie degli impliciti). Questi tre errori vengono **sommati in un'unica Loss globale**. In questo modo, la rete neurale viene forzata a imparare tutti i task simultaneamente, ottimizzando i pesi interni per comprendere a fondo sia ci√≤ che √® scritto, sia ci√≤ che √® sottinteso.\n",
    "\n",
    "Infine, il modello viene caricato sulla GPU e accoppiato a un ottimizzatore **AdamW a 8-bit** (`bitsandbytes`) per massimizzare l'efficienza e prevenire l'esaurimento della memoria VRAM durante le epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f295063ce313419484372584433d44d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODELLO MULTI-TASK PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base (Custom ACOS Extractor)\n",
      "Task: Token Classification + 2x Binary Classification (Impliciti)\n",
      "Numero di Classi Token: 5\n",
      "Optimizer: AdamW 8-bit (lr=5e-5)\n",
      "Loss Function: Loss Combinata (calcolata internamente)\n"
     ]
    }
   ],
   "source": [
    "# Definiamo le 5 etichette: 0=O, 1=B-ASP, 2=I-ASP, 3=B-OPI, 4=I-OPI\n",
    "NUM_LABELS = 5 \n",
    "\n",
    "# --- 1. IL \"CERVELLO\" + 3 TESTE ---\n",
    "class ModernBertACOS_Extractor(nn.Module):\n",
    "    def __init__(self, model_name=\"answerdotai/ModernBERT-base\", num_labels=5):\n",
    "        super().__init__()\n",
    "        # Carichiamo la \"schiena\" del modello (l'encoder base)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Testolina 1: Trova le parole scritte (Token Classification)\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "        # Testoline 2 e 3: Indovinano se ci sono impliciti (Classificazione Binaria)\n",
    "        self.implicit_aspect_classifier = nn.Linear(hidden_size, 2)\n",
    "        self.implicit_opinion_classifier = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, implicit_aspect_labels=None, implicit_opinion_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        \n",
    "        # Prendiamo il token [CLS] (posizione 0) per i classificatori binari\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "        \n",
    "        # Le tre testoline fanno le loro previsioni\n",
    "        token_logits = self.token_classifier(sequence_output)\n",
    "        imp_asp_logits = self.implicit_aspect_classifier(cls_output)\n",
    "        imp_opi_logits = self.implicit_opinion_classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        # Calcolo della \"Loss Combinata\" durante il training\n",
    "        if labels is not None and implicit_aspect_labels is not None and implicit_opinion_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Loss 1: Token (ignorando il padding -100)\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = token_logits.view(-1, 5)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss_token = loss_fct(active_logits, active_labels)\n",
    "            \n",
    "            # Loss 2 & 3: Impliciti\n",
    "            loss_asp = loss_fct(imp_asp_logits, implicit_aspect_labels)\n",
    "            loss_opi = loss_fct(imp_opi_logits, implicit_opinion_labels)\n",
    "            \n",
    "            # Sommiamo gli errori in modo che impari tutto insieme!\n",
    "            loss = loss_token + loss_asp + loss_opi\n",
    "            \n",
    "        return {\n",
    "            \"loss\": loss, \n",
    "            \"token_logits\": token_logits, \n",
    "            \"imp_asp_logits\": imp_asp_logits, \n",
    "            \"imp_opi_logits\": imp_opi_logits\n",
    "        }\n",
    "\n",
    "print(\"Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\")\n",
    "\n",
    "# Inizializziamo il nostro modello custom invece di AutoModelForTokenClassification\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=NUM_LABELS)\n",
    "\n",
    "# Spostiamo il modello sul dispositivo di calcolo (GPU/MPS/CPU)\n",
    "model_step1.to(device)\n",
    "\n",
    "# --- 2. CONFIGURAZIONE DELL'OTTIMIZZATORE E DELLA LOSS ---\n",
    "\n",
    "# Manteniamo la tua ottima scelta di usare l'optimizer a 8-bit per non saturare la memoria!\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELLO MULTI-TASK PRONTO PER IL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architettura: ModernBERT-base (Custom ACOS Extractor)\")\n",
    "print(f\"Task: Token Classification + 2x Binary Classification (Impliciti)\")\n",
    "print(f\"Numero di Classi Token: {NUM_LABELS}\")\n",
    "print(f\"Optimizer: AdamW 8-bit (lr=5e-5)\")\n",
    "print(f\"Loss Function: Loss Combinata (calcolata internamente)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c723566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attivazione Ottimizzazioni di Memoria...\n",
      "\n",
      "Training Multi-Task su LAPTOP: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti: ogni 4 step | FP16: Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1:   2%|‚ñè         | 3/184 [00:01<00:53,  3.40it/s, loss=3.37]/home/al3th3ia/Scrivania/Cristina/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:38<00:00,  4.77it/s, loss=1.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9724 | Valid Loss: 1.4677\n",
      "Miglior modello trovato (Loss: 1.4677)\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:39<00:00,  4.63it/s, loss=1.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2984 | Valid Loss: 1.0846\n",
      "Miglior modello trovato (Loss: 1.0846)\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:40<00:00,  4.59it/s, loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8684 | Valid Loss: 1.0093\n",
      "Miglior modello trovato (Loss: 1.0093)\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:40<00:00,  4.59it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5553 | Valid Loss: 0.9949\n",
      "Miglior modello trovato (Loss: 0.9949)\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:40<00:00,  4.58it/s, loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2954 | Valid Loss: 1.0662\n",
      "Nessun miglioramento. Patience: 1/2\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 184/184 [00:40<00:00,  4.59it/s, loss=0.0911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1583 | Valid Loss: 1.3611\n",
      "Nessun miglioramento. Patience: 2/2\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 6.\n",
      "\n",
      "Fine Addestramento Multi-Task.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>valid_loss_epoch</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.09107</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>train_loss_epoch</td><td>0.15831</td></tr><tr><td>valid_loss_epoch</td><td>1.36106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xwfj1yam' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xwfj1yam</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260222_170922-xwfj1yam/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA E WANDB ---\n",
    "print(\"Attivazione Ottimizzazioni di Memoria...\")\n",
    "\n",
    "# Gradient Checkpointing: si applica all'encoder interno (BERT) della nostra classe custom\n",
    "model_step1.bert.gradient_checkpointing_enable()\n",
    "\n",
    "accumulation_steps = wandb.config.get('accumulation_steps', 4) \n",
    "patience = wandb.config.get('patience', 2)\n",
    "epochs = wandb.config.get('epochs', 40)\n",
    "lr = wandb.config.get('learning_rate', 5e-5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Optimizer a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=lr)\n",
    "\n",
    "# Scaler per Mixed Precision (FP16)\n",
    "scaler = GradScaler() \n",
    "\n",
    "total_steps = (len(train_loader_laptop) // accumulation_steps) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (MULTI-TASK) ---\n",
    "\n",
    "def evaluate_model_multitask(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "            imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast in valutazione per risparmiare memoria\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels,\n",
    "                    implicit_aspect_labels=imp_asp_labels,\n",
    "                    implicit_opinion_labels=imp_opi_labels\n",
    "                )\n",
    "            \n",
    "            total_loss += outputs['loss'].item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_multitask(model, data_loader, optimizer, scheduler, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass (FP16)\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels,\n",
    "                implicit_aspect_labels=imp_asp_labels,\n",
    "                implicit_opinion_labels=imp_opi_labels\n",
    "            )\n",
    "            # Normalizziamo la loss per l'accumulo dei gradienti\n",
    "            loss = outputs['loss'] / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler (Evita underflow/overflow dell'FP16)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        wandb.log({\"batch_loss\": loss.item() * accumulation_steps})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"\\nTraining Multi-Task su LAPTOP: {epochs} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti: ogni {accumulation_steps} step | FP16: Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "output_dir = \"./best_multitask_extractor_laptop\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{epochs} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch_multitask(model_step1, train_loader_laptop, optimizer, scheduler, device, epoch, scaler, accumulation_steps)\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model_multitask(model_step1, dev_loader_laptop, device)\n",
    "    \n",
    "    # Pulizia spietata della cache della GPU a fine epoca!\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_laptop:.4f} | Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Salvataggio Custom per l'architettura Multi-Task\n",
    "        torch.save(model_step1.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Multi-Task.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6584a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello Multi-Task migliore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce12cb5bed3d4922b881e4b451410969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inizio Test Multi-Task sul Dataset Laptop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test LAPTOP: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:06<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\n",
      "============================================================\n",
      "Overall Precision: 0.6703\n",
      "Overall Recall:    0.6588\n",
      "Overall F1-Score:  0.6645\n",
      "\n",
      "Dettaglio per Classe (Quello che conta per il paper):\n",
      "--------------------------------------------------\n",
      "   ASP:\n",
      "   Precision: 0.5979\n",
      "   Recall:    0.6322\n",
      "   F1-Score:  0.6145\n",
      "   Support:   802\n",
      "   OPI:\n",
      "   Precision: 0.7578\n",
      "   Recall:    0.6865\n",
      "   F1-Score:  0.7204\n",
      "   Support:   775\n",
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\n",
      "============================================================\n",
      "üîπ Accuratezza Aspetti Impliciti: 0.8897\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.92      0.94      0.93       627\n",
      "Implicito (1)       0.78      0.74      0.76       189\n",
      "\n",
      "     accuracy                           0.89       816\n",
      "    macro avg       0.85      0.84      0.84       816\n",
      " weighted avg       0.89      0.89      0.89       816\n",
      "\n",
      "--------------------------------------------------\n",
      "üîπ Accuratezza Opinioni Implicite: 0.8174\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.89      0.82      0.85       530\n",
      "Implicito (1)       0.71      0.81      0.76       286\n",
      "\n",
      "     accuracy                           0.82       816\n",
      "    macro avg       0.80      0.82      0.81       816\n",
      " weighted avg       0.83      0.82      0.82       816\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- A. CARICAMENTO DEL \"CAMPIONE\" MULTI-TASK ---\n",
    "print(\"Caricamento del modello Multi-Task migliore...\")\n",
    "\n",
    "# 1. Inizializziamo la nostra architettura custom\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5)\n",
    "\n",
    "# 2. Carichiamo i pesi salvati del miglior modello (solo i pesi, non l'intero oggetto)\n",
    "model_path = \"./best_multitask_extractor_laptop/pytorch_model.bin\"\n",
    "model_step1.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "model_step1.to(device)\n",
    "model_step1.eval() # Modalit√† esame (spegne dropout)\n",
    "\n",
    "# --- B. PREPARAZIONE METRICHE ---\n",
    "# Carichiamo la metrica seqeval (standard per NER/ABSA)\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Mappa per decodificare i numeri in etichette\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"\\nInizio Test Multi-Task sul Dataset Laptop...\")\n",
    "\n",
    "# --- C. CICLO DI PREVISIONE ---\n",
    "predictions_tokens = []\n",
    "true_labels_tokens = []\n",
    "\n",
    "# Liste per salvare le predizioni binarie (Impliciti)\n",
    "true_imp_asp, pred_imp_asp = [], []\n",
    "true_imp_opi, pred_imp_opi = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_laptop, desc=\"Test LAPTOP\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "\n",
    "        # 1. Il modello fa le sue 3 predizioni contemporaneamente\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model_step1(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 2. Estraiamo i risultati dalle 3 testoline\n",
    "        token_preds = torch.argmax(outputs['token_logits'], dim=-1)\n",
    "        asp_preds = torch.argmax(outputs['imp_asp_logits'], dim=-1)\n",
    "        opi_preds = torch.argmax(outputs['imp_opi_logits'], dim=-1)\n",
    "        \n",
    "        # Salviamo i risultati binari\n",
    "        true_imp_asp.extend(imp_asp_labels.cpu().tolist())\n",
    "        pred_imp_asp.extend(asp_preds.cpu().tolist())\n",
    "        \n",
    "        true_imp_opi.extend(imp_opi_labels.cpu().tolist())\n",
    "        pred_imp_opi.extend(opi_preds.cpu().tolist())\n",
    "\n",
    "        # 3. Convertiamo i numeri in etichette BIO (pulendo il padding)\n",
    "        for i in range(len(labels)):\n",
    "            true_label_row = []\n",
    "            pred_label_row = []\n",
    "            \n",
    "            for j in range(len(labels[i])):\n",
    "                # Ignoriamo i token di padding (dove attention_mask √® 0 o la label √® -100)\n",
    "                if labels[i][j] != -100 and attention_mask[i][j] == 1: \n",
    "                    true_label_row.append(id2label[labels[i][j].item()])\n",
    "                    pred_label_row.append(id2label[token_preds[i][j].item()])\n",
    "            \n",
    "            true_labels_tokens.append(true_label_row)\n",
    "            predictions_tokens.append(pred_label_row)\n",
    "\n",
    "# --- D. CALCOLO E STAMPA RISULTATI ---\n",
    "results_seq = metric.compute(predictions=predictions_tokens, references=true_labels_tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Overall Precision: {results_seq['overall_precision']:.4f}\")\n",
    "print(f\"Overall Recall:    {results_seq['overall_recall']:.4f}\")\n",
    "print(f\"Overall F1-Score:  {results_seq['overall_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nDettaglio per Classe (Quello che conta per il paper):\")\n",
    "print(\"-\" * 50)\n",
    "for key in results_seq.keys():\n",
    "    if key in ['ASP', 'OPI']: \n",
    "        print(f\"   {key}:\")\n",
    "        print(f\"   Precision: {results_seq[key]['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {results_seq[key]['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {results_seq[key]['f1']:.4f}\")\n",
    "        print(f\"   Support:   {results_seq[key]['number']}\") \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metriche per Aspetti Impliciti\n",
    "acc_asp = accuracy_score(true_imp_asp, pred_imp_asp)\n",
    "print(f\"üîπ Accuratezza Aspetti Impliciti: {acc_asp:.4f}\")\n",
    "print(classification_report(true_imp_asp, pred_imp_asp, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Metriche per Opinioni Implicite\n",
    "acc_opi = accuracy_score(true_imp_opi, pred_imp_opi)\n",
    "print(f\"üîπ Accuratezza Opinioni Implicite: {acc_opi:.4f}\")\n",
    "print(classification_report(true_imp_opi, pred_imp_opi, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9758056",
   "metadata": {},
   "source": [
    "## Classificatore Category-Sentiment (Extract-Classify-ACOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b648b",
   "metadata": {},
   "source": [
    "Implementiamo il **secondo stadio** dell'architettura proposta nel paper. Dopo aver estratto gli Aspetti e le Opinioni nello Step 1, ora dobbiamo capire a quale Categoria appartengono e qual √® il loro Sentiment.\n",
    "\n",
    "Il codice di preparazione √® diviso in tre componenti fondamentali:\n",
    "\n",
    " 1. Il Dataset PyTorch (`ACOSPairDataset`)\n",
    "\n",
    " 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "\n",
    " 3. Inizializzazione e DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4462143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACOSPairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        \n",
    "        # Gli span sono gi√† corretti con il +1 per il [CLS]!\n",
    "        a_span = row['aspect_span']\n",
    "        o_span = row['opinion_span']\n",
    "\n",
    "        # Tokenizzazione (ModernBERT usa il token [CLS] in automatico)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'aspect_span': torch.tensor(a_span),\n",
    "            'opinion_span': torch.tensor(o_span),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364034ae",
   "metadata": {},
   "source": [
    "### 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "Questa √® la vera \"magia\" matematica del paper, tradotta in codice:\n",
    "* **Il Corpo (Backbone):** Invece di partire da zero, carichiamo il *corpo* del modello che hai gi√† addestrato nello Step 1 (`best_model_laptop`). In questo modo, la rete conosce gi√† il dominio tecnico dei computer!\n",
    "* **Span Pooling:** Il modello estrae i vettori (hidden states) corrispondenti alle parole dell'Aspetto e dell'Opinione e ne calcola la media. Se un elemento √® implicito (`-1`), pesca automaticamente il vettore globale del token `[CLS]`.\n",
    "* **Feature Fusion:** Concatena il vettore dell'aspetto ($u_a$) e dell'opinione ($u_o$) in un unico grande vettore di dimensione 1536.\n",
    "* **Le 121 Teste (Multiple Multi-class):** Passa questo vettore in 121 classificatori lineari paralleli. Ognuno di essi decider√† se per la *sua* categoria la coppia √® `Positive (0)`, `Negative (1)`, `Neutral (2)` o `Invalid (3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBertACOSClassifier(nn.Module):\n",
    "    def __init__(self, path_to_best_model, num_categories):\n",
    "        super(ModernBertACOSClassifier, self).__init__()\n",
    "        \n",
    "        # Carichiamo SOLO IL CORPO dal tuo modello dello Step 1\n",
    "        self.modernbert = AutoModel.from_pretrained(path_to_best_model)\n",
    "        hidden_size = self.modernbert.config.hidden_size # 768\n",
    "        \n",
    "        # Le 121 teste (ognuna prende il vettore concatenato 1536 e sputa 4 classi)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size * 2, 4) for _ in range(num_categories)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, aspect_spans, opinion_spans):\n",
    "        outputs = self.modernbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state # [Batch, Seq_Len, 768]\n",
    "        \n",
    "        batch_size = last_hidden_state.size(0)\n",
    "        u_a_list, u_o_list = [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Pooling Aspetto\n",
    "            a_start, a_end = aspect_spans[i]\n",
    "            if a_start == -1: \n",
    "                u_a = last_hidden_state[i, 0, :] # Token [CLS]\n",
    "            else:\n",
    "                u_a = last_hidden_state[i, a_start:a_end, :].mean(dim=0)\n",
    "            \n",
    "            # Pooling Opinione\n",
    "            o_start, o_end = opinion_spans[i]\n",
    "            if o_start == -1: \n",
    "                u_o = last_hidden_state[i, 0, :] # Token [CLS]\n",
    "            else:\n",
    "                u_o = last_hidden_state[i, o_start:o_end, :].mean(dim=0)\n",
    "            \n",
    "            u_a_list.append(u_a)\n",
    "            u_o_list.append(u_o)\n",
    "\n",
    "        # Concatenazione: [u_a ; u_o]\n",
    "        combined_features = torch.cat((torch.stack(u_a_list), torch.stack(u_o_list)), dim=-1)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "\n",
    "        # Passiamo il vettore nelle teste lineari\n",
    "        logits = [head(combined_features) for head in self.heads]\n",
    "        \n",
    "        # Output: [Batch, 121, 4]\n",
    "        return torch.stack(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d70d3e",
   "metadata": {},
   "source": [
    "### 3. Inizializzazione e DataLoaders\n",
    "L'ultimo blocco carica fisicamente i file salvati dalla nostra \"Fabbrica dei Dati\", istanzia i `Dataset`, e crea i `DataLoader` (con batch size = 16) per \"nutrire\" la GPU in modo efficiente durante l'addestramento. Infine, sposta il modello sulla scheda video (CUDA) pronto per il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103aad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47eeaa705dc4ea0b44b5761d95d5f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_laptop\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello inizializzato! Categorie: 121 | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Carichiamo la lista delle categorie salvata prima\n",
    "with open(\"data_coppie/laptop_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list) # 121\n",
    "\n",
    "# 2. Carichiamo i DataFrame di Train e Dev\n",
    "df_train = pd.read_pickle(\"data_coppie/train_laptop_pairs.pkl\")\n",
    "df_dev = pd.read_pickle(\"data_coppie/dev_laptop_pairs.pkl\")\n",
    "df_test = pd.read_pickle(\"data_coppie/test_laptop_pairs.pkl\") # <-- AGGIUNTO!\n",
    "\n",
    "# 3. Inizializziamo il Tokenizer e i Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "train_dataset = ACOSPairDataset(df_train, tokenizer)\n",
    "dev_dataset = ACOSPairDataset(df_dev, tokenizer)\n",
    "test_dataset = ACOSPairDataset(df_test, tokenizer) # <-- AGGIUNTO!\n",
    "\n",
    "# 4. Creiamo i DataLoader (Batch size 16 √® un buon compromesso tra velocit√† e VRAM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dev_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16) # <-- AGGIUNTO!\n",
    "\n",
    "# 5. Inizializziamo il Modello usando i pesi dello Step 1!\n",
    "model = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modello inizializzato! Categorie: {num_categories} | Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca5086",
   "metadata": {},
   "source": [
    "## WANDB per il Monitoraggio dello Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95073d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cristinatomaciello/Desktop/UniversitaÃÄ/2anno/1semstre/big data/progetto-text-mining/wandb/run-20260222_131310-zvck79eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/zvck79eq' target=\"_blank\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/zvck79eq' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/zvck79eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS\n"
     ]
    }
   ],
   "source": [
    "# Chiudiamo per sicurezza qualsiasi run precedente rimasta aperta nello stesso notebook\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters per lo STEP 2\n",
    "config_step2 = {\n",
    "    \"learning_rate\": 2e-5, # Solitamente per lo Step 2 un LR leggermente pi√π basso √® meglio (es. 2e-5)\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 4, # Aggiunto per il tuo training loop ottimizzato!\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\", \n",
    "    \"seed\": 42,\n",
    "    \"patience\": 2  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run per lo Step 2\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config_step2,\n",
    "    # Aggiungiamo \"Step2_Class\" al nome per distinguerlo dallo Step 1\n",
    "    name=f\"Step2_Class_{config_step2['model_name']}_{config_step2['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\" W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6178aa",
   "metadata": {},
   "source": [
    "## Training e valutazione del modello su Sentiment e Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf680e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training STEP 2 su LAPTOP: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti ogni 4 step | FP16 Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1:   1%|          | 3/342 [00:00<01:37,  3.49it/s, loss=1.73]/home/al3th3ia/Scrivania/Cristina/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:17<00:00,  4.40it/s, loss=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0917 | Valid Loss: 0.8533\n",
      "Miglior modello trovato (Loss: 0.8533)! Salvataggio...\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:19<00:00,  4.33it/s, loss=0.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7580 | Valid Loss: 0.7928\n",
      "Miglior modello trovato (Loss: 0.7928)! Salvataggio...\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:19<00:00,  4.32it/s, loss=0.575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6127 | Valid Loss: 0.7131\n",
      "Miglior modello trovato (Loss: 0.7131)! Salvataggio...\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:18<00:00,  4.38it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4775 | Valid Loss: 0.6727\n",
      "Miglior modello trovato (Loss: 0.6727)! Salvataggio...\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:20<00:00,  4.27it/s, loss=0.31] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3668 | Valid Loss: 0.6500\n",
      "Miglior modello trovato (Loss: 0.6500)! Salvataggio...\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:18<00:00,  4.35it/s, loss=0.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2960 | Valid Loss: 0.6262\n",
      "Miglior modello trovato (Loss: 0.6262)! Salvataggio...\n",
      "\n",
      "--- Epoca 7/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:19<00:00,  4.32it/s, loss=0.165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2228 | Valid Loss: 0.6176\n",
      "Miglior modello trovato (Loss: 0.6176)! Salvataggio...\n",
      "\n",
      "--- Epoca 8/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:22<00:00,  4.16it/s, loss=0.185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1747 | Valid Loss: 0.6212\n",
      "Nessun miglioramento. Patience: 1/2\n",
      "\n",
      "--- Epoca 9/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 342/342 [01:19<00:00,  4.28it/s, loss=0.286] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1428 | Valid Loss: 0.6527\n",
      "Nessun miglioramento. Patience: 2/2\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 9.\n",
      "\n",
      "Fine Addestramento Step 2.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>‚ñá‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>valid_loss_epoch</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.28619</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss_epoch</td><td>0.14278</td></tr><tr><td>valid_loss_epoch</td><td>0.65272</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xl7a5g33' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/xl7a5g33</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260221_145847-xl7a5g33/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "\n",
    "# Gradient Checkpointing: abilitato SOLO sul corpo di ModernBERT\n",
    "model.modernbert.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config_step2.get('accumulation_steps', 4) \n",
    "patience = config_step2.get('patience', 2)\n",
    "patience_counter = 0\n",
    "\n",
    "# Ottimizzatore AdamW a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config_step2['learning_rate']\n",
    ")\n",
    "\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "# La Loss per le 121 teste\n",
    "weights = torch.tensor([150.0, 400.0, 100.0, 1.0]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Calcolo degli step totali per lo scheduler\n",
    "total_steps = (len(train_loader) // accumulation_steps) * config_step2['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (STEP 2) ---\n",
    "\n",
    "def evaluate_model_step2(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            aspect_span = batch['aspect_span'].to(device)\n",
    "            opinion_span = batch['opinion_span'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast anche in valutazione\n",
    "            with autocast(device_type='cuda'):\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                               aspect_spans=aspect_span, opinion_spans=opinion_span)\n",
    "                \n",
    "                # Calcolo Loss su tutte le teste\n",
    "                loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_step2(model, data_loader, optimizer, scheduler, criterion, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        aspect_span = batch['aspect_span'].to(device)\n",
    "        opinion_span = batch['opinion_span'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass\n",
    "        with autocast(device_type='cuda'):\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                           aspect_spans=aspect_span, opinion_spans=opinion_span)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            # Dividiamo la loss per gli step di accumulo\n",
    "            loss = loss / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Ricalcoliamo il valore reale della loss per i log\n",
    "        real_loss = loss.item() * accumulation_steps\n",
    "        total_loss += real_loss\n",
    "        \n",
    "        wandb.log({\"batch_loss\": real_loss})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=real_loss)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"Training STEP 2 su LAPTOP: {config_step2['epochs']} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "\n",
    "for epoch in range(config_step2['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config_step2['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch_step2(\n",
    "        model, train_loader, optimizer, scheduler, criterion, \n",
    "        device, epoch, scaler, accumulation_steps\n",
    "    )\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model_step2(model, val_loader, criterion, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_laptop:.4f} | Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})! Salvataggio...\")\n",
    "        \n",
    "        # Salvataggio custom model (Solo i pesi)\n",
    "        save_dir = \"./best_classifier_laptop\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Step 2.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4c16a",
   "metadata": {},
   "source": [
    "## Test su sentiment e categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a16f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/5pbn9c97' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/5pbn9c97</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260221_162933-5pbn9c97/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260221_162941-14l82vg4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/14l82vg4' target=\"_blank\">TEST_Step2_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/14l82vg4' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/14l82vg4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio Test sul Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b4647211b244c18fbb52f27f527d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_laptop\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CHECK CARICAMENTO PESI ---\n",
      "Chiavi Inaspettate (OK se sono dello Step 1): 0\n",
      "Chiavi Mancanti (PROBLEMA se sono 'heads'): 0\n",
      "SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\n",
      "Estrazione di tutte le probabilit√† dal modello in corso (attendere)...\n",
      "\n",
      "üîç Avvio Grid Search per la migliore Soglia (MACRO F1)...\n",
      "IL VINCITORE √à... THRESHOLD A 0.80 (80%)\n",
      "\n",
      "--- MIGLIOR CLASSIFICATION REPORT (Basato su Macro F1) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Positive (0)       0.16      0.21      0.18       379\n",
      "Negative (1)       0.27      0.05      0.08        65\n",
      " Neutral (2)       0.26      0.46      0.33       712\n",
      "\n",
      "   micro avg       0.23      0.35      0.28      1156\n",
      "   macro avg       0.23      0.24      0.20      1156\n",
      "weighted avg       0.23      0.35      0.27      1156\n",
      "\n",
      "BEST MACRO F1-Score: 0.1969\n",
      "CORRISPONDENTE MICRO F1: 0.2795\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>‚ñÅ</td></tr><tr><td>best_test_micro_f1</td><td>‚ñÅ</td></tr><tr><td>optimal_threshold</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>0.19692</td></tr><tr><td>best_test_micro_f1</td><td>0.27948</td></tr><tr><td>optimal_threshold</td><td>0.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TEST_Step2_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/14l82vg4' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/14l82vg4</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260221_162941-14l82vg4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chiudiamo run appese\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    # Chiamo la run con il prefisso \"TEST_\"\n",
    "    name=f\"TEST_Step2_{config_step2['model_name']}_{config_step2['dataset']}\",\n",
    "    job_type=\"test\"\n",
    ")\n",
    "\n",
    "# --- 1. PREPARAZIONE ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Avvio Test sul Device: {device}\")\n",
    "\n",
    "# Ricreiamo l'architettura del modello\n",
    "num_categories = len(category_list)\n",
    "model_test = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories)\n",
    "\n",
    "# Carichiamo i pesi dello Step 2\n",
    "model_path = \"./best_classifier_laptop/pytorch_model.bin\"\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# USIAMO STRICT=FALSE\n",
    "missing_keys, unexpected_keys = model_test.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"\\n--- CHECK CARICAMENTO PESI ---\")\n",
    "print(f\"Chiavi Inaspettate (OK se sono dello Step 1): {len(unexpected_keys)}\")\n",
    "print(f\"Chiavi Mancanti (PROBLEMA se sono 'heads'): {len(missing_keys)}\")\n",
    "\n",
    "# Verifica specifica sulle teste\n",
    "heads_missing = [k for k in missing_keys if \"heads\" in k]\n",
    "if heads_missing:\n",
    "    print(f\"ERRORE CRITICO: Le teste di classificazione non sono state caricate! {heads_missing[:5]}\")\n",
    "else:\n",
    "    print(\"SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\")\n",
    "    \n",
    "model_test.to(device)\n",
    "model_test.eval()\n",
    "\n",
    "# --- 2. ESTRAZIONE PROBABILITA'---\n",
    "print(\"Estrazione di tutte le probabilit√† dal modello in corso (attendere)...\")\n",
    "\n",
    "all_probs_list = []\n",
    "all_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader: # (Usa test_loader_rest per i ristoranti!)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        aspect_span = batch['aspect_span'].to(device)\n",
    "        opinion_span = batch['opinion_span'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            logits = model_test(input_ids, attention_mask, aspect_span, opinion_span)\n",
    "            \n",
    "        # Trasformiamo i logits in probabilit√† e li portiamo su CPU per non intasarla\n",
    "        probs = torch.softmax(logits, dim=-1).cpu() \n",
    "        \n",
    "        all_probs_list.append(probs)\n",
    "        all_true_list.append(labels.cpu())\n",
    "\n",
    "# Uniamo tutti i batch in un unico grande blocco di memoria\n",
    "all_probs = torch.cat(all_probs_list, dim=0) # Forma: [Tot_Frasi, 121, 4]\n",
    "all_true = torch.cat(all_true_list, dim=0).numpy().flatten()\n",
    "\n",
    "\n",
    "# --- 3. GRID SEARCH SUL THRESHOLD (OTTIMIZZATA PER MACRO F1) ---\n",
    "print(\"\\nüîç Avvio Grid Search per la migliore Soglia (MACRO F1)...\")\n",
    "\n",
    "# SCENDIAMO FINO A 0.30! Diamo al modello la possibilit√† di essere meno sicuro.\n",
    "thresholds_to_test = np.arange(0.40, 0.99, 0.02) \n",
    "best_micro_f1 = 0.0\n",
    "best_macro_f1 = 0.0\n",
    "best_threshold = 0.0\n",
    "best_report = \"\"\n",
    "\n",
    "target_names = ['Positive (0)', 'Negative (1)', 'Neutral (2)']\n",
    "labels_to_eval = [0, 1, 2]\n",
    "\n",
    "# INIZIO CICLO SILENZIOSO\n",
    "for thresh in thresholds_to_test:\n",
    "    valid_class_probs, valid_class_preds = torch.max(all_probs[:, :, :3], dim=-1)\n",
    "    final_preds = torch.full_like(valid_class_preds, 3)\n",
    "    \n",
    "    mask = valid_class_probs > thresh\n",
    "    final_preds[mask] = valid_class_preds[mask]\n",
    "    \n",
    "    preds_flat = final_preds.numpy().flatten()\n",
    "    \n",
    "    current_micro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='micro')\n",
    "    current_macro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='macro')\n",
    "    \n",
    "    # ORA VINCE CHI ALZA IL MACRO F1!\n",
    "    if current_macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = current_macro_f1\n",
    "        best_micro_f1 = current_micro_f1\n",
    "        best_threshold = thresh\n",
    "        best_report = classification_report(\n",
    "            all_true, preds_flat, labels=labels_to_eval, \n",
    "            target_names=target_names, zero_division=0\n",
    "        )\n",
    "\n",
    "# --- 4. STAMPA DEI RISULTATI VINCITORI E LOG W&B ---\n",
    "print(f\"IL VINCITORE √à... THRESHOLD A {best_threshold:.2f} ({(best_threshold*100):.0f}%)\")\n",
    "\n",
    "print(\"\\n--- MIGLIOR CLASSIFICATION REPORT (Basato su Macro F1) ---\")\n",
    "print(best_report)\n",
    "print(f\"BEST MACRO F1-Score: {best_macro_f1:.4f}\")\n",
    "print(f\"CORRISPONDENTE MICRO F1: {best_micro_f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Salviamo su Weights & Biases\n",
    "wandb.log({\n",
    "    \"best_test_micro_f1\": best_micro_f1,\n",
    "    \"best_test_macro_f1\": best_macro_f1,\n",
    "    \"optimal_threshold\": best_threshold\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92443f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "url = \"data_coppie/test_laptop_pairs.pkl\"\n",
    "df = pd.read_pickle(url)\n",
    "df.sample(5)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# 1. Estraiamo tutti i singoli valori dalle liste della colonna 'labels'\n",
    "all_labels = []\n",
    "for row in df['labels']:\n",
    "    # Se i dati sono stringhe, usiamo ast.literal_eval per convertirli in liste\n",
    "    if isinstance(row, str):\n",
    "        l = ast.literal_eval(row)\n",
    "    else:\n",
    "        l = row\n",
    "    all_labels.extend(l)\n",
    "\n",
    "# 2. Contiamo le occorrenze di ogni classe\n",
    "unique, counts = np.unique(all_labels, return_counts=True)\n",
    "label_map = {0: 'Positive (0)', 1: 'Negative (1)', 2: 'Neutral (2)', 3: 'Invalid (3)'}\n",
    "names = [label_map.get(x, f\"Classe {x}\") for x in unique]\n",
    "\n",
    "# 3. Creazione del Grafico\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#2ca02c', '#d62728', '#1f77b4', '#7f7f7f'] # Verde, Rosso, Blu, Grigio\n",
    "bars = plt.bar(names, counts, color=colors[:len(names)])\n",
    "\n",
    "# Aggiungiamo i valori numerici sopra ogni barra\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + (max(counts)*0.01), \n",
    "             int(yval), ha='center', fontweight='bold')\n",
    "\n",
    "plt.title('Distribuzione delle Etichette (Step 2: Sentiment)', fontsize=14)\n",
    "plt.ylabel('Frequenza (Numero di occorrenze)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
