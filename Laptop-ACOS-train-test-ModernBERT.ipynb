{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7266c47e",
   "metadata": {},
   "source": [
    "# Classificazione di Aspect e Opinion con ModernBERT su Laptop-ACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a40e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie caricate.\n",
      " Acceleratore Apple Metal (MPS) Trovato\n"
     ]
    }
   ],
   "source": [
    "# Import delle librerie necessarie\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup, AutoModel\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb #per gpu invidia\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.amp import autocast, GradScaler # Per Mixed Precision\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"Librerie caricate.\")\n",
    "\n",
    "# --- 1. CONFIGURAZIONE DEL DEVICE ---\n",
    "# Se hai una GPU NVIDIA, userà 'cuda'. Se hai un Mac M1/M2, userà 'mps'. Altrimenti 'cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\" GPU Trovata: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Acceleratore Apple Metal (MPS) Trovato\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Nessuna GPU trovata. L'addestramento sarà lento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70bed1",
   "metadata": {},
   "source": [
    "### Impostazioni per la riproducibilità "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds impostati su 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Imposto i seed per la riproducibilità.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        # Imposto anche i seed per la GPU, se disponibile\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Esegui l'impostazione del seed\n",
    "set_seed(42) \n",
    "print(\"Random seeds impostati su 42.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb63eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260225_144832-dy14ku66</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/dy14ku66' target=\"_blank\">Step1_Class_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/dy14ku66' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/dy14ku66</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B inizializzato per il progetto: BigData-TextMining-ACOS\n"
     ]
    }
   ],
   "source": [
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\",\n",
    "    \"seed\": 42,\n",
    "    'patience': 5 # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config,\n",
    "    name=f\"Step1_Class_{config['model_name']}_{config['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\"W&B inizializzato per il progetto: {wandb.run.project}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84319964",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bba6",
   "metadata": {},
   "source": [
    "### Creazione di PyTorch Dataset e DataLoader\n",
    "Questa cella si occupa di caricare i dati pre-processati e di \"impacchettarli\" nel formato esatto richiesto dalla nostra nuova architettura PyTorch personalizzata. Rappresenta un passaggio cruciale per replicare fedelmente il paper originale, permettendoci di gestire gli elementi impliciti.\n",
    "\n",
    "Nello specifico, il codice esegue tre operazioni fondamentali:\n",
    "\n",
    "1. **Caricamento dei DataFrame:** Legge i file `.pkl` (Train, Dev e Test per il dominio Laptop) che abbiamo precedentemente aggiornato. Questi file ora contengono le annotazioni binarie che indicano la presenza di aspetti o opinioni implicite.\n",
    "2. **Definizione della Classe Custom `ACOSDataset`:** Questa è la modifica principale rispetto a una pipeline standard di HuggingFace. Invece di restituire solo i classici 3 tensori della Token Classification (`input_ids`, `attention_mask` e `labels` con i tag BIO), questa classe sovrascritta restituisce **5 tensori** per ogni frase. Vengono infatti estratti e passati al modello anche `implicit_aspect_labels` e `implicit_opinion_labels`. Questi tensori serviranno ad addestrare in parallelo le due teste di classificazione binaria sul token `[CLS]`.\n",
    "3. **Configurazione dei DataLoader:** Crea gli iteratori di PyTorch che alimenteranno il modello durante l'addestramento e il test, processando blocchi (batch) di 16 frasi alla volta. I dati di training vengono rimescolati (`shuffle=True`) per stabilizzare l'apprendimento della rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae7f7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dei dataset pre-processati aggiornati...\n",
      "Dataset e DataLoaders creati con successo!\n",
      "Esempi nel set di Training LAPTOP: 2934\n"
     ]
    }
   ],
   "source": [
    "cartella_dati = \"../data_allineati\"\n",
    "\n",
    "print(\"Caricamento dei dataset pre-processati aggiornati...\")\n",
    "df_train_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"train_laptop_aligned.pkl\"))\n",
    "df_dev_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"dev_laptop_aligned.pkl\"))\n",
    "df_test_align_laptop = pd.read_pickle(os.path.join(cartella_dati, \"test_laptop_aligned.pkl\"))\n",
    "\n",
    "class ACOSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "        # Estraiamo le colonne per gli impliciti!\n",
    "        self.implicit_aspect_label = df['implicit_aspect_label'].tolist()\n",
    "        self.implicit_opinion_label = df['implicit_opinion_label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            # Passiamo le etichette al Dataloader\n",
    "            'implicit_aspect_labels': torch.tensor(self.implicit_aspect_label[idx], dtype=torch.long),\n",
    "            'implicit_opinion_labels': torch.tensor(self.implicit_opinion_label[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- CREAZIONE DELLE ISTANZE ---\n",
    "\n",
    "# Creiamo i dataset per il dominio Laptop\n",
    "train_dataset_laptop = ACOSDataset(df_train_align_laptop)\n",
    "dev_dataset_laptop = ACOSDataset(df_dev_align_laptop)\n",
    "test_dataset_laptop = ACOSDataset(df_test_align_laptop)\n",
    "\n",
    "\n",
    "# --- CONFIGURAZIONE DATALOADERS ---\n",
    "\n",
    "BATCH_SIZE = 16 # Numero di frasi analizzate contemporaneamente\n",
    "\n",
    "train_loader_laptop = DataLoader(train_dataset_laptop, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_laptop = DataLoader(dev_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "test_loader_laptop = DataLoader(test_dataset_laptop, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dataset e DataLoaders creati con successo!\")\n",
    "print(f\"Esempi nel set di Training LAPTOP: {len(train_dataset_laptop)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70053",
   "metadata": {},
   "source": [
    "### Architettura Multi-Task: ModernBERT ACOS Extractor\n",
    "\n",
    "In questa cella abbandoniamo l'architettura standard di Token Classification per costruire un modello personalizzato in PyTorch, progettato per replicare fedelmente la logica di estrazione del paper ACOS originale.\n",
    "\n",
    "Il problema dei modelli tradizionali è che falliscono quando un aspetto o un'opinione non sono scritti esplicitamente nel testo (elementi *Impliciti*). Per risolvere questa criticità, abbiamo progettato una rete **Multi-Task** composta da un \"cervello\" centrale (l'encoder ModernBERT) e **tre teste di classificazione indipendenti**:\n",
    "\n",
    "1. **Testa di Token Classification (Estrazione Esplicita):** Analizza ogni singola parola della frase per assegnare i tag BIO (B-ASP, I-ASP, B-OPI, I-OPI, O), estraendo gli span di testo espliciti.\n",
    "2. **Testa per Aspetti Impliciti (Classificazione Binaria):** Sfrutta il token speciale `[CLS]`, che racchiude il significato globale della frase, per prevedere matematicamente (Sì/No) se la recensione contiene un aspetto sottinteso.\n",
    "3. **Testa per Opinioni Implicite (Classificazione Binaria):** Sfrutta sempre il token `[CLS]` per indovinare se c'è un'opinione sottintesa.\n",
    "\n",
    "**La Loss Combinata (L'addestramento simultaneo)**\n",
    "Il vero \"motore\" di questa classe è nella funzione `forward`. Durante l'addestramento, il modello calcola contemporaneamente tre errori separati (uno per l'estrazione e due per le previsioni binarie degli impliciti). Questi tre errori vengono **sommati in un'unica Loss globale**. In questo modo, la rete neurale viene forzata a imparare tutti i task simultaneamente, ottimizzando i pesi interni per comprendere a fondo sia ciò che è scritto, sia ciò che è sottinteso.\n",
    "\n",
    "Infine, il modello viene caricato sulla GPU e accoppiato a un ottimizzatore **AdamW a 8-bit** (`bitsandbytes`) per massimizzare l'efficienza e prevenire l'esaurimento della memoria VRAM durante le epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5b686c55f24052a5aa1fbe40f30514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODELLO MULTI-TASK PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base (Custom ACOS Extractor)\n",
      "Task: Token Classification + 2x Binary Classification (Impliciti)\n",
      "Numero di Classi Token: 5\n",
      "Optimizer: AdamW 8-bit (lr=2e-5)\n",
      "Loss Function: Loss Combinata (calcolata internamente)\n",
      "Device: mps\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Definiamo le 5 etichette: 0=O, 1=B-ASP, 2=I-ASP, 3=B-OPI, 4=I-OPI\n",
    "NUM_LABELS = 5 \n",
    "\n",
    "class ModernBertACOS_Extractor(nn.Module):\n",
    "    def __init__(self, model_name=\"answerdotai/ModernBERT-base\", num_labels=5):\n",
    "        super().__init__()\n",
    "        # Carichiamo la \"schiena\" del modello (l'encoder base)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Testolina 1: Trova le parole scritte (Token Classification)\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "        # Testoline 2 e 3: Indovinano se ci sono impliciti (Classificazione Binaria)\n",
    "        self.implicit_aspect_classifier = nn.Linear(hidden_size, 2)\n",
    "        self.implicit_opinion_classifier = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, implicit_aspect_labels=None, implicit_opinion_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        \n",
    "        # Prendiamo il token [CLS] (posizione 0) per i classificatori binari\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "        \n",
    "        # Le tre testoline fanno le loro previsioni\n",
    "        token_logits = self.token_classifier(sequence_output)\n",
    "        imp_asp_logits = self.implicit_aspect_classifier(cls_output)\n",
    "        imp_opi_logits = self.implicit_opinion_classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        # Calcolo della \"Loss Combinata\" durante il training\n",
    "        if labels is not None and implicit_aspect_labels is not None and implicit_opinion_labels is not None:\n",
    "            # Estraiamo il device (CUDA o CPU) per creare i tensori dei pesi nel posto giusto\n",
    "            device = input_ids.device\n",
    "            \n",
    "            # --- NOVITÀ 1: Pesi per i Token (Lotta allo sbilanciamento delle 'O') ---\n",
    "            # Indice 0 ('O') ha peso 1.0. \n",
    "            # Gli indici 1, 2, 3, 4 (Aspetti e Opinioni) hanno peso 10.0! Sbagliarli costa carissimo.\n",
    "            token_weights = torch.tensor([1.0, 10.0, 10.0, 10.0, 10.0], device=device)\n",
    "            loss_fct_token = nn.CrossEntropyLoss(weight=token_weights)\n",
    "            \n",
    "            # --- NOVITÀ 2: Pesi per gli Impliciti ---\n",
    "            # Classe 0 (Esplicito) ha peso 1.0. Classe 1 (Implicito) ha peso 5.0.\n",
    "            # Diciamo al modello: \"Non ignorare gli impliciti solo perché sono rari!\"\n",
    "            implicit_weights = torch.tensor([1.0, 5.0], device=device)\n",
    "            loss_fct_implicit = nn.CrossEntropyLoss(weight=implicit_weights)\n",
    "            \n",
    "            # Loss 1: Token (ignorando il padding -100)\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = token_logits.view(-1, 5)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss_token = loss_fct_token(active_logits, active_labels)\n",
    "            \n",
    "            # Loss 2 & 3: Impliciti\n",
    "            loss_asp = loss_fct_implicit(imp_asp_logits, implicit_aspect_labels)\n",
    "            loss_opi = loss_fct_implicit(imp_opi_logits, implicit_opinion_labels)\n",
    "            \n",
    "            # --- NOVITÀ 3: Moltiplicatori della Loss Multi-Task ---\n",
    "            loss = loss_token + (1.5 * loss_asp) + (2.0 * loss_opi)\n",
    "            \n",
    "        return {\n",
    "            \"loss\": loss, \n",
    "            \"token_logits\": token_logits, \n",
    "            \"imp_asp_logits\": imp_asp_logits, \n",
    "            \"imp_opi_logits\": imp_opi_logits\n",
    "        }\n",
    "\n",
    "print(\"Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\")\n",
    "\n",
    "# Inizializziamo il nostro modello custom invece di AutoModelForTokenClassification\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=NUM_LABELS)\n",
    "\n",
    "# Spostiamo il modello sul dispositivo di calcolo (GPU/MPS/CPU)\n",
    "model_step1.to(device)\n",
    "\n",
    "# Manteniamo la tua ottima scelta di usare l'optimizer a 8-bit per non saturare la memoria!\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELLO MULTI-TASK PRONTO PER IL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architettura: ModernBERT-base (Custom ACOS Extractor)\")\n",
    "print(f\"Task: Token Classification + 2x Binary Classification (Impliciti)\")\n",
    "print(f\"Numero di Classi Token: {NUM_LABELS}\")\n",
    "print(f\"Optimizer: AdamW 8-bit (lr=2e-5)\")\n",
    "print(f\"Loss Function: Loss Combinata (calcolata internamente)\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c723566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attivazione Ottimizzazioni di Memoria...\n",
      "\n",
      "Training Multi-Task su LAPTOP: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti: ogni 4 step | FP16: Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1:   2%|▏         | 3/184 [00:00<00:51,  3.49it/s, loss=5.27]/home/al3th3ia/Scrivania/Cristina/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoca 1: 100%|██████████| 184/184 [00:37<00:00,  4.90it/s, loss=3.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.7196 | Valid Loss: 3.2787\n",
      "Miglior modello trovato (Loss: 3.2787)\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|██████████| 184/184 [00:36<00:00,  5.01it/s, loss=3.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7601 | Valid Loss: 2.6623\n",
      "Miglior modello trovato (Loss: 2.6623)\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|██████████| 184/184 [00:36<00:00,  4.99it/s, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2679 | Valid Loss: 2.3294\n",
      "Miglior modello trovato (Loss: 2.3294)\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|██████████| 184/184 [00:37<00:00,  4.94it/s, loss=2.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8967 | Valid Loss: 1.9571\n",
      "Miglior modello trovato (Loss: 1.9571)\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|██████████| 184/184 [00:37<00:00,  4.92it/s, loss=0.969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4737 | Valid Loss: 1.7447\n",
      "Miglior modello trovato (Loss: 1.7447)\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|██████████| 184/184 [00:37<00:00,  4.95it/s, loss=1.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1551 | Valid Loss: 1.8408\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 7/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 7: 100%|██████████| 184/184 [00:37<00:00,  4.95it/s, loss=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8446 | Valid Loss: 2.0665\n",
      "Nessun miglioramento. Patience: 2/5\n",
      "\n",
      "--- Epoca 8/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 8: 100%|██████████| 184/184 [00:37<00:00,  4.95it/s, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5937 | Valid Loss: 2.4040\n",
      "Nessun miglioramento. Patience: 3/5\n",
      "\n",
      "--- Epoca 9/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 9: 100%|██████████| 184/184 [00:37<00:00,  4.95it/s, loss=0.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4311 | Valid Loss: 2.8616\n",
      "Nessun miglioramento. Patience: 4/5\n",
      "\n",
      "--- Epoca 10/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 10: 100%|██████████| 184/184 [00:37<00:00,  4.95it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3280 | Valid Loss: 2.9815\n",
      "Nessun miglioramento. Patience: 5/5\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 10.\n",
      "\n",
      "Fine Addestramento Multi-Task per LAPTOP.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▇▅▅▅▄▃▄▄▄▃▃▃▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>valid_loss_epoch</td><td>█▅▄▂▁▁▂▄▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.3045</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss_epoch</td><td>0.328</td></tr><tr><td>valid_loss_epoch</td><td>2.98155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step1_Class_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6xqju6vx' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6xqju6vx</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260223_154840-6xqju6vx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA E WANDB ---\n",
    "print(\"Attivazione Ottimizzazioni di Memoria...\")\n",
    "\n",
    "# Gradient Checkpointing: si applica all'encoder interno (BERT) della nostra classe custom\n",
    "model_step1.bert.gradient_checkpointing_enable()\n",
    "\n",
    "accumulation_steps = wandb.config.get('accumulation_steps', 4) \n",
    "patience = wandb.config.get('patience', 2)\n",
    "epochs = wandb.config.get('epochs', 40)\n",
    "lr = wandb.config.get('learning_rate', 5e-5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Optimizer a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=lr)\n",
    "\n",
    "# Scaler per Mixed Precision (FP16)\n",
    "scaler = GradScaler() \n",
    "\n",
    "total_steps = (len(train_loader_laptop) // accumulation_steps) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (MULTI-TASK) ---\n",
    "\n",
    "def evaluate_model_multitask(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "            imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast in valutazione per risparmiare memoria\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels,\n",
    "                    implicit_aspect_labels=imp_asp_labels,\n",
    "                    implicit_opinion_labels=imp_opi_labels\n",
    "                )\n",
    "            \n",
    "            total_loss += outputs['loss'].item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_multitask(model, data_loader, optimizer, scheduler, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass (FP16)\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels,\n",
    "                implicit_aspect_labels=imp_asp_labels,\n",
    "                implicit_opinion_labels=imp_opi_labels\n",
    "            )\n",
    "            # Normalizziamo la loss per l'accumulo dei gradienti\n",
    "            loss = outputs['loss'] / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler (Evita underflow/overflow dell'FP16)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        wandb.log({\"batch_loss\": loss.item() * accumulation_steps})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"\\nTraining Multi-Task su LAPTOP: {epochs} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti: ogni {accumulation_steps} step | FP16: Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "output_dir = \"./best_multitask_extractor_laptop\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{epochs} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch_multitask(model_step1, train_loader_laptop, optimizer, scheduler, device, epoch, scaler, accumulation_steps)\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model_multitask(model_step1, dev_loader_laptop, device)\n",
    "    \n",
    "    # Pulizia spietata della cache della GPU a fine epoca!\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_laptop:.4f} | Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Salvataggio Custom per l'architettura Multi-Task\n",
    "        torch.save(model_step1.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Multi-Task per LAPTOP.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6584a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello Multi-Task migliore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319f19241d164d8b8039e148b962524b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inizio Test Multi-Task sul Dataset Laptop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test LAPTOP: 100%|██████████| 51/51 [00:05<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\n",
      "============================================================\n",
      "Overall Precision: 0.4375\n",
      "Overall Recall:    0.8104\n",
      "Overall F1-Score:  0.5683\n",
      "\n",
      "Dettaglio per Classe (Quello che conta per il paper):\n",
      "--------------------------------------------------\n",
      "   ASP:\n",
      "   Precision: 0.4053\n",
      "   Recall:    0.7581\n",
      "   F1-Score:  0.5282\n",
      "   Support:   802\n",
      "   OPI:\n",
      "   Precision: 0.4715\n",
      "   Recall:    0.8645\n",
      "   F1-Score:  0.6102\n",
      "   Support:   775\n",
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\n",
      "============================================================\n",
      "Accuratezza Aspetti Impliciti: 0.8542\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.94      0.87      0.90       627\n",
      "Implicito (1)       0.65      0.80      0.72       189\n",
      "\n",
      "     accuracy                           0.85       816\n",
      "    macro avg       0.79      0.84      0.81       816\n",
      " weighted avg       0.87      0.85      0.86       816\n",
      "\n",
      "--------------------------------------------------\n",
      "Accuratezza Opinioni Implicite: 0.7819\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.90      0.74      0.82       530\n",
      "Implicito (1)       0.64      0.85      0.73       286\n",
      "\n",
      "     accuracy                           0.78       816\n",
      "    macro avg       0.77      0.80      0.77       816\n",
      " weighted avg       0.81      0.78      0.79       816\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- A. CARICAMENTO DEL \"CAMPIONE\" MULTI-TASK ---\n",
    "print(\"Caricamento del modello Multi-Task migliore...\")\n",
    "\n",
    "# 1. Inizializziamo la nostra architettura custom\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5)\n",
    "\n",
    "# 2. Carichiamo i pesi salvati del miglior modello (solo i pesi, non l'intero oggetto)\n",
    "model_path = \"./best_multitask_extractor_laptop/pytorch_model.bin\"\n",
    "model_step1.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "model_step1.to(device)\n",
    "model_step1.eval() # Modalità esame (spegne dropout)\n",
    "\n",
    "# --- B. PREPARAZIONE METRICHE ---\n",
    "# Carichiamo la metrica seqeval (standard per NER/ABSA)\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Mappa per decodificare i numeri in etichette\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"\\nInizio Test Multi-Task sul Dataset Laptop...\")\n",
    "\n",
    "# --- C. CICLO DI PREVISIONE ---\n",
    "predictions_tokens = []\n",
    "true_labels_tokens = []\n",
    "\n",
    "# Liste per salvare le predizioni binarie (Impliciti)\n",
    "true_imp_asp, pred_imp_asp = [], []\n",
    "true_imp_opi, pred_imp_opi = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_laptop, desc=\"Test LAPTOP\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "\n",
    "        # 1. Il modello fa le sue 3 predizioni contemporaneamente\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model_step1(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 2. Estraiamo i risultati dalle 3 testoline\n",
    "        token_preds = torch.argmax(outputs['token_logits'], dim=-1)\n",
    "        asp_preds = torch.argmax(outputs['imp_asp_logits'], dim=-1)\n",
    "        opi_preds = torch.argmax(outputs['imp_opi_logits'], dim=-1)\n",
    "        \n",
    "        # Salviamo i risultati binari\n",
    "        true_imp_asp.extend(imp_asp_labels.cpu().tolist())\n",
    "        pred_imp_asp.extend(asp_preds.cpu().tolist())\n",
    "        \n",
    "        true_imp_opi.extend(imp_opi_labels.cpu().tolist())\n",
    "        pred_imp_opi.extend(opi_preds.cpu().tolist())\n",
    "\n",
    "        # 3. Convertiamo i numeri in etichette BIO (pulendo il padding)\n",
    "        for i in range(len(labels)):\n",
    "            true_label_row = []\n",
    "            pred_label_row = []\n",
    "            \n",
    "            for j in range(len(labels[i])):\n",
    "                # Ignoriamo i token di padding (dove attention_mask è 0 o la label è -100)\n",
    "                if labels[i][j] != -100 and attention_mask[i][j] == 1: \n",
    "                    true_label_row.append(id2label[labels[i][j].item()])\n",
    "                    pred_label_row.append(id2label[token_preds[i][j].item()])\n",
    "            \n",
    "            true_labels_tokens.append(true_label_row)\n",
    "            predictions_tokens.append(pred_label_row)\n",
    "\n",
    "# --- D. CALCOLO E STAMPA RISULTATI ---\n",
    "results_seq = metric.compute(predictions=predictions_tokens, references=true_labels_tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Overall Precision: {results_seq['overall_precision']:.4f}\")\n",
    "print(f\"Overall Recall:    {results_seq['overall_recall']:.4f}\")\n",
    "print(f\"Overall F1-Score:  {results_seq['overall_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nDettaglio per Classe (Quello che conta per il paper):\")\n",
    "print(\"-\" * 50)\n",
    "for key in results_seq.keys():\n",
    "    if key in ['ASP', 'OPI']: \n",
    "        print(f\"   {key}:\")\n",
    "        print(f\"   Precision: {results_seq[key]['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {results_seq[key]['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {results_seq[key]['f1']:.4f}\")\n",
    "        print(f\"   Support:   {results_seq[key]['number']}\") \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metriche per Aspetti Impliciti\n",
    "acc_asp = accuracy_score(true_imp_asp, pred_imp_asp)\n",
    "print(f\"Accuratezza Aspetti Impliciti: {acc_asp:.4f}\")\n",
    "print(classification_report(true_imp_asp, pred_imp_asp, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Metriche per Opinioni Implicite\n",
    "acc_opi = accuracy_score(true_imp_opi, pred_imp_opi)\n",
    "print(f\"Accuratezza Opinioni Implicite: {acc_opi:.4f}\")\n",
    "print(classification_report(true_imp_opi, pred_imp_opi, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9758056",
   "metadata": {},
   "source": [
    "## Classificatore Category-Sentiment (Extract-Classify-ACOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b648b",
   "metadata": {},
   "source": [
    "Implementiamo il **secondo stadio** dell'architettura proposta nel paper. Dopo aver estratto gli Aspetti e le Opinioni nello Step 1, ora dobbiamo capire a quale Categoria appartengono e qual è il loro Sentiment.\n",
    "\n",
    "Il codice di preparazione è diviso in tre componenti fondamentali:\n",
    "\n",
    " 1. Il Dataset PyTorch (`ACOSPairDataset`)\n",
    "\n",
    " 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "\n",
    " 3. Inizializzazione e DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4462143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACOSPairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        \n",
    "        # Estraiamo gli span (togliendo il +1 che avevamo messo per il [CLS] \n",
    "        # perché ora ci servono per tagliare la stringa originale)\n",
    "        a_span = row['aspect_span']\n",
    "        o_span = row['opinion_span']\n",
    "        \n",
    "        words = text.split()\n",
    "        a_start, a_end = a_span[0] - 1, a_span[1] - 1\n",
    "        o_start, o_end = o_span[0] - 1, o_span[1] - 1\n",
    "        \n",
    "        # Estraiamo le parole (o \"null\" se è implicito)\n",
    "        aspect_str = \" \".join(words[a_start:a_end]) if a_start >= 0 else \"null\"\n",
    "        opinion_str = \" \".join(words[o_start:o_end]) if o_start >= 0 else \"null\"\n",
    "        \n",
    "        # MAGIA CROSS-ENCODER: Creiamo la stringa contesto!\n",
    "        cross_text = f\"aspect: {aspect_str} opinion: {opinion_str}\"\n",
    "\n",
    "        # Il tokenizer unirà il 'text' e il 'cross_text' in automatico\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            cross_text, # Passiamo la seconda stringa!\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "            # Niente più aspect_span e opinion_span da passare!\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364034ae",
   "metadata": {},
   "source": [
    "### 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "Questa è la vera \"magia\" matematica del paper, tradotta in codice:\n",
    "* **Il Corpo (Backbone):** Invece di partire da zero, carichiamo il *corpo* del modello che hai già addestrato nello Step 1 (`best_model_laptop`). In questo modo, la rete conosce già il dominio tecnico dei computer!\n",
    "* **Span Pooling:** Il modello estrae i vettori (hidden states) corrispondenti alle parole dell'Aspetto e dell'Opinione e ne calcola la media. Se un elemento è implicito (`-1`), pesca automaticamente il vettore globale del token `[CLS]`.\n",
    "* **Feature Fusion:** Concatena il vettore dell'aspetto ($u_a$) e dell'opinione ($u_o$) in un unico grande vettore di dimensione 1536.\n",
    "* **Le 121 Teste (Multiple Multi-class):** Passa questo vettore in 121 classificatori lineari paralleli. Ognuno di essi deciderà se per la *sua* categoria la coppia è `Positive (0)`, `Negative (1)`, `Neutral (2)` o `Invalid (3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843e426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBertACOSClassifier(nn.Module):\n",
    "    def __init__(self, path_to_best_model, num_categories):\n",
    "        super(ModernBertACOSClassifier, self).__init__()\n",
    "        \n",
    "        # Carichiamo il corpo dal modello Step 1\n",
    "        self.modernbert = AutoModel.from_pretrained(path_to_best_model)\n",
    "        hidden_size = self.modernbert.config.hidden_size # 768\n",
    "        \n",
    "        # Le 121 teste ORA PRENDONO 768 (non più 1536)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, 4) for _ in range(num_categories)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask): # Span rimossi dai parametri!\n",
    "        outputs = self.modernbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Prendiamo semplicemente il token [CLS] dell'intera sequenza Cross-Encoder\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :] \n",
    "        cls_output = self.dropout(cls_output)\n",
    "\n",
    "        # Passiamo il vettore nelle teste lineari\n",
    "        logits = [head(cls_output) for head in self.heads]\n",
    "        \n",
    "        return torch.stack(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d70d3e",
   "metadata": {},
   "source": [
    "### 3. Inizializzazione e DataLoaders\n",
    "L'ultimo blocco carica fisicamente i file salvati dalla nostra \"Fabbrica dei Dati\", istanzia i `Dataset`, e crea i `DataLoader` (con batch size = 16) per \"nutrire\" la GPU in modo efficiente durante l'addestramento. Infine, sposta il modello sulla scheda video (CUDA) pronto per il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103aad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb6ef39e3b24b0e865f05dd1cba7ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ../best_model_laptop\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello inizializzato! Categorie: 121 | Device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. Carichiamo la lista delle categorie salvata prima\n",
    "with open(\"data_coppie/laptop_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list) # 121\n",
    "\n",
    "# 2. Carichiamo i DataFrame di Train e Dev\n",
    "df_train = pd.read_pickle(\"data_coppie/train_laptop_pairs.pkl\")\n",
    "df_dev = pd.read_pickle(\"data_coppie/dev_laptop_pairs.pkl\")\n",
    "df_test = pd.read_pickle(\"data_coppie/test_laptop_pairs.pkl\") \n",
    "\n",
    "# 3. Inizializziamo il Tokenizer e i Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "train_dataset = ACOSPairDataset(df_train, tokenizer)\n",
    "dev_dataset = ACOSPairDataset(df_dev, tokenizer)\n",
    "test_dataset = ACOSPairDataset(df_test, tokenizer) \n",
    "\n",
    "# 4. Creiamo i DataLoader (Batch size 16 è un buon compromesso tra velocità e VRAM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dev_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16) \n",
    "\n",
    "# 5. Inizializziamo il Modello usando i pesi dello Step 1!\n",
    "model = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modello inizializzato! Categorie: {num_categories} | Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ca5086",
   "metadata": {},
   "source": [
    "## WANDB per il Monitoraggio dello Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95073d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step1_Class_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/dy14ku66' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/dy14ku66</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260225_144832-dy14ku66/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260225_144854-mwimnpdn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/mwimnpdn' target=\"_blank\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/mwimnpdn' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/mwimnpdn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS\n"
     ]
    }
   ],
   "source": [
    "# Chiudiamo per sicurezza qualsiasi run precedente rimasta aperta nello stesso notebook\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters per lo STEP 2\n",
    "config_step2 = {\n",
    "    \"learning_rate\": 2e-5, # Solitamente per lo Step 2 un LR leggermente più basso è meglio (es. 2e-5)\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 4, # Aggiunto per il training loop ottimizzato!\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Laptop-ACOS\", \n",
    "    \"seed\": 42,\n",
    "    \"patience\": 5  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run per lo Step 2\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config_step2,\n",
    "    # Aggiungiamo \"Step2_Class\" al nome per distinguerlo dallo Step 1\n",
    "    name=f\"Step2_Class_{config_step2['model_name']}_{config_step2['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\" W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6178aa",
   "metadata": {},
   "source": [
    "## Training e valutazione del modello su Sentiment e Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf680e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training STEP 2 su LAPTOP: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti ogni 4 step | FP16 Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1: 100%|██████████| 578/578 [02:33<00:00,  3.76it/s, loss=0.0757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2396 | Valid Loss: 0.0614\n",
      "Miglior modello trovato (Loss: 0.0614)! Salvataggio...\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|██████████| 578/578 [02:35<00:00,  3.71it/s, loss=0.0383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0537 | Valid Loss: 0.0449\n",
      "Miglior modello trovato (Loss: 0.0449)! Salvataggio...\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|██████████| 578/578 [02:35<00:00,  3.71it/s, loss=0.0381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0437 | Valid Loss: 0.0391\n",
      "Miglior modello trovato (Loss: 0.0391)! Salvataggio...\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|██████████| 578/578 [02:37<00:00,  3.68it/s, loss=0.0561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0384 | Valid Loss: 0.0354\n",
      "Miglior modello trovato (Loss: 0.0354)! Salvataggio...\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|██████████| 578/578 [02:36<00:00,  3.68it/s, loss=0.0386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0337 | Valid Loss: 0.0319\n",
      "Miglior modello trovato (Loss: 0.0319)! Salvataggio...\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|██████████| 578/578 [02:36<00:00,  3.69it/s, loss=0.0273] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0290 | Valid Loss: 0.0291\n",
      "Miglior modello trovato (Loss: 0.0291)! Salvataggio...\n",
      "\n",
      "--- Epoca 7/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 7: 100%|██████████| 578/578 [02:38<00:00,  3.65it/s, loss=0.0213] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0251 | Valid Loss: 0.0277\n",
      "Miglior modello trovato (Loss: 0.0277)! Salvataggio...\n",
      "\n",
      "--- Epoca 8/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 8: 100%|██████████| 578/578 [02:37<00:00,  3.67it/s, loss=0.0255] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0215 | Valid Loss: 0.0263\n",
      "Miglior modello trovato (Loss: 0.0263)! Salvataggio...\n",
      "\n",
      "--- Epoca 9/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 9: 100%|██████████| 578/578 [02:38<00:00,  3.64it/s, loss=0.0162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0178 | Valid Loss: 0.0252\n",
      "Miglior modello trovato (Loss: 0.0252)! Salvataggio...\n",
      "\n",
      "--- Epoca 10/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 10: 100%|██████████| 578/578 [02:38<00:00,  3.65it/s, loss=0.0162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0145 | Valid Loss: 0.0248\n",
      "Miglior modello trovato (Loss: 0.0248)! Salvataggio...\n",
      "\n",
      "--- Epoca 11/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 11: 100%|██████████| 578/578 [02:38<00:00,  3.64it/s, loss=0.00788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0116 | Valid Loss: 0.0243\n",
      "Miglior modello trovato (Loss: 0.0243)! Salvataggio...\n",
      "\n",
      "--- Epoca 12/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 12: 100%|██████████| 578/578 [02:10<00:00,  4.42it/s, loss=0.00717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0092 | Valid Loss: 0.0248\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 13/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 13: 100%|██████████| 578/578 [02:13<00:00,  4.33it/s, loss=0.0176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0099 | Valid Loss: 0.0243\n",
      "Miglior modello trovato (Loss: 0.0243)! Salvataggio...\n",
      "\n",
      "--- Epoca 14/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 14: 100%|██████████| 578/578 [02:13<00:00,  4.33it/s, loss=0.00717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0078 | Valid Loss: 0.0246\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 15/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 15: 100%|██████████| 578/578 [02:08<00:00,  4.50it/s, loss=0.00589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0049 | Valid Loss: 0.0251\n",
      "Nessun miglioramento. Patience: 2/5\n",
      "\n",
      "--- Epoca 16/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 16: 100%|██████████| 578/578 [02:05<00:00,  4.62it/s, loss=0.00248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0038 | Valid Loss: 0.0258\n",
      "Nessun miglioramento. Patience: 3/5\n",
      "\n",
      "--- Epoca 17/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 17: 100%|██████████| 578/578 [02:04<00:00,  4.65it/s, loss=0.00221] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0031 | Valid Loss: 0.0261\n",
      "Nessun miglioramento. Patience: 4/5\n",
      "\n",
      "--- Epoca 18/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 18: 100%|██████████| 578/578 [02:05<00:00,  4.62it/s, loss=0.00218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0027 | Valid Loss: 0.0264\n",
      "Nessun miglioramento. Patience: 5/5\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 18.\n",
      "\n",
      "Fine Addestramento Step 2.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▆▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss_epoch</td><td>█▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.00218</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>train_loss_epoch</td><td>0.00271</td></tr><tr><td>valid_loss_epoch</td><td>0.02642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/mwimnpdn' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/mwimnpdn</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260225_144854-mwimnpdn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "\n",
    "# Gradient Checkpointing: abilitato SOLO sul corpo di ModernBERT\n",
    "model.modernbert.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config_step2.get('accumulation_steps', 4) \n",
    "patience = config_step2.get('patience', 5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Ottimizzatore AdamW a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config_step2['learning_rate']\n",
    ")\n",
    "\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "# Un bilanciamento molto più equo! Il modello non avrà più il terrore di usare \"Invalido\"\n",
    "weights = torch.tensor([2.0, 2.0, 2.0, 1.0]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Calcolo degli step totali per lo scheduler\n",
    "total_steps = (len(train_loader) // accumulation_steps) * config_step2['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (STEP 2) ---\n",
    "\n",
    "def evaluate_model_step2(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda'):\n",
    "                # Span rimossi dal forward\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask) \n",
    "                loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_step2(model, data_loader, optimizer, scheduler, criterion, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            # Span rimossi dal forward\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            loss = loss / accumulation_steps \n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        real_loss = loss.item() * accumulation_steps\n",
    "        total_loss += real_loss\n",
    "        \n",
    "        wandb.log({\"batch_loss\": real_loss})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=real_loss)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"Training STEP 2 su LAPTOP: {config_step2['epochs']} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_laptop = float('inf')\n",
    "\n",
    "for epoch in range(config_step2['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config_step2['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_laptop = train_epoch_step2(\n",
    "        model, train_loader, optimizer, scheduler, criterion, \n",
    "        device, epoch, scaler, accumulation_steps\n",
    "    )\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_laptop = evaluate_model_step2(model, val_loader, criterion, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_laptop:.4f} | Valid Loss: {valid_loss_laptop:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_laptop,\n",
    "        \"valid_loss_epoch\": valid_loss_laptop\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_laptop < best_valid_loss_laptop:\n",
    "        best_valid_loss_laptop = valid_loss_laptop\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_laptop:.4f})! Salvataggio...\")\n",
    "        \n",
    "        # Salvataggio custom model (Solo i pesi)\n",
    "        save_dir = \"./best_classifier_laptop\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Step 2.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4c16a",
   "metadata": {},
   "source": [
    "## Test su sentiment e categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740a16f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260225_160101-i9gdx08w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/i9gdx08w' target=\"_blank\">TEST_Step2_answerdotai/ModernBERT-base_Laptop-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/i9gdx08w' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/i9gdx08w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio Test sul Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b500b8faa6245ae9db3ef78daaaf6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_laptop\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "classifier.weight | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CHECK CARICAMENTO PESI ---\n",
      "Chiavi Inaspettate (OK se sono dello Step 1): 0\n",
      "Chiavi Mancanti (PROBLEMA se sono 'heads'): 0\n",
      "SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\n",
      "Estrazione di tutte le probabilità dal modello in corso (attendere)...\n",
      "\n",
      "🔍 Avvio Grid Search per la migliore Soglia (MACRO F1)...\n",
      "IL VINCITORE È... THRESHOLD A 0.40 (40%)\n",
      "\n",
      "--- MIGLIOR CLASSIFICATION REPORT (Basato su Macro F1) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Positive (0)       0.41      0.18      0.26       379\n",
      "Negative (1)       0.00      0.00      0.00        65\n",
      " Neutral (2)       0.61      0.42      0.50       712\n",
      "\n",
      "   micro avg       0.56      0.32      0.40      1156\n",
      "   macro avg       0.34      0.20      0.25      1156\n",
      "weighted avg       0.51      0.32      0.39      1156\n",
      "\n",
      "BEST MACRO F1-Score: 0.2500\n",
      "CORRISPONDENTE MICRO F1: 0.4048\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>▁</td></tr><tr><td>best_test_micro_f1</td><td>▁</td></tr><tr><td>optimal_threshold</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>0.25001</td></tr><tr><td>best_test_micro_f1</td><td>0.40484</td></tr><tr><td>optimal_threshold</td><td>0.4</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TEST_Step2_answerdotai/ModernBERT-base_Laptop-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/i9gdx08w' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/i9gdx08w</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260225_160101-i9gdx08w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chiudiamo run appese\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    # Chiamo la run con il prefisso \"TEST_\"\n",
    "    name=f\"TEST_Step2_{config_step2['model_name']}_{config_step2['dataset']}\",\n",
    "    job_type=\"test\"\n",
    ")\n",
    "\n",
    "# --- 1. PREPARAZIONE ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Avvio Test sul Device: {device}\")\n",
    "\n",
    "# Ricreiamo l'architettura del modello\n",
    "num_categories = len(category_list)\n",
    "model_test = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories)\n",
    "\n",
    "# Carichiamo i pesi dello Step 2\n",
    "model_path = \"./best_classifier_laptop/pytorch_model.bin\"\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# USIAMO STRICT=FALSE\n",
    "missing_keys, unexpected_keys = model_test.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"\\n--- CHECK CARICAMENTO PESI ---\")\n",
    "print(f\"Chiavi Inaspettate (OK se sono dello Step 1): {len(unexpected_keys)}\")\n",
    "print(f\"Chiavi Mancanti (PROBLEMA se sono 'heads'): {len(missing_keys)}\")\n",
    "\n",
    "# Verifica specifica sulle teste\n",
    "heads_missing = [k for k in missing_keys if \"heads\" in k]\n",
    "if heads_missing:\n",
    "    print(f\"ERRORE CRITICO: Le teste di classificazione non sono state caricate! {heads_missing[:5]}\")\n",
    "else:\n",
    "    print(\"SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\")\n",
    "    \n",
    "model_test.to(device)\n",
    "model_test.eval()\n",
    "\n",
    "# --- 2. ESTRAZIONE PROBABILITA'---\n",
    "print(\"Estrazione di tutte le probabilità dal modello in corso (attendere)...\")\n",
    "\n",
    "all_probs_list = []\n",
    "all_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader: \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            # 🛑 CORRETTO: Rimosso aspect_span e opinion_span che non esistono più!\n",
    "            logits = model_test(input_ids, attention_mask)\n",
    "            \n",
    "        probs = torch.softmax(logits, dim=-1).cpu() \n",
    "        \n",
    "        all_probs_list.append(probs)\n",
    "        all_true_list.append(labels.cpu())\n",
    "\n",
    "# Uniamo tutti i batch in un unico grande blocco di memoria\n",
    "all_probs = torch.cat(all_probs_list, dim=0) # Forma: [Tot_Frasi, 121, 4]\n",
    "all_true = torch.cat(all_true_list, dim=0).numpy().flatten()\n",
    "\n",
    "\n",
    "# --- 3. GRID SEARCH SUL THRESHOLD (OTTIMIZZATA PER MACRO F1) ---\n",
    "print(\"\\n🔍 Avvio Grid Search per la migliore Soglia (MACRO F1)...\")\n",
    "\n",
    "# SCENDIAMO FINO A 0.30! Diamo al modello la possibilità di essere meno sicuro.\n",
    "thresholds_to_test = np.arange(0.40, 0.99, 0.02) \n",
    "best_micro_f1 = 0.0\n",
    "best_macro_f1 = 0.0\n",
    "best_threshold = 0.0\n",
    "best_report = \"\"\n",
    "\n",
    "target_names = ['Positive (0)', 'Negative (1)', 'Neutral (2)']\n",
    "labels_to_eval = [0, 1, 2]\n",
    "\n",
    "# INIZIO CICLO SILENZIOSO\n",
    "for thresh in thresholds_to_test:\n",
    "    valid_class_probs, valid_class_preds = torch.max(all_probs[:, :, :3], dim=-1)\n",
    "    final_preds = torch.full_like(valid_class_preds, 3)\n",
    "    \n",
    "    mask = valid_class_probs > thresh\n",
    "    final_preds[mask] = valid_class_preds[mask]\n",
    "    \n",
    "    preds_flat = final_preds.numpy().flatten()\n",
    "    \n",
    "    current_micro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='micro')\n",
    "    current_macro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='macro')\n",
    "    \n",
    "    # ORA VINCE CHI ALZA IL MACRO F1!\n",
    "    if current_macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = current_macro_f1\n",
    "        best_micro_f1 = current_micro_f1\n",
    "        best_threshold = thresh\n",
    "        best_report = classification_report(\n",
    "            all_true, preds_flat, labels=labels_to_eval, \n",
    "            target_names=target_names, zero_division=0\n",
    "        )\n",
    "\n",
    "# --- 4. STAMPA DEI RISULTATI VINCITORI E LOG W&B ---\n",
    "print(f\"IL VINCITORE È... THRESHOLD A {best_threshold:.2f} ({(best_threshold*100):.0f}%)\")\n",
    "\n",
    "print(\"\\n--- MIGLIOR CLASSIFICATION REPORT (Basato su Macro F1) ---\")\n",
    "print(best_report)\n",
    "print(f\"BEST MACRO F1-Score: {best_macro_f1:.4f}\")\n",
    "print(f\"CORRISPONDENTE MICRO F1: {best_micro_f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Salviamo su Weights & Biases\n",
    "wandb.log({\n",
    "    \"best_test_micro_f1\": best_micro_f1,\n",
    "    \"best_test_macro_f1\": best_macro_f1,\n",
    "    \"optimal_threshold\": best_threshold\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e1535",
   "metadata": {},
   "source": [
    "### Test sulla quadrupla completa (Aspetto-Opinione-Categoria-Sentiment)\n",
    "In questa cella, eseguiamo il test finale del nostro modello su tutte e quattro le dimensioni del problema: Aspetto, Opinione, Categoria e Sentiment.\n",
    "Il processo è il seguente:\n",
    "1. **Iterazione sui Dati di Test:** Per ogni batch di coppie (Aspetto-Opinione) estratte dallo Step 1, il modello predice la Categoria e il Sentiment.\n",
    "2. **Raccolta delle Predizioni:** Le predizioni vengono raccole in liste separate per le categorie e i sentimenti, insieme alle etichette vere.\n",
    "3. **Calcolo delle Metriche:** Utilizziamo `accuracy_score` e `classification_report` di `sklearn` per valutare le prestazioni del modello sia sulla Categoria che sul Sentiment, considerando solo le coppie che sono state correttamente estratte nello Step 1 (ovvero quelle con etichetta diversa da \"Invalid (3)\").\n",
    "4. **Stampa dei Risultati:** I risultati vengono stampati in modo chiaro, mostrando l'accuratezza e il report dettagliato per entrambe le dimensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento configurazioni Laptop...\n",
      "Caricamento Step 1 (L'Investigatore Multi-Task LAPTOP)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b985ffa4404f33962295241dad6bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento Step 2 (Lo Psicologo Classificatore LAPTOP)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977227f93c744f6084acb798badc5d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ../best_model_laptop\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avvio Valutazione End-to-End sul Test Set LAPTOP...\n",
      "Dataset caricato! Numero di frasi da analizzare: 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi Frasi Laptop: 100%|██████████| 816/816 [05:56<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RISULTATI FINALI EXACT MATCH ACOS (LAPTOP)\n",
      "==================================================\n",
      "Quadruple Reali totali:    1156\n",
      "Quadruple Predette totali: 894\n",
      "Quadruple Esatte:          285\n",
      "--------------------------------------------------\n",
      "Precision: 0.3188\n",
      "Recall:    0.2465\n",
      "F1-Score:  0.2780\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. CARICAMENTO DEI MODELLI E PREPARATIVI\n",
    "\n",
    "print(\"Caricamento configurazioni Laptop...\")\n",
    "\n",
    "with open(\"data_coppie/laptop_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list)\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"Caricamento Step 1 (L'Investigatore Multi-Task LAPTOP)...\")\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5).to(device)\n",
    "model_step1.load_state_dict(torch.load(\"./best_multitask_extractor_laptop/pytorch_model.bin\", map_location=device, weights_only=True))\n",
    "model_step1.eval()\n",
    "\n",
    "print(\"Caricamento Step 2 (Lo Psicologo Classificatore LAPTOP)...\")\n",
    "\n",
    "model_step2 = ModernBertACOSClassifier(\"./best_model_laptop\", num_categories).to(device)\n",
    "model_step2.load_state_dict(torch.load(\"./best_classifier_laptop/pytorch_model.bin\", map_location=device, weights_only=True))\n",
    "model_step2.eval()\n",
    "\n",
    "# 2. FUNZIONI DI SUPPORTO PER L'ESTRAZIONE\n",
    "# ==========================================\n",
    "def get_spans(tags, b_tag, i_tag):\n",
    "    spans = []\n",
    "    start = -1\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == b_tag:\n",
    "            if start != -1: spans.append((start, i))\n",
    "            start = i\n",
    "        elif tag == i_tag and start != -1: continue\n",
    "        else:\n",
    "            if start != -1:\n",
    "                spans.append((start, i))\n",
    "                start = -1\n",
    "    if start != -1: spans.append((start, len(tags)))\n",
    "    return spans\n",
    "\n",
    "def predict_quadruples_e2e(text, model_1, model_2, tokenizer, cat_list):\n",
    "    words = text.split()\n",
    "    inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True, max_length=128, padding='max_length').to(device)\n",
    "    \n",
    "    # --- FASE 1: L'Investigatore (Invariato) ---\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type=device.type):\n",
    "            out1 = model_1(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "    token_preds = torch.argmax(out1['token_logits'], dim=-1)[0].cpu().numpy()\n",
    "    imp_asp = torch.argmax(out1['imp_asp_logits'], dim=-1)[0].item()\n",
    "    imp_opi = torch.argmax(out1['imp_opi_logits'], dim=-1)[0].item()\n",
    "    \n",
    "    word_ids = inputs.word_ids()\n",
    "    word_tags = [\"O\"] * len(words)\n",
    "    \n",
    "    for idx, w_id in enumerate(word_ids):\n",
    "        if w_id is not None and w_id < len(words) and word_tags[w_id] == \"O\":\n",
    "            word_tags[w_id] = id2label[token_preds[idx]]\n",
    "                \n",
    "    asp_spans = get_spans(word_tags, \"B-ASP\", \"I-ASP\")\n",
    "    opi_spans = get_spans(word_tags, \"B-OPI\", \"I-OPI\")\n",
    "    \n",
    "    if imp_asp == 1 or len(asp_spans) == 0: asp_spans.append((-1, -1))\n",
    "    if imp_opi == 1 or len(opi_spans) == 0: opi_spans.append((-1, -1))\n",
    "    \n",
    "    asp_spans = list(set(asp_spans))\n",
    "    opi_spans = list(set(opi_spans))\n",
    "    quadruples = []\n",
    "    \n",
    "    # --- FASE 2: Lo Psicologo (Ora in versione Cross-Encoder) ---\n",
    "    for a in asp_spans:\n",
    "        for o in opi_spans:\n",
    "            \n",
    "            # Estraiamo le stringhe\n",
    "            asp_str = \" \".join(words[a[0]:a[1]]) if a != (-1, -1) else \"null\"\n",
    "            opi_str = \" \".join(words[o[0]:o[1]]) if o != (-1, -1) else \"null\"\n",
    "            cross_text = f\"aspect: {asp_str} opinion: {opi_str}\"\n",
    "            \n",
    "            # Re-Tokenizziamo al volo per il Cross-Encoder\n",
    "            pair_inputs = tokenizer(\n",
    "                text, cross_text, \n",
    "                return_tensors=\"pt\", truncation=True, \n",
    "                max_length=128, padding='max_length'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(device_type=device.type):\n",
    "                    # Niente più tensori t_a e t_o! Passiamo i nuovi input formattati\n",
    "                    out2 = model_2(\n",
    "                        input_ids=pair_inputs['input_ids'], \n",
    "                        attention_mask=pair_inputs['attention_mask']\n",
    "                    )\n",
    "            \n",
    "            logits = out2['logits'] if isinstance(out2, dict) else out2 \n",
    "            probs = torch.softmax(logits[0], dim=-1) \n",
    "            \n",
    "            for cat_idx, prob_dist in enumerate(probs):\n",
    "                best_class = torch.argmax(prob_dist).item()\n",
    "                \n",
    "                # Ci fidiamo dell'addestramento! Se dice Invalido (3), scartiamo.\n",
    "                if best_class == 3:\n",
    "                    continue\n",
    "                \n",
    "                # 🛑 RIMUOVIAMO IL FILTRO DELLA SOGLIA\n",
    "                # if prob_dist[best_class] < 0.40:\n",
    "                #     continue\n",
    "                \n",
    "                quadruples.append({\n",
    "                    'aspect_span': a,\n",
    "                    'opinion_span': o,\n",
    "                    'category': cat_list[cat_idx],\n",
    "                    'sentiment': best_class\n",
    "                })\n",
    "                    \n",
    "    return quadruples\n",
    "\n",
    "# 3. TEST SULL'INTERO DATASET LAPTOP\n",
    "# ==========================================\n",
    "print(\"\\nAvvio Valutazione End-to-End sul Test Set LAPTOP...\")\n",
    "true_quads = []\n",
    "pred_quads = []\n",
    "\n",
    "\n",
    "percorso_file = os.path.join(\"../data_parsing\", \"test_laptop_parsed.pkl\")\n",
    "test_lap_parsed = pd.read_pickle(percorso_file)\n",
    "\n",
    "print(f\"Dataset caricato! Numero di frasi da analizzare: {len(test_lap_parsed)}\")\n",
    "\n",
    "for idx, row in tqdm(test_lap_parsed.iterrows(), total=len(test_lap_parsed), desc=\"Analisi Frasi Laptop\"):\n",
    "    text = row['review_text']\n",
    "    preds = predict_quadruples_e2e(text, model_step1, model_step2, tokenizer, category_list)\n",
    "    \n",
    "    p_set = set()\n",
    "    for q in preds:\n",
    "        p_set.add((tuple(q['aspect_span']), q['category'], tuple(q['opinion_span']), q['sentiment']))\n",
    "    pred_quads.append(p_set)\n",
    "    \n",
    "    t_set = set()\n",
    "    for q in row['parsed_quadruples']:\n",
    "        a = tuple(q.get('span_A', [-1, -1]))\n",
    "        o = tuple(q.get('span_B', [-1, -1]))\n",
    "        c = q['category_aspect']\n",
    "        s = int(q['sentiment'])\n",
    "        t_set.add((a, c, o, s))\n",
    "    true_quads.append(t_set)\n",
    "\n",
    "# Metriche finali\n",
    "total_pred = sum(len(p) for p in pred_quads)\n",
    "total_true = sum(len(t) for t in true_quads)\n",
    "correct = sum(len(p_set.intersection(t_set)) for p_set, t_set in zip(pred_quads, true_quads))\n",
    "\n",
    "precision = correct / total_pred if total_pred > 0 else 0\n",
    "recall = correct / total_true if total_true > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RISULTATI FINALI EXACT MATCH ACOS (LAPTOP)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Quadruple Reali totali:    {total_true}\")\n",
    "print(f\"Quadruple Predette totali: {total_pred}\")\n",
    "print(f\"Quadruple Esatte:          {correct}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f2e9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 TESTO: bought this last year on july .\n",
      "\n",
      "✅ QUADRUPLE VERE (Ground Truth):\n",
      "   Aspetto: (-1, -1) | Opinione: (-1, -1) | Categoria: LAPTOP#GENERAL | Sent: 1\n",
      "\n",
      "🤖 QUADRUPLE PREDETTE DAL MODELLO:\n"
     ]
    }
   ],
   "source": [
    "# --- DEBUG: GUARDIE E LADRI SULLA PRIMA FRASE ---\n",
    "\n",
    "test_row = test_lap_parsed.iloc[7] \n",
    "test_text = test_row['review_text']\n",
    "\n",
    "print(f\"📝 TESTO: {test_text}\")\n",
    "print(\"\\n✅ QUADRUPLE VERE (Ground Truth):\")\n",
    "for q in test_row['parsed_quadruples']:\n",
    "    print(f\"   Aspetto: {q.get('span_A', [-1,-1])} | Opinione: {q.get('span_B', [-1,-1])} | Categoria: {q['category_aspect']} | Sent: {q['sentiment']}\")\n",
    "\n",
    "print(\"\\n🤖 QUADRUPLE PREDETTE DAL MODELLO:\")\n",
    "preds = predict_quadruples_e2e(test_text, model_step1, model_step2, tokenizer, category_list)\n",
    "for p in preds:\n",
    "    print(f\"   Aspetto: {p['aspect_span']} | Opinione: {p['opinion_span']} | Categoria: {p['category']} | Sent: {p['sentiment']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
