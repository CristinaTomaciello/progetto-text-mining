{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7266c47e",
   "metadata": {},
   "source": [
    "# Estrazione delle quadruple (aspect-opinion-category-sentiment) con ModernBERT su Restaurant-ACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a40e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acceleratore Apple Metal (MPS) Trovato\n",
      "Librerie caricate.\n"
     ]
    }
   ],
   "source": [
    "# Import delle librerie necessarie\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup, AutoModel\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "import pickle\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler # Per Mixed Precision\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from torchcrf import CRF  # Ricordati di fare !pip install pytorch-crf\n",
    "\n",
    "# --- 1. CONFIGURAZIONE DEL DEVICE ---\n",
    "# Se hai una GPU NVIDIA, userà 'cuda'. Se hai un Mac M1/M2, userà 'mps'. Altrimenti 'cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\" GPU Trovata: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Acceleratore Apple Metal (MPS) Trovato\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Nessuna GPU trovata. L'addestramento sarà lento.\")\n",
    "\n",
    "print(\"Librerie caricate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70bed1",
   "metadata": {},
   "source": [
    "### Impostazioni per la riproducibilità "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds impostati su 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Imposto i seed per la riproducibilità.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        # Imposto anche i seed per la GPU, se disponibile\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Esegui l'impostazione del seed\n",
    "set_seed(42) \n",
    "print(\"Random seeds impostati su 42.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb63eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260226_143206-vy0i75kr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/vy0i75kr' target=\"_blank\">Step1_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/vy0i75kr' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/vy0i75kr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: Step1_Class_answerdotai/ModernBERT-base_Restaurant-ACOS\n"
     ]
    }
   ],
   "source": [
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Restaurant-ACOS\",  \n",
    "    \"seed\": 42,\n",
    "    'patience': 5  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config,\n",
    "    name=f\"Step1_Class_{config['model_name']}_{config['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\"W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84319964",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bba6",
   "metadata": {},
   "source": [
    "### Creazione di PyTorch Dataset e DataLoader\n",
    "Questa cella si occupa di caricare i dati pre-processati e di \"impacchettarli\" nel formato esatto richiesto dalla nostra nuova architettura PyTorch personalizzata. Rappresenta un passaggio cruciale per replicare fedelmente il paper originale, permettendoci di gestire gli elementi impliciti.\n",
    "\n",
    "Nello specifico, il codice esegue tre operazioni fondamentali:\n",
    "\n",
    "1. **Caricamento dei DataFrame:** Legge i file `.pkl` (Train, Dev e Test per il dominio Laptop) che abbiamo precedentemente aggiornato. Questi file ora contengono le annotazioni binarie che indicano la presenza di aspetti o opinioni implicite.\n",
    "2. **Definizione della Classe Custom `ACOSDataset`:** Questa è la modifica principale rispetto a una pipeline standard di HuggingFace. Invece di restituire solo i classici 3 tensori della Token Classification (`input_ids`, `attention_mask` e `labels` con i tag BIO), questa classe sovrascritta restituisce **5 tensori** per ogni frase. Vengono infatti estratti e passati al modello anche `implicit_aspect_labels` e `implicit_opinion_labels`. Questi tensori serviranno ad addestrare in parallelo le due teste di classificazione binaria sul token `[CLS]`.\n",
    "3. **Configurazione dei DataLoader:** Crea gli iteratori di PyTorch che alimenteranno il modello durante l'addestramento e il test, processando blocchi (batch) di 16 frasi alla volta. I dati di training vengono rimescolati (`shuffle=True`) per stabilizzare l'apprendimento della rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7f7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dei dataset pre-processati...\n",
      "Dataset e DataLoaders creati con successo!\n",
      "Esempi nel set di Training RESTAURANT: 1530\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CARICAMENTO DEI DATASET SALVATI  ---\n",
    "cartella_dati = \"data_allineati\"\n",
    "\n",
    "print(\"Caricamento dei dataset pre-processati...\")\n",
    "# Carichiamo i Ristoranti\n",
    "df_train_align_rest = pd.read_pickle(os.path.join(cartella_dati, \"train_rest_aligned.pkl\"))\n",
    "df_dev_align_rest = pd.read_pickle(os.path.join(cartella_dati, \"dev_rest_aligned.pkl\"))\n",
    "df_test_align_rest = pd.read_pickle(os.path.join(cartella_dati, \"test_rest_aligned.pkl\"))\n",
    "\n",
    "\n",
    "class ACOSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "        # Estraiamo le colonne per gli impliciti!\n",
    "        self.implicit_aspect_label = df['implicit_aspect_label'].tolist()\n",
    "        self.implicit_opinion_label = df['implicit_opinion_label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            # Passiamo le etichette al Dataloader\n",
    "            'implicit_aspect_labels': torch.tensor(self.implicit_aspect_label[idx], dtype=torch.long),\n",
    "            'implicit_opinion_labels': torch.tensor(self.implicit_opinion_label[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- CREAZIONE DELLE ISTANZE ---\n",
    "\n",
    "# Creiamo i dataset per il dominio restaruant\n",
    "train_dataset_rest = ACOSDataset(df_train_align_rest)\n",
    "dev_dataset_rest = ACOSDataset(df_dev_align_rest)\n",
    "test_dataset_rest = ACOSDataset(df_test_align_rest)\n",
    "\n",
    "# --- CONFIGURAZIONE DATALOADERS ---\n",
    "\n",
    "BATCH_SIZE = 16 # Numero di frasi analizzate contemporaneamente\n",
    "\n",
    "train_loader_rest = DataLoader(train_dataset_rest, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_rest = DataLoader(dev_dataset_rest, batch_size=BATCH_SIZE)\n",
    "test_loader_rest = DataLoader(test_dataset_rest, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Dataset e DataLoaders creati con successo!\")\n",
    "print(f\"Esempi nel set di Training RESTAURANT: {len(train_dataset_rest)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70053",
   "metadata": {},
   "source": [
    "### Architettura Multi-Task: ModernBERT ACOS Extractor\n",
    "\n",
    "In questa cella definiamo il cuore dello Step 1 (L'Investigatore) della nostra pipeline. Invece di usare un modello standard pre-confezionato, abbiamo costruito un'architettura di rete neurale custom basata su ModernBERT per risolvere tre task contemporaneamente (Multi-Task Learning), eguagliando l'approccio dei paper State-of-the-Art.\n",
    "\n",
    "Ecco le innovazioni matematiche e strutturali introdotte in questa classe:\n",
    "\n",
    "* Layer CRF (Conditional Random Field): Il vero \"game changer\". Invece di usare una semplice CrossEntropyLoss che classifica ogni parola in isolamento, abbiamo inserito un layer statistico che impara le \"regole\" delle etichette BIO (es. è impossibile che un tag I-ASP appaia se prima non c'è stato un B-ASP). Agisce come un potentissimo \"correttore ortografico\" per l'estrazione degli span, massimizzando il Recall.\n",
    "\n",
    "* Teste per gli Impliciti sul [CLS]: Abbiamo agganciato due classificatori binari indipendenti al token speciale [CLS] (che racchiude il significato dell'intera frase) per determinare se l'aspetto o l'opinione sono sottointesi (-1, -1).\n",
    "\n",
    "* Loss Combinata e Bilanciata: Durante l'addestramento, il modello calcola la Negative Log-Likelihood del CRF e la somma alla Loss dei due task impliciti. Per risolvere il forte sbilanciamento del dataset, abbiamo assegnato un peso 5.0 alla classe \"Implicito\" (implicit_weights = [1.0, 5.0]), forzando il modello a non ignorare questi casi rari.\n",
    "\n",
    "* Decodifica di Viterbi: In fase di inferenza (test), la rete non emette probabilità grezze da filtrare con argmax, ma usa l'algoritmo di Viterbi per calcolare matematicamente la sequenza grammaticale perfetta, restituendola già decodificata e pronta all'uso.\n",
    "\n",
    "* Memory Optimization: Instanziamo l'ottimizzatore AdamW8bit della libreria bitsandbytes per permettere l'addestramento di questa complessa struttura Multi-Task senza saturare la memoria VRAM (Out Of Memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9262d848566b4618927bc10464447b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODELLO MULTI-TASK (CON CRF) PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base + Layer CRF (Custom ACOS Extractor)\n",
      "Task: CRF Sequence Labeling + 2x Binary Classification (Impliciti)\n",
      "Numero di Classi Token: 5\n",
      "Optimizer: AdamW 8-bit (lr=2e-5)\n",
      "Loss Function: Neg Log-Likelihood (CRF) + 2x CrossEntropy (Impliciti)\n",
      "Device: mps\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Definiamo le 5 etichette: 0=O, 1=B-ASP, 2=I-ASP, 3=B-OPI, 4=I-OPI\n",
    "NUM_LABELS = 5 \n",
    "\n",
    "class ModernBertACOS_Extractor(nn.Module):\n",
    "    def __init__(self, model_name=\"answerdotai/ModernBERT-base\", num_labels=5):\n",
    "        super().__init__()\n",
    "        # Carichiamo la \"schiena\" del modello (l'encoder base)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Testolina 1: Emette i \"punteggi grezzi\" per il CRF\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "        # IL LAYER CRF (Correttore Ortografico per Sequenze) \n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        \n",
    "        # Testoline 2 e 3: Indovinano se ci sono impliciti (Manteniamo la tua intuizione!)\n",
    "        self.implicit_aspect_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        self.implicit_opinion_classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, implicit_aspect_labels=None, implicit_opinion_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        \n",
    "        # Prendiamo il token [CLS] (posizione 0) per i classificatori binari\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "        \n",
    "        # Punteggi grezzi (emissions) per il CRF\n",
    "        emissions = self.token_classifier(sequence_output)\n",
    "        \n",
    "        # Le due testoline calcolano i logit per gli impliciti\n",
    "        imp_asp_logits = self.implicit_aspect_classifier(cls_output)\n",
    "        imp_opi_logits = self.implicit_opinion_classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        # --- FASE DI ADDESTRAMENTO (Calcolo della Loss) ---\n",
    "        if labels is not None and implicit_aspect_labels is not None and implicit_opinion_labels is not None:\n",
    "            device = input_ids.device\n",
    "            \n",
    "            # --- NOVITÀ 1: LOSS DEL CRF ---\n",
    "            # Il CRF non tollera le label \"-100\" (usate spesso per il padding in HF).\n",
    "            # Creiamo una maschera valida e sostituiamo i -100 con 0 per sicurezza.\n",
    "            valid_mask = (labels >= 0) & attention_mask.bool()\n",
    "            safe_labels = torch.where(labels >= 0, labels, torch.zeros_like(labels))\n",
    "            \n",
    "            # Calcoliamo la loss negativa della log-likelihood del CRF\n",
    "            loss_token = -self.crf(emissions, safe_labels, mask=valid_mask, reduction='mean')\n",
    "            \n",
    "            # --- NOVITÀ 2: Pesi per gli Impliciti (Intatti!) ---\n",
    "            # Classe 0 (Esplicito) = 1.0. Classe 1 (Implicito) = 5.0.\n",
    "            implicit_weights = torch.tensor([1.0, 5.0], device=device)\n",
    "            loss_fct_implicit = nn.CrossEntropyLoss(weight=implicit_weights)\n",
    "            \n",
    "            loss_asp = loss_fct_implicit(imp_asp_logits, implicit_aspect_labels)\n",
    "            loss_opi = loss_fct_implicit(imp_opi_logits, implicit_opinion_labels)\n",
    "            \n",
    "            # --- NOVITÀ 3: Moltiplicatori della Loss Multi-Task ---\n",
    "            loss = loss_token + (1.5 * loss_asp) + (2.0 * loss_opi)\n",
    "            \n",
    "            \n",
    "        # --- FASE DI INFERENZA (Decodifica) ---\n",
    "        # Usiamo l'algoritmo di Viterbi per trovare la sequenza grammaticalmente perfetta!\n",
    "        mask_crf = attention_mask.bool()\n",
    "        token_preds = self.crf.decode(emissions, mask=mask_crf)\n",
    "        \n",
    "        # Poiché il CRF restituisce liste di lunghezza variabile (taglia via il padding),\n",
    "        # le ri-paddiamo con zeri per restituire un tensore uniforme e non rompere il tuo test.\n",
    "        batch_size = input_ids.shape[0]\n",
    "        max_seq_length = input_ids.shape[1]\n",
    "        padded_preds = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pred = token_preds[i]\n",
    "            pad_len = max_seq_length - len(pred)\n",
    "            padded_preds.append(pred + [0] * pad_len)\n",
    "            \n",
    "        # Restituiamo il tensore finale\n",
    "        token_preds_tensor = torch.tensor(padded_preds, device=input_ids.device)\n",
    "            \n",
    "        return {\n",
    "            \"loss\": loss, \n",
    "            \"token_logits\": token_preds_tensor,  # ORA CONTIENE GIA' I TAG DECISI!\n",
    "            \"imp_asp_logits\": imp_asp_logits, \n",
    "            \"imp_opi_logits\": imp_opi_logits\n",
    "        }\n",
    "\n",
    "print(\"Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\")\n",
    "\n",
    "# Inizializziamo il nostro modello custom invece di AutoModelForTokenClassification\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=NUM_LABELS)\n",
    "\n",
    "# Spostiamo il modello sul dispositivo di calcolo (GPU/MPS/CPU)\n",
    "model_step1.to(device)\n",
    "\n",
    "# --- 2. CONFIGURAZIONE DELL'OTTIMIZZATORE E DELLA LOSS ---\n",
    "\n",
    "# Manteniamo la tua ottima scelta di usare l'optimizer a 8-bit per non saturare la memoria!\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=2e-5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELLO MULTI-TASK (CON CRF) PRONTO PER IL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architettura: ModernBERT-base + Layer CRF (Custom ACOS Extractor)\")\n",
    "print(f\"Task: CRF Sequence Labeling + 2x Binary Classification (Impliciti)\")\n",
    "print(f\"Numero di Classi Token: {NUM_LABELS}\")\n",
    "print(f\"Optimizer: AdamW 8-bit (lr=2e-5)\")\n",
    "print(f\"Loss Function: Neg Log-Likelihood (CRF) + 2x CrossEntropy (Impliciti)\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e772824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attivazione Ottimizzazioni di Memoria...\n",
      "\n",
      "Training Multi-Task su RESTAURANT: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti: ogni 4 step | FP16: Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1:   3%|▎         | 3/96 [00:01<00:39,  2.36it/s, loss=40.6]/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoca 1: 100%|██████████| 96/96 [00:30<00:00,  3.18it/s, loss=23.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 33.0723 | Valid Loss: 17.7910\n",
      "Miglior modello trovato (Loss: 17.7910)\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|██████████| 96/96 [00:29<00:00,  3.27it/s, loss=17.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 17.0509 | Valid Loss: 12.5556\n",
      "Miglior modello trovato (Loss: 12.5556)\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|██████████| 96/96 [00:29<00:00,  3.25it/s, loss=14.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.7090 | Valid Loss: 10.0611\n",
      "Miglior modello trovato (Loss: 10.0611)\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|██████████| 96/96 [00:30<00:00,  3.20it/s, loss=8.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.4810 | Valid Loss: 8.4874\n",
      "Miglior modello trovato (Loss: 8.4874)\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|██████████| 96/96 [00:31<00:00,  3.07it/s, loss=8.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.2710 | Valid Loss: 7.1114\n",
      "Miglior modello trovato (Loss: 7.1114)\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|██████████| 96/96 [00:29<00:00,  3.22it/s, loss=7.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.4828 | Valid Loss: 6.9683\n",
      "Miglior modello trovato (Loss: 6.9683)\n",
      "\n",
      "--- Epoca 7/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 7: 100%|██████████| 96/96 [00:29<00:00,  3.23it/s, loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8234 | Valid Loss: 7.1423\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 8/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 8: 100%|██████████| 96/96 [00:30<00:00,  3.13it/s, loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5802 | Valid Loss: 7.8128\n",
      "Nessun miglioramento. Patience: 2/5\n",
      "\n",
      "--- Epoca 9/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 9: 100%|██████████| 96/96 [00:30<00:00,  3.10it/s, loss=1.99] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7012 | Valid Loss: 7.9876\n",
      "Nessun miglioramento. Patience: 3/5\n",
      "\n",
      "--- Epoca 10/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 10: 100%|██████████| 96/96 [00:29<00:00,  3.21it/s, loss=0.919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1301 | Valid Loss: 8.6259\n",
      "Nessun miglioramento. Patience: 4/5\n",
      "\n",
      "--- Epoca 11/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 11: 100%|██████████| 96/96 [00:29<00:00,  3.23it/s, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6777 | Valid Loss: 9.0897\n",
      "Nessun miglioramento. Patience: 5/5\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 11.\n",
      "\n",
      "Fine Addestramento Multi-Task.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▇▆▆▅▅▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▂▂▁▁▁▁</td></tr><tr><td>valid_loss_epoch</td><td>█▅▃▂▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.72757</td></tr><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss_epoch</td><td>0.67766</td></tr><tr><td>valid_loss_epoch</td><td>9.08971</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step1_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/vy0i75kr' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/vy0i75kr</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260226_143206-vy0i75kr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA E WANDB ---\n",
    "print(\"Attivazione Ottimizzazioni di Memoria...\")\n",
    "\n",
    "# Gradient Checkpointing: si applica all'encoder interno (BERT) della nostra classe custom\n",
    "model_step1.bert.gradient_checkpointing_enable()\n",
    "\n",
    "accumulation_steps = wandb.config.get('accumulation_steps', 4) \n",
    "patience = wandb.config.get('patience', 5)\n",
    "epochs = wandb.config.get('epochs', 40)\n",
    "lr = wandb.config.get('learning_rate', 2e-5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Optimizer a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=lr)\n",
    "\n",
    "# Scaler per Mixed Precision (FP16)\n",
    "scaler = GradScaler() \n",
    "\n",
    "total_steps = (len(train_loader_rest) // accumulation_steps) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (MULTI-TASK) ---\n",
    "\n",
    "def evaluate_model_multitask(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "            imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast in valutazione per risparmiare memoria\n",
    "            #nel caso il trianing dia errori di memoria, commenta la riga con autocast\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels,\n",
    "                    implicit_aspect_labels=imp_asp_labels,\n",
    "                    implicit_opinion_labels=imp_opi_labels\n",
    "                )\n",
    "            \n",
    "            total_loss += outputs['loss'].item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_multitask(model, data_loader, optimizer, scheduler, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass (FP16)\n",
    "        #nel caso il trianing dia errori di memoria, commenta la riga con autocast\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels,\n",
    "                implicit_aspect_labels=imp_asp_labels,\n",
    "                implicit_opinion_labels=imp_opi_labels\n",
    "            )\n",
    "            # Normalizziamo la loss per l'accumulo dei gradienti\n",
    "            loss = outputs['loss'] / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler (Evita underflow/overflow dell'FP16)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            \n",
    "            # GRADIENT CLIPPING\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            #nel caso di errori di memoria, commenta le righe con scaler.step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        wandb.log({\"batch_loss\": loss.item() * accumulation_steps})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"\\nTraining Multi-Task su RESTAURANT: {epochs} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti: ogni {accumulation_steps} step | FP16: Attivato\")\n",
    "\n",
    "best_valid_loss_rest = float('inf')\n",
    "output_dir = \"./best_multitask_extractor_restaurant\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{epochs} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_rest = train_epoch_multitask(model_step1, train_loader_rest, optimizer, scheduler, device, epoch, scaler, accumulation_steps)\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_rest = evaluate_model_multitask(model_step1, dev_loader_rest, device)\n",
    "    \n",
    "    # Pulizia spietata della cache della GPU a fine epoca!\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_rest:.4f} | Valid Loss: {valid_loss_rest:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_rest,\n",
    "        \"valid_loss_epoch\": valid_loss_rest\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_rest < best_valid_loss_rest:\n",
    "        best_valid_loss_rest = valid_loss_rest\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_rest:.4f})\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Salvataggio Custom per l'architettura Multi-Task\n",
    "        torch.save(model_step1.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Multi-Task.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6584a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello Multi-Task migliore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694caf8d5e474334b260f6d17b9724d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inizio Test Multi-Task sul Dataset Restaurant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test RESTAURANT: 100%|██████████| 37/37 [00:06<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\n",
      "============================================================\n",
      "Overall Precision: 0.6066\n",
      "Overall Recall:    0.6616\n",
      "Overall F1-Score:  0.6329\n",
      "\n",
      "Dettaglio per Classe (Quello che conta per il paper):\n",
      "--------------------------------------------------\n",
      "   ASP:\n",
      "   Precision: 0.5634\n",
      "   Recall:    0.5921\n",
      "   F1-Score:  0.5774\n",
      "   Support:   608\n",
      "   OPI:\n",
      "   Precision: 0.6443\n",
      "   Recall:    0.7269\n",
      "   F1-Score:  0.6831\n",
      "   Support:   648\n",
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\n",
      "============================================================\n",
      "Accuratezza Aspetti Impliciti: 0.5369\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.95      0.32      0.48       386\n",
      "Implicito (1)       0.42      0.96      0.58       197\n",
      "\n",
      "     accuracy                           0.54       583\n",
      "    macro avg       0.68      0.64      0.53       583\n",
      " weighted avg       0.77      0.54      0.51       583\n",
      "\n",
      "--------------------------------------------------\n",
      "Accuratezza Opinioni Implicite: 0.5352\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.97      0.37      0.53       419\n",
      "Implicito (1)       0.37      0.97      0.54       164\n",
      "\n",
      "     accuracy                           0.54       583\n",
      "    macro avg       0.67      0.67      0.54       583\n",
      " weighted avg       0.80      0.54      0.53       583\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- A. CARICAMENTO DEL \"CAMPIONE\" MULTI-TASK ---\n",
    "print(\"Caricamento del modello Multi-Task migliore...\")\n",
    "\n",
    "# 1. Inizializziamo la nostra architettura custom\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5)\n",
    "\n",
    "# 2. Carichiamo i pesi salvati del miglior modello (solo i pesi, non l'intero oggetto)\n",
    "model_path = \"./best_multitask_extractor_restaurant/pytorch_model.bin\"\n",
    "model_step1.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "model_step1.to(device)\n",
    "model_step1.eval() # Modalità esame (spegne dropout)\n",
    "\n",
    "# --- B. PREPARAZIONE METRICHE ---\n",
    "# Carichiamo la metrica seqeval (standard per NER/ABSA)\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Mappa per decodificare i numeri in etichette\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"\\nInizio Test Multi-Task sul Dataset Restaurant...\")\n",
    "\n",
    "# --- C. CICLO DI PREVISIONE ---\n",
    "predictions_tokens = []\n",
    "true_labels_tokens = []\n",
    "\n",
    "# Liste per salvare le predizioni binarie (Impliciti)\n",
    "true_imp_asp, pred_imp_asp = [], []\n",
    "true_imp_opi, pred_imp_opi = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_rest, desc=\"Test RESTAURANT\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "\n",
    "        # 1. Il modello fa le sue 3 predizioni contemporaneamente\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model_step1(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 2. Estraiamo i risultati dalle 3 testoline\n",
    "        token_logits = outputs['token_logits']\n",
    "        token_preds = token_logits # Il CRF ha già scelto!\n",
    "        asp_preds = torch.argmax(outputs['imp_asp_logits'], dim=-1)\n",
    "        opi_preds = torch.argmax(outputs['imp_opi_logits'], dim=-1)\n",
    "        \n",
    "        # Salviamo i risultati binari\n",
    "        true_imp_asp.extend(imp_asp_labels.cpu().tolist())\n",
    "        pred_imp_asp.extend(asp_preds.cpu().tolist())\n",
    "        \n",
    "        true_imp_opi.extend(imp_opi_labels.cpu().tolist())\n",
    "        pred_imp_opi.extend(opi_preds.cpu().tolist())\n",
    "\n",
    "        # 3. Convertiamo i numeri in etichette BIO (pulendo il padding)\n",
    "        for i in range(len(labels)):\n",
    "            true_label_row = []\n",
    "            pred_label_row = []\n",
    "            \n",
    "            for j in range(len(labels[i])):\n",
    "                # Ignoriamo i token di padding (dove attention_mask è 0 o la label è -100)\n",
    "                if labels[i][j] != -100 and attention_mask[i][j] == 1: \n",
    "                    true_label_row.append(id2label[labels[i][j].item()])\n",
    "                    pred_label_row.append(id2label[token_preds[i][j].item()])\n",
    "            \n",
    "            true_labels_tokens.append(true_label_row)\n",
    "            predictions_tokens.append(pred_label_row)\n",
    "\n",
    "# --- D. CALCOLO E STAMPA RISULTATI ---\n",
    "results_seq = metric.compute(predictions=predictions_tokens, references=true_labels_tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Overall Precision: {results_seq['overall_precision']:.4f}\")\n",
    "print(f\"Overall Recall:    {results_seq['overall_recall']:.4f}\")\n",
    "print(f\"Overall F1-Score:  {results_seq['overall_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nDettaglio per Classe (Quello che conta per il paper):\")\n",
    "print(\"-\" * 50)\n",
    "for key in results_seq.keys():\n",
    "    if key in ['ASP', 'OPI']: \n",
    "        print(f\"   {key}:\")\n",
    "        print(f\"   Precision: {results_seq[key]['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {results_seq[key]['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {results_seq[key]['f1']:.4f}\")\n",
    "        print(f\"   Support:   {results_seq[key]['number']}\") \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metriche per Aspetti Impliciti\n",
    "acc_asp = accuracy_score(true_imp_asp, pred_imp_asp)\n",
    "print(f\"Accuratezza Aspetti Impliciti: {acc_asp:.4f}\")\n",
    "print(classification_report(true_imp_asp, pred_imp_asp, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Metriche per Opinioni Implicite\n",
    "acc_opi = accuracy_score(true_imp_opi, pred_imp_opi)\n",
    "print(f\"Accuratezza Opinioni Implicite: {acc_opi:.4f}\")\n",
    "print(classification_report(true_imp_opi, pred_imp_opi, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e4cbc",
   "metadata": {},
   "source": [
    "## Classificatore Category-Sentiment (Extract-Classify-ACOS)\n",
    "\n",
    "Implementiamo il **secondo stadio** dell'architettura proposta nel paper. Dopo aver estratto gli Aspetti e le Opinioni nello Step 1, ora dobbiamo capire a quale Categoria appartengono e qual è il loro Sentiment.\n",
    "\n",
    "Il codice di preparazione è diviso in tre componenti fondamentali:\n",
    "\n",
    " 1. Il Dataset PyTorch (`ACOSPairDataset`)\n",
    "\n",
    " 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "\n",
    " 3. Inizializzazione e DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b46c0",
   "metadata": {},
   "source": [
    "### Creazione del Dataset e DataLoader per il training e il testing\n",
    "In questa cella definiamo la classe ACOSPairDataset per alimentare lo Step 2. Poiché abbiamo deciso di utilizzare un'architettura Cross-Encoder, il modo in cui prepariamo i dati è completamente diverso e molto più elegante rispetto agli approcci classici.\n",
    "\n",
    "Invece di passare al modello gli indici numerici di dove si trovano l'aspetto e l'opinione nella frase, traduciamo le coordinate in vero e proprio testo, creando una \"seconda frase\" artificiale da affiancare alla recensione originale.\n",
    "\n",
    "Ecco i passaggi chiave (\"La Magia Cross-Encoder\"):\n",
    "\n",
    "* Estrazione delle Parole: Il codice prende gli indici numerici (a_span, o_span) e li usa per ritagliare le parole esatte dalla frase originale. Se un elemento è implicito (coordinate -1), viene convertito automaticamente nella stringa testuale \"null\".\n",
    "\n",
    "* Creazione del Prompt (Cross-Text): Viene creata una nuova stringa di contesto formattata esattamente così: aspect: [parola] opinion: [parola].\n",
    "\n",
    "* Tokenizzazione a Doppia Frase: Sfruttiamo una funzionalità nativa (e potentissima) del Tokenizer di HuggingFace. Passandogli due stringhe separate (text e cross_text), il tokenizer le unisce in automatico inserendo il token separatore in mezzo:\n",
    "[CLS] Testo della recensione originale [SEP] aspect: pizza opinion: buonissima [SEP]\n",
    "\n",
    "* Semplificazione dell'Output: Grazie a questa formattazione testuale, il modello leggerà contemporaneamente la frase e la coppia da analizzare, calcolandone l'interazione tramite la Self-Attention. Di conseguenza, il Dataset restituisce solo gli input_ids e le labels, senza più bisogno di complicati tensori con le posizioni degli span!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a13b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACOSPairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        \n",
    "        # Estraiamo gli span (togliendo il +1 che avevamo messo per il [CLS] \n",
    "        # perché ora ci servono per tagliare la stringa originale)\n",
    "        a_span = row['aspect_span']\n",
    "        o_span = row['opinion_span']\n",
    "        \n",
    "        words = text.split()\n",
    "        a_start, a_end = a_span[0] - 1, a_span[1] - 1\n",
    "        o_start, o_end = o_span[0] - 1, o_span[1] - 1\n",
    "        \n",
    "        # Estraiamo le parole (o \"null\" se è implicito)\n",
    "        aspect_str = \" \".join(words[a_start:a_end]) if a_start >= 0 else \"null\"\n",
    "        opinion_str = \" \".join(words[o_start:o_end]) if o_start >= 0 else \"null\"\n",
    "        \n",
    "        # MAGIA CROSS-ENCODER: Creiamo la stringa contesto!\n",
    "        cross_text = f\"aspect: {aspect_str} opinion: {opinion_str}\"\n",
    "\n",
    "        # Il tokenizer unirà il 'text' e il 'cross_text' in automatico\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            cross_text, # Passiamo la seconda stringa!\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "            # Niente più aspect_span e opinion_span da passare!\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe6e74",
   "metadata": {},
   "source": [
    "### 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "In questa cella definiamo il modello per lo Step 2 della nostra pipeline. Il suo compito è prendere in input la frase originale abbinata a una specifica coppia Aspetto/Opinione e determinare a quale Categoria appartiene e con quale Sentimento.\n",
    "\n",
    "Rispetto al codice originale del paper del 2021, questa architettura presenta un upgrade ingegneristico fondamentale: Il passaggio da Bi-Encoder a Cross-Encoder.\n",
    "\n",
    "Ecco le innovazioni chiave di questa classe:\n",
    "\n",
    "* Architettura Cross-Encoder (Dimensione 768): Nel paper originale, gli autori estraevano i vettori separati dell'aspetto e dell'opinione (768 + 768) e li concatenavano in un vettore da 1536 dimensioni (Bi-Encoder). Noi, invece, passiamo al modello un'unica stringa formattata (es. Testo recensione + \"aspect: X opinion: Y\"). Questo permette al meccanismo di Self-Attention di calcolare l'interazione tra la coppia e il testo in modo nativo. Di conseguenza, ci basta usare il singolo vettore [CLS] da 768 dimensioni!\n",
    "\n",
    "* Weight Sharing (Continuità di Apprendimento): Invece di inizializzare un ModernBERT da zero, carichiamo i pesi del modello \"vincitore\" dallo Step 1 (path_to_best_model). In questo modo, lo Step 2 eredita tutta la comprensione linguistica e sintattica che l'Investigatore ha già imparato sul nostro dominio specifico.\n",
    "\n",
    "* Teste di Classificazione Multiple (ModuleList): Abbiamo creato dinamicamente un array di teste lineari indipendenti, una per ogni singola categoria del dataset (es. 121 teste per i ristoranti).\n",
    "\n",
    "* Filtro Integrato (Classi di output = 4): Ogni testa restituisce 4 logit. Le prime tre classi rappresentano il Sentimento (Positivo, Negativo, Neutro). La quarta classe (Classe 3) funge da \"Buttafuori\": se il modello ritiene che la coppia Aspetto/Opinione sia falsa o non c'entri nulla con quella categoria, la classifica come \"Invalida\", scartando i falsi positivi generati dal prodotto cartesiano dello Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8a0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBertACOSClassifier(nn.Module):\n",
    "    def __init__(self, path_to_best_model, num_categories):\n",
    "        super(ModernBertACOSClassifier, self).__init__()\n",
    "        \n",
    "        # Carichiamo il corpo dal modello Step 1\n",
    "        self.modernbert = AutoModel.from_pretrained(path_to_best_model)\n",
    "        hidden_size = self.modernbert.config.hidden_size # 768\n",
    "        \n",
    "        # Le 121 teste ORA PRENDONO 768 (non più 1536)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, 4) for _ in range(num_categories)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask): # Span rimossi dai parametri!\n",
    "        outputs = self.modernbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Prendiamo semplicemente il token [CLS] dell'intera sequenza Cross-Encoder\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :] \n",
    "        cls_output = self.dropout(cls_output)\n",
    "\n",
    "        # Passiamo il vettore nelle teste lineari\n",
    "        logits = [head(cls_output) for head in self.heads]\n",
    "        \n",
    "        return torch.stack(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebed1d",
   "metadata": {},
   "source": [
    "### 3. Inizializzazione e DataLoaders\n",
    "L'ultimo blocco carica fisicamente i file salvati dalla nostra \"Fabbrica dei Dati\", istanzia i `Dataset`, e crea i `DataLoader` (con batch size = 16) per \"nutrire\" la GPU in modo efficiente durante l'addestramento. Infine, sposta il modello sulla scheda video (CUDA) pronto per il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2509e976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e99b0fd1404b0caadfd8ba5cdc0728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_restaurant\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello inizializzato! Categorie: 13 | Device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. Carichiamo la lista delle categorie salvata prima\n",
    "with open(\"data_coppie/restaurant_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list) # 121\n",
    "\n",
    "# 2. Carichiamo i DataFrame di Train e Dev\n",
    "df_train = pd.read_pickle(\"data_coppie/train_restaurant_pairs.pkl\")\n",
    "df_dev = pd.read_pickle(\"data_coppie/dev_restaurant_pairs.pkl\")\n",
    "df_test = pd.read_pickle(\"data_coppie/test_restaurant_pairs.pkl\")\n",
    "\n",
    "# 3. Inizializziamo il Tokenizer e i Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "train_dataset = ACOSPairDataset(df_train, tokenizer)\n",
    "dev_dataset = ACOSPairDataset(df_dev, tokenizer)\n",
    "test_dataset = ACOSPairDataset(df_test, tokenizer) \n",
    "\n",
    "# 4. Creiamo i DataLoader (Batch size 16 è un buon compromesso tra velocità e VRAM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dev_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16) \n",
    "# 5. Inizializziamo il Modello usando i pesi dello Step 1!\n",
    "model = ModernBertACOSClassifier(\"./best_model_restaurant\", num_categories)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modello inizializzato! Categorie: {num_categories} | Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834f631",
   "metadata": {},
   "source": [
    "## WANDB per lo step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274d34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cristinatomaciello/Desktop/Università/2anno/1semstre/big data/progetto-text-mining/wandb/run-20260226_172601-7wbflpgv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/7wbflpgv' target=\"_blank\">Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/7wbflpgv' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/7wbflpgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS\n"
     ]
    }
   ],
   "source": [
    "# Chiudiamo per sicurezza qualsiasi run precedente rimasta aperta nello stesso notebook\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters per lo STEP 2\n",
    "config_step2 = {\n",
    "    \"learning_rate\": 2e-5, # Solitamente per lo Step 2 un LR leggermente più basso è meglio (es. 2e-5)\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 4, # Aggiunto per il tuo training loop ottimizzato!\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Restaurant-ACOS\", \n",
    "    \"seed\": 42,\n",
    "    \"patience\": 5  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run per lo Step 2\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config_step2,\n",
    "    # Aggiungiamo \"Step2_Class\" al nome per distinguerlo dallo Step 1\n",
    "    name=f\"Step2_Class_{config_step2['model_name']}_{config_step2['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\" W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f483c",
   "metadata": {},
   "source": [
    "## Train su Sentiment e Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ca2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training STEP 2 su RESTAURANT: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti ogni 4 step | FP16 Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1: 100%|██████████| 398/398 [01:21<00:00,  4.88it/s, loss=0.184] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2675 | Valid Loss: 0.2097\n",
      "Miglior modello trovato (Loss: 0.2097)! Salvataggio...\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|██████████| 398/398 [01:21<00:00,  4.90it/s, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1697 | Valid Loss: 0.1702\n",
      "Miglior modello trovato (Loss: 0.1702)! Salvataggio...\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|██████████| 398/398 [01:21<00:00,  4.88it/s, loss=0.137] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1332 | Valid Loss: 0.1455\n",
      "Miglior modello trovato (Loss: 0.1455)! Salvataggio...\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|██████████| 398/398 [01:22<00:00,  4.85it/s, loss=0.0966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1069 | Valid Loss: 0.1389\n",
      "Miglior modello trovato (Loss: 0.1389)! Salvataggio...\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|██████████| 398/398 [01:22<00:00,  4.80it/s, loss=0.0642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0816 | Valid Loss: 0.1258\n",
      "Miglior modello trovato (Loss: 0.1258)! Salvataggio...\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|██████████| 398/398 [01:22<00:00,  4.82it/s, loss=0.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0584 | Valid Loss: 0.1262\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 7/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 7: 100%|██████████| 398/398 [01:23<00:00,  4.76it/s, loss=0.0808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0406 | Valid Loss: 0.1210\n",
      "Miglior modello trovato (Loss: 0.1210)! Salvataggio...\n",
      "\n",
      "--- Epoca 8/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 8: 100%|██████████| 398/398 [01:21<00:00,  4.85it/s, loss=0.0327] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0264 | Valid Loss: 0.1288\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 9/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 9: 100%|██████████| 398/398 [01:22<00:00,  4.84it/s, loss=0.0248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0271 | Valid Loss: 0.1316\n",
      "Nessun miglioramento. Patience: 2/5\n",
      "\n",
      "--- Epoca 10/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 10: 100%|██████████| 398/398 [01:22<00:00,  4.84it/s, loss=0.00609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0161 | Valid Loss: 0.1281\n",
      "Nessun miglioramento. Patience: 3/5\n",
      "\n",
      "--- Epoca 11/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 11: 100%|██████████| 398/398 [01:22<00:00,  4.80it/s, loss=0.0149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0129 | Valid Loss: 0.1280\n",
      "Nessun miglioramento. Patience: 4/5\n",
      "\n",
      "--- Epoca 12/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 12: 100%|██████████| 398/398 [01:22<00:00,  4.83it/s, loss=0.00632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0089 | Valid Loss: 0.1340\n",
      "Nessun miglioramento. Patience: 5/5\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 12.\n",
      "\n",
      "Fine Addestramento Step 2 (Restaurant).\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>██▇▅▆▇▅▅▇▃▄▅▃▂▃▃▅▃▄▄▂▄▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▃▂▂▁▁▁▁▁</td></tr><tr><td>valid_loss_epoch</td><td>█▅▃▂▁▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.00632</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>train_loss_epoch</td><td>0.0089</td></tr><tr><td>valid_loss_epoch</td><td>0.13398</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/bo1rte1y' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/bo1rte1y</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260225_193009-bo1rte1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "# Gradient Checkpointing: abilitato SOLO sul corpo di ModernBERT\n",
    "model.modernbert.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config_step2.get('accumulation_steps', 4) \n",
    "patience = config_step2.get('patience', 5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Ottimizzatore AdamW a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config_step2['learning_rate']\n",
    ")\n",
    "\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "# Un bilanciamento molto più equo! Il modello non avrà più il terrore di usare \"Invalido\"\n",
    "weights = torch.tensor([2.0, 2.0, 2.0, 1.0]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Calcolo degli step totali per lo scheduler\n",
    "total_steps = (len(train_loader) // accumulation_steps) * config_step2['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (STEP 2) ---\n",
    "\n",
    "def evaluate_model_step2(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda'):\n",
    "                # RIMOSSI aspect_spans e opinion_spans\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_step2(model, data_loader, optimizer, scheduler, criterion, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            # RIMOSSI aspect_spans e opinion_spans\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            loss = loss / accumulation_steps \n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        real_loss = loss.item() * accumulation_steps\n",
    "        total_loss += real_loss\n",
    "        \n",
    "        wandb.log({\"batch_loss\": real_loss})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=real_loss)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"Training STEP 2 su RESTAURANT: {config_step2['epochs']} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_restaurant = float('inf')\n",
    "\n",
    "for epoch in range(config_step2['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config_step2['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_restaurant = train_epoch_step2(\n",
    "        model, train_loader, optimizer, scheduler, criterion, \n",
    "        device, epoch, scaler, accumulation_steps\n",
    "    )\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_restaurant = evaluate_model_step2(model, val_loader, criterion, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_restaurant:.4f} | Valid Loss: {valid_loss_restaurant:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_restaurant,\n",
    "        \"valid_loss_epoch\": valid_loss_restaurant\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_restaurant < best_valid_loss_restaurant:\n",
    "        best_valid_loss_restaurant = valid_loss_restaurant\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_restaurant:.4f})! Salvataggio...\")\n",
    "        \n",
    "        # Salvataggio custom model nella cartella dei RESTAURANT\n",
    "        save_dir = \"./best_classifier_restaurant\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Step 2 (Restaurant).\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac5bb2",
   "metadata": {},
   "source": [
    "### Test su Sentiment e Categoria\n",
    "In questa cella effettuiamo il collaudo dello Step 2 sul dataset di Test e andiamo alla ricerca della calibrazione perfetta per le sue predizioni.\n",
    "\n",
    "Il codice si divide in tre fasi fondamentali:\n",
    "\n",
    "1. Caricamento e Inferenza (Estrazione Probabilità):\n",
    "Inizializziamo il modello ModernBertACOSClassifier e carichiamo i pesi addestrati. Successivamente, facciamo passare tutto il set di test all'interno della rete, ma invece di fargli prendere subito una decisione netta, estraiamo le probabilità grezze (Softmax) per ogni classe (Positivo, Negativo, Neutro, Invalido).\n",
    "\n",
    "2. Perché facciamo un ciclo for (Grid Search)?\n",
    "Il nostro modello deve decidere se una coppia Aspetto/Opinione è valida e, in caso affermativo, assegnarle un sentimento. Non possiamo fidarci ciecamente della probabilità più alta in assoluto (argmax). A volte i modelli neurali sono troppo \"spavaldi\" o troppo \"timidi\".\n",
    "Attraverso il ciclo for, eseguiamo una Grid Search (Ricerca a Griglia): testiamo pazientemente ogni singola soglia di confidenza dal 50% al 99% (a scatti dell'1%). Per ogni soglia, diciamo al modello: \"Accetta questo sentimento SOLO se sei sicuro almeno al X%, altrimenti scarta la coppia nella Classe 3 (Invalido)\".\n",
    "\n",
    "3. In base a cosa scegliamo la soglia migliore?\n",
    "Come si vede nel codice, salviamo la soglia che massimizza il Macro F1-Score (if current_macro_f1 > best_macro_f1).\n",
    "Perché proprio il Macro F1 e non il Micro F1 o l'accuratezza globale?\n",
    "Nei dataset di recensioni, le classi sono spesso molto sbilanciate (es. ci sono tantissime recensioni \"Positive\" e pochissime \"Neutre\"). Se usassimo il Micro F1, il modello potrebbe barare, imparando a identificare benissimo solo la classe maggioritaria e ignorando le altre. Il Macro F1, invece, calcola la precisione e il recall per ogni singolo sentimento (Positivo, Negativo, Neutro) in modo indipendente e ne fa la media matematica.\n",
    "\n",
    "Scegliere la soglia basandoci sul Macro F1 ci garantisce di trovare il punto di equilibrio perfetto: un modello robusto, equilibrato, e capace di riconoscere con la stessa efficacia sia i complimenti che le critiche!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd7ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/7wbflpgv' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/7wbflpgv</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260226_172601-7wbflpgv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cristinatomaciello/Desktop/Università/2anno/1semstre/big data/progetto-text-mining/wandb/run-20260226_172614-6j8mmxzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6j8mmxzo' target=\"_blank\">TEST_Step2_answerdotai/ModernBERT-base_Restaurant-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6j8mmxzo' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6j8mmxzo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8890b4fa14464efabb0bc8c453b41c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_restaurant\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CHECK CARICAMENTO PESI ---\n",
      "Chiavi Inaspettate (OK se sono dello Step 1): 0\n",
      "Chiavi Mancanti (PROBLEMA se sono 'heads'): 0\n",
      "SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\n",
      "Estrazione di tutte le probabilità dal modello in corso (attendere)...\n",
      "\n",
      "🔍 Avvio Grid Search per la migliore Soglia di Confidenza (Macro F1)...\n",
      "IL MILGIOR THRESHOLD E'= 0.57 (57%)\n",
      "\n",
      "--- MIGLIOR CLASSIFICATION REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Negative (0)       0.51      0.53      0.52       205\n",
      " Neutral (1)       0.50      0.02      0.04        44\n",
      "Positive (2)       0.68      0.64      0.66       667\n",
      "\n",
      "   micro avg       0.64      0.58      0.61       916\n",
      "   macro avg       0.56      0.40      0.41       916\n",
      "weighted avg       0.63      0.58      0.60       916\n",
      "\n",
      "BEST MICRO F1-Score: 0.6074\n",
      "CORRISPONDENTE MACRO F1: 0.4057\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>▁</td></tr><tr><td>best_test_micro_f1</td><td>▁</td></tr><tr><td>optimal_threshold</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>0.40569</td></tr><tr><td>best_test_micro_f1</td><td>0.60741</td></tr><tr><td>optimal_threshold</td><td>0.57</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TEST_Step2_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6j8mmxzo' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/6j8mmxzo</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260226_172614-6j8mmxzo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chiudiamo run appese\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=f\"TEST_Step2_{config_step2['model_name']}_{config_step2['dataset']}\",\n",
    "    job_type=\"test\"\n",
    ")\n",
    "\n",
    "# Ricreiamo l'architettura del modello\n",
    "num_categories = len(category_list)\n",
    "model_test = ModernBertACOSClassifier(\"./best_model_restaurant\", num_categories)\n",
    "\n",
    "# Carichiamo i pesi dello Step 2\n",
    "model_path = \"./best_classifier_restaurant/pytorch_model.bin\"\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# USIAMO STRICT=FALSE\n",
    "missing_keys, unexpected_keys = model_test.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"\\n--- CHECK CARICAMENTO PESI ---\")\n",
    "print(f\"Chiavi Inaspettate (OK se sono dello Step 1): {len(unexpected_keys)}\")\n",
    "print(f\"Chiavi Mancanti (PROBLEMA se sono 'heads'): {len(missing_keys)}\")\n",
    "\n",
    "heads_missing = [k for k in missing_keys if \"heads\" in k]\n",
    "if heads_missing:\n",
    "    print(f\"ERRORE CRITICO: Le teste di classificazione non sono state caricate! {heads_missing[:5]}\")\n",
    "else:\n",
    "    print(\"SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\")\n",
    "    \n",
    "model_test.to(device)\n",
    "model_test.eval()\n",
    "\n",
    "# --- 1. ESTRAZIONE PROBABILITA' ---\n",
    "print(\"Estrazione di tutte le probabilità dal modello in corso (attendere)...\")\n",
    "\n",
    "all_probs_list = []\n",
    "all_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader: \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        # Gestione sicura dell'autocast basata sull'hardware\n",
    "        if device.type == 'cuda':\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                # RIMOSSI aspect_spans e opinion_spans\n",
    "                logits = model_test(input_ids, attention_mask)\n",
    "        else:\n",
    "            # Per Mac MPS o CPU facciamo il forward pass standard\n",
    "            logits = model_test(input_ids, attention_mask)\n",
    "            \n",
    "        probs = torch.softmax(logits, dim=-1).cpu() \n",
    "        \n",
    "        all_probs_list.append(probs)\n",
    "        all_true_list.append(labels.cpu())\n",
    "\n",
    "all_probs = torch.cat(all_probs_list, dim=0) \n",
    "all_true = torch.cat(all_true_list, dim=0).numpy().flatten()\n",
    "\n",
    "\n",
    "# --- 2. GRID SEARCH SUL THRESHOLD (OTTIMIZZATA PER MACRO F1) ---\n",
    "print(\"\\n🔍 Avvio Grid Search per la migliore Soglia di Confidenza (Macro F1)...\")\n",
    "\n",
    "thresholds_to_test = np.arange(0.50, 1.00, 0.01) \n",
    "best_micro_f1 = 0.0\n",
    "best_macro_f1 = 0.0\n",
    "best_threshold = 0.0\n",
    "best_report = \"\"\n",
    "\n",
    "target_names = ['Negative (0)', 'Neutral (1)', 'Positive (2)']\n",
    "labels_to_eval = [0, 1, 2]\n",
    "# --- AGGIUNGI QUESTE DUE RIGHE QUI ---\n",
    "prob_invalid = all_probs[:, :, 3] \n",
    "best_sentiment_preds = torch.argmax(all_probs[:, :, :3], dim=-1)\n",
    "\n",
    "# INIZIO CICLO SILENZIOSO\n",
    "for thresh in thresholds_to_test:\n",
    "    # 1. Partiamo fiduciosi: diamo a tutto il miglior sentimento predetto\n",
    "    final_preds = best_sentiment_preds.clone()\n",
    "    \n",
    "    # 2. Il Buttafuori: se l'Invalido (Classe 3) supera la soglia, scartiamo la quadrupla\n",
    "    mask_invalid = prob_invalid > thresh\n",
    "    final_preds[mask_invalid] = 3\n",
    "    \n",
    "    preds_flat = final_preds.numpy().flatten()\n",
    "    \n",
    "    current_micro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='micro')\n",
    "    current_macro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='macro')\n",
    "    \n",
    "    # ORA VINCE CHI ALZA IL MACRO F1!\n",
    "    if current_macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = current_macro_f1\n",
    "        best_micro_f1 = current_micro_f1\n",
    "        best_threshold = thresh\n",
    "        best_report = classification_report(\n",
    "            all_true, preds_flat, labels=labels_to_eval, \n",
    "            target_names=target_names, zero_division=0\n",
    "        )\n",
    "\n",
    "# --- 3. STAMPA DEI RISULTATI VINCITORI E LOG W&B ---\n",
    "print(f\"IL MILGIOR THRESHOLD E'= {best_threshold:.2f} ({(best_threshold*100):.0f}%)\")\n",
    "\n",
    "print(\"\\n--- MIGLIOR CLASSIFICATION REPORT ---\")\n",
    "print(best_report)\n",
    "print(f\"BEST MICRO F1-Score: {best_micro_f1:.4f}\")\n",
    "print(f\"CORRISPONDENTE MACRO F1: {best_macro_f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "wandb.log({\n",
    "    \"best_test_micro_f1\": best_micro_f1,\n",
    "    \"best_test_macro_f1\": best_macro_f1,\n",
    "    \"optimal_threshold\": best_threshold\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0e040",
   "metadata": {},
   "source": [
    "### Test sulla quadrupla completa (Aspetto-Opinione-Categoria-Sentiment)\n",
    "In questa cella mettiamo finalmente alla prova l'intera architettura che abbiamo costruito, unendo le forze dello Step 1 (L'Investigatore) e dello Step 2 (Lo Psicologo) per estrarre le quadruple complete direttamente dal testo grezzo.\n",
    "\n",
    "Questo è il banco di prova definitivo. Il codice esegue i seguenti passaggi fondamentali:\n",
    "\n",
    "1. Ripristino dei Pesi: Carichiamo le configurazioni e i pesi ottimali (pytorch_model.bin) che i nostri due modelli ModernBERT hanno appreso durante le rispettive fasi di training.\n",
    "\n",
    "2. L'Estrazione (Step 1): La funzione predict_quadruples_e2e passa la frase al primo modello. Il Layer CRF decodifica direttamente la sequenza perfetta dei tag (senza più bisogno di argmax), isolando le parole scritte. Le due teste binarie valutano invece l'eventuale presenza di elementi impliciti (-1, -1).\n",
    "\n",
    "3. Il Cross-Encoder in Azione (Step 2): Il codice crea tutte le combinazioni possibili (Prodotto Cartesiano) tra Aspetti e Opinioni trovate. Per ogni coppia, re-tokenizza la frase originale aggiungendo il nostro prompt (\"aspect: X opinion: Y\") e la passa allo Psicologo.\n",
    "\n",
    "4. Filtraggio Intelligente: Il modello analizza le 13 categorie simultaneamente. Grazie all'Hard Negative Sampling visto in addestramento, ci fidiamo ciecamente del suo giudizio: rimuoviamo ogni soglia manuale di probabilità e scartiamo solo le coppie che il modello classifica esplicitamente come \"Invalide\" (Classe 3).\n",
    "\n",
    "5. Valutazione Exact Match: Confrontiamo le quadruple predette con la Ground Truth (la verità di base) del dataset di test. La valutazione è spietata: per essere considerata corretta (True Positive), la quadrupla predetta deve corrispondere esattamente all'originale in tutte e 4 le dimensioni (Aspetto, Opinione, Categoria, Sentimento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9ffa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento configurazioni...\n",
      "Caricamento Step 1 (L'Investigatore Multi-Task)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8872624a65904420832f0fd77aa04669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento Step 2 (Lo Psicologo Classificatore)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80418ff9b89e46408f4ea57e006bffa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_restaurant\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avvio Valutazione End-to-End sul Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi Frasi: 100%|██████████| 583/583 [03:01<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RISULTATI FINALI EXACT MATCH ACOS (RESTAURANT)\n",
      "==================================================\n",
      "Quadruple Reali totali:    916\n",
      "Quadruple Predette totali: 832\n",
      "Quadruple Esatte:          357\n",
      "--------------------------------------------------\n",
      "Precision: 0.4291\n",
      "Recall:    0.3897\n",
      "F1-Score:  0.4085\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CARICAMENTO DEI MODELLI E PREPARATIVI\n",
    "# ==========================================\n",
    "print(\"Caricamento configurazioni...\")\n",
    "with open(\"data_coppie/restaurant_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list)\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"Caricamento Step 1 (L'Investigatore Multi-Task)...\")\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5).to(device)\n",
    "model_step1.load_state_dict(torch.load(\"./best_multitask_extractor_restaurant/pytorch_model.bin\", map_location=device, weights_only=True))\n",
    "model_step1.eval()\n",
    "\n",
    "print(\"Caricamento Step 2 (Lo Psicologo Classificatore)...\")\n",
    "model_step2 = ModernBertACOSClassifier(\"./best_model_restaurant\", num_categories).to(device)\n",
    "model_step2.load_state_dict(torch.load(\"./best_classifier_restaurant/pytorch_model.bin\", map_location=device, weights_only=True))\n",
    "model_step2.eval()\n",
    "\n",
    "# ==========================================\n",
    "# 2. FUNZIONI DI SUPPORTO PER L'ESTRAZIONE\n",
    "# ==========================================\n",
    "def get_spans(tags, b_tag, i_tag):\n",
    "    spans = []\n",
    "    start = -1\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == b_tag:\n",
    "            if start != -1: spans.append((start, i))\n",
    "            start = i\n",
    "        elif tag == i_tag and start != -1: continue\n",
    "        else:\n",
    "            if start != -1:\n",
    "                spans.append((start, i))\n",
    "                start = -1\n",
    "    if start != -1: spans.append((start, len(tags)))\n",
    "    return spans\n",
    "\n",
    "def predict_quadruples_e2e(text, model_1, model_2, tokenizer, cat_list):\n",
    "    \"\"\"La Pipeline End-to-End con filtri severi antidisturbo.\"\"\"\n",
    "    words = text.split()\n",
    "    inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True, max_length=128, padding='max_length').to(device)\n",
    "    \n",
    "    # --- FASE 1: L'Investigatore ---\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type=device.type):\n",
    "            out1 = model_1(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "    token_preds = out1['token_logits'][0].cpu().numpy()\n",
    "    \n",
    "    # usiamo l'argmax classico (sicurezza > 50%) per evitare allucinazioni!\n",
    "    imp_asp = torch.argmax(out1['imp_asp_logits'], dim=-1)[0].item()\n",
    "    imp_opi = torch.argmax(out1['imp_opi_logits'], dim=-1)[0].item()\n",
    "    \n",
    "    word_ids = inputs.word_ids()\n",
    "    word_tags = [\"O\"] * len(words)\n",
    "    \n",
    "    for idx, w_id in enumerate(word_ids):\n",
    "        if w_id is not None and w_id < len(words) and word_tags[w_id] == \"O\":\n",
    "            word_tags[w_id] = id2label[token_preds[idx]]\n",
    "                \n",
    "    asp_spans = get_spans(word_tags, \"B-ASP\", \"I-ASP\")\n",
    "    opi_spans = get_spans(word_tags, \"B-OPI\", \"I-OPI\")\n",
    "    \n",
    "    if imp_asp == 1 or len(asp_spans) == 0: asp_spans.append((-1, -1))\n",
    "    if imp_opi == 1 or len(opi_spans) == 0: opi_spans.append((-1, -1))\n",
    "    \n",
    "    asp_spans = list(set(asp_spans))\n",
    "    opi_spans = list(set(opi_spans))\n",
    "    \n",
    "    quadruples = []\n",
    "    \n",
    "# --- FASE 2: Lo Psicologo (Versione Cross-Encoder per Ristoranti!) ---\n",
    "    for a in asp_spans:\n",
    "        for o in opi_spans:\n",
    "            \n",
    "            # Estraiamo le stringhe\n",
    "            asp_str = \" \".join(words[a[0]:a[1]]) if a != (-1, -1) else \"null\"\n",
    "            opi_str = \" \".join(words[o[0]:o[1]]) if o != (-1, -1) else \"null\"\n",
    "            cross_text = f\"aspect: {asp_str} opinion: {opi_str}\"\n",
    "            \n",
    "            # Re-Tokenizziamo al volo per il Cross-Encoder\n",
    "            pair_inputs = tokenizer(\n",
    "                text, cross_text, \n",
    "                return_tensors=\"pt\", truncation=True, \n",
    "                max_length=128, padding='max_length'\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(device_type=device.type):\n",
    "                    # Niente più tensori t_a e t_o! Passiamo i nuovi input formattati\n",
    "                    out2 = model_2(\n",
    "                        input_ids=pair_inputs['input_ids'], \n",
    "                        attention_mask=pair_inputs['attention_mask']\n",
    "                    )\n",
    "            \n",
    "            logits = out2['logits'] if isinstance(out2, dict) else out2 \n",
    "            probs = torch.softmax(logits[0], dim=-1) \n",
    "            \n",
    "            for cat_idx, prob_dist in enumerate(probs):\n",
    "                \n",
    "                prob_invalido = prob_dist[3].item()\n",
    "                \n",
    "                # Se la probabilità di Invalido supera il 57%, scartiamo!\n",
    "                if prob_invalido > 0.57:\n",
    "                    continue\n",
    "                \n",
    "                # Altrimenti, ha passato il controllo! \n",
    "                best_sentiment = torch.argmax(prob_dist[:3]).item()\n",
    "                \n",
    "                quadruples.append({\n",
    "                    'aspect_span': a,\n",
    "                    'opinion_span': o,\n",
    "                    'category': cat_list[cat_idx],\n",
    "                    'sentiment': best_sentiment\n",
    "                })\n",
    "                    \n",
    "    return quadruples\n",
    "\n",
    "# ==========================================\n",
    "# 3. TEST SULL'INTERO DATASET RESTAURANT\n",
    "# ==========================================\n",
    "print(\"\\nAvvio Valutazione End-to-End sul Test Set...\")\n",
    "true_quads = []\n",
    "pred_quads = []\n",
    "\n",
    "percorso_file = os.path.join(\"data_parsing\", \"test_rest_parsed.pkl\")\n",
    "test_rest_parsed = pd.read_pickle(percorso_file)\n",
    "\n",
    "for idx, row in tqdm(test_rest_parsed.iterrows(), total=len(test_rest_parsed), desc=\"Analisi Frasi\"):\n",
    "    text = row['review_text']\n",
    "    preds = predict_quadruples_e2e(text, model_step1, model_step2, tokenizer, category_list)\n",
    "    \n",
    "    p_set = set()\n",
    "    for q in preds:\n",
    "        p_set.add((tuple(q['aspect_span']), q['category'], tuple(q['opinion_span']), q['sentiment']))\n",
    "    pred_quads.append(p_set)\n",
    "    \n",
    "    t_set = set()\n",
    "    for q in row['parsed_quadruples']:\n",
    "        a = tuple(q.get('span_A', [-1, -1]))\n",
    "        o = tuple(q.get('span_B', [-1, -1]))\n",
    "        c = q['category_aspect']\n",
    "        s = int(q['sentiment'])\n",
    "        t_set.add((a, c, o, s))\n",
    "    true_quads.append(t_set)\n",
    "\n",
    "# Metriche finali\n",
    "total_pred = sum(len(p) for p in pred_quads)\n",
    "total_true = sum(len(t) for t in true_quads)\n",
    "correct = sum(len(p_set.intersection(t_set)) for p_set, t_set in zip(pred_quads, true_quads))\n",
    "\n",
    "precision = correct / total_pred if total_pred > 0 else 0\n",
    "recall = correct / total_true if total_true > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RISULTATI FINALI EXACT MATCH ACOS (RESTAURANT)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Quadruple Reali totali:    {total_true}\")\n",
    "print(f\"Quadruple Predette totali: {total_pred}\")\n",
    "print(f\"Quadruple Esatte:          {correct}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
