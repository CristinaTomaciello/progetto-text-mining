{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7266c47e",
   "metadata": {},
   "source": [
    "# Estrazione delle quadruple (aspect-opinion-category-sentiment) con ModernBERT su Restaurant-ACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acceleratore Apple Metal (MPS) Trovato\n",
      "Librerie caricate.\n"
     ]
    }
   ],
   "source": [
    "# Import delle librerie necessarie\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup, AutoModel\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "import pickle\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler # Per Mixed Precision\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# --- 1. CONFIGURAZIONE DEL DEVICE ---\n",
    "# Se hai una GPU NVIDIA, user√† 'cuda'. Se hai un Mac M1/M2, user√† 'mps'. Altrimenti 'cpu'.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\" GPU Trovata: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\" Acceleratore Apple Metal (MPS) Trovato\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\" Nessuna GPU trovata. L'addestramento sar√† lento.\")\n",
    "\n",
    "print(\"Librerie caricate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70bed1",
   "metadata": {},
   "source": [
    "### Impostazioni per la riproducibilit√† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds impostati su 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Imposto i seed per la riproducibilit√†.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        # Imposto anche i seed per la GPU, se disponibile\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# Esegui l'impostazione del seed\n",
    "set_seed(42) \n",
    "print(\"Random seeds impostati su 42.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260222_171532-2o15ivh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/2o15ivh3' target=\"_blank\">run_answerdotai/ModernBERT-base_Restaurant-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/2o15ivh3' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/2o15ivh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: run_answerdotai/ModernBERT-base_Restaurant-ACOS\n"
     ]
    }
   ],
   "source": [
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Restaurant-ACOS\",  \n",
    "    \"seed\": 42,\n",
    "    'patience': 5  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config,\n",
    "    name=f\"Step1_Class_{config['model_name']}_{config['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\"W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84319964",
   "metadata": {},
   "source": [
    "## PyTorch Dataset & DataLoader Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114bba6",
   "metadata": {},
   "source": [
    "### Creazione di PyTorch Dataset e DataLoader\n",
    "Questa cella si occupa di caricare i dati pre-processati e di \"impacchettarli\" nel formato esatto richiesto dalla nostra nuova architettura PyTorch personalizzata. Rappresenta un passaggio cruciale per replicare fedelmente il paper originale, permettendoci di gestire gli elementi impliciti.\n",
    "\n",
    "Nello specifico, il codice esegue tre operazioni fondamentali:\n",
    "\n",
    "1. **Caricamento dei DataFrame:** Legge i file `.pkl` (Train, Dev e Test per il dominio Laptop) che abbiamo precedentemente aggiornato. Questi file ora contengono le annotazioni binarie che indicano la presenza di aspetti o opinioni implicite.\n",
    "2. **Definizione della Classe Custom `ACOSDataset`:** Questa √® la modifica principale rispetto a una pipeline standard di HuggingFace. Invece di restituire solo i classici 3 tensori della Token Classification (`input_ids`, `attention_mask` e `labels` con i tag BIO), questa classe sovrascritta restituisce **5 tensori** per ogni frase. Vengono infatti estratti e passati al modello anche `implicit_aspect_labels` e `implicit_opinion_labels`. Questi tensori serviranno ad addestrare in parallelo le due teste di classificazione binaria sul token `[CLS]`.\n",
    "3. **Configurazione dei DataLoader:** Crea gli iteratori di PyTorch che alimenteranno il modello durante l'addestramento e il test, processando blocchi (batch) di 16 frasi alla volta. I dati di training vengono rimescolati (`shuffle=True`) per stabilizzare l'apprendimento della rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7f7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dei dataset pre-processati...\n",
      "Dataset e DataLoaders creati con successo!\n",
      "Esempi nel set di Training RESTAURANT: 1530\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CARICAMENTO DEI DATASET SALVATI  ---\n",
    "cartella_dati = \"data_allineati\"\n",
    "\n",
    "print(\"Caricamento dei dataset pre-processati...\")\n",
    "# Carichiamo i Ristoranti\n",
    "df_train_align_rest = pd.read_pickle(os.path.join(cartella_dati, \"train_rest_aligned.pkl\"))\n",
    "df_dev_align_rest = pd.read_pickle(os.path.join(cartella_dati, \"dev_rest_aligned.pkl\"))\n",
    "df_test_align_rest = pd.read_pickle(os.path.join(cartella_dati, \"test_rest_aligned.pkl\"))\n",
    "\n",
    "\n",
    "class ACOSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.input_ids = df['input_ids'].tolist()\n",
    "        self.attention_mask = df['attention_mask'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "        # Estraiamo le colonne per gli impliciti!\n",
    "        self.implicit_aspect_label = df['implicit_aspect_label'].tolist()\n",
    "        self.implicit_opinion_label = df['implicit_opinion_label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            # Passiamo le etichette al Dataloader\n",
    "            'implicit_aspect_labels': torch.tensor(self.implicit_aspect_label[idx], dtype=torch.long),\n",
    "            'implicit_opinion_labels': torch.tensor(self.implicit_opinion_label[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- CREAZIONE DELLE ISTANZE ---\n",
    "\n",
    "# Creiamo i dataset per il dominio restaruant\n",
    "train_dataset_rest = ACOSDataset(df_train_align_rest)\n",
    "dev_dataset_rest = ACOSDataset(df_dev_align_rest)\n",
    "test_dataset_rest = ACOSDataset(df_test_align_rest)\n",
    "\n",
    "# --- CONFIGURAZIONE DATALOADERS ---\n",
    "\n",
    "BATCH_SIZE = 16 # Numero di frasi analizzate contemporaneamente\n",
    "\n",
    "train_loader_rest = DataLoader(train_dataset_rest, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader_rest = DataLoader(dev_dataset_rest, batch_size=BATCH_SIZE)\n",
    "test_loader_rest = DataLoader(test_dataset_rest, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Dataset e DataLoaders creati con successo!\")\n",
    "print(f\"Esempi nel set di Training RESTAURANT: {len(train_dataset_rest)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b70053",
   "metadata": {},
   "source": [
    "### Architettura Multi-Task: ModernBERT ACOS Extractor\n",
    "\n",
    "In questa cella abbandoniamo l'architettura standard di Token Classification per costruire un modello personalizzato in PyTorch, progettato per replicare fedelmente la logica di estrazione del paper ACOS originale.\n",
    "\n",
    "Il problema dei modelli tradizionali √® che falliscono quando un aspetto o un'opinione non sono scritti esplicitamente nel testo (elementi *Impliciti*). Per risolvere questa criticit√†, abbiamo progettato una rete **Multi-Task** composta da un \"cervello\" centrale (l'encoder ModernBERT) e **tre teste di classificazione indipendenti**:\n",
    "\n",
    "1. **Testa di Token Classification (Estrazione Esplicita):** Analizza ogni singola parola della frase per assegnare i tag BIO (B-ASP, I-ASP, B-OPI, I-OPI, O), estraendo gli span di testo espliciti.\n",
    "2. **Testa per Aspetti Impliciti (Classificazione Binaria):** Sfrutta il token speciale `[CLS]`, che racchiude il significato globale della frase, per prevedere matematicamente (S√¨/No) se la recensione contiene un aspetto sottinteso.\n",
    "3. **Testa per Opinioni Implicite (Classificazione Binaria):** Sfrutta sempre il token `[CLS]` per indovinare se c'√® un'opinione sottintesa.\n",
    "\n",
    "**La Loss Combinata (L'addestramento simultaneo)**\n",
    "Il vero \"motore\" di questa classe √® nella funzione `forward`. Durante l'addestramento, il modello calcola contemporaneamente tre errori separati (uno per l'estrazione e due per le previsioni binarie degli impliciti). Questi tre errori vengono **sommati in un'unica Loss globale**. In questo modo, la rete neurale viene forzata a imparare tutti i task simultaneamente, ottimizzando i pesi interni per comprendere a fondo sia ci√≤ che √® scritto, sia ci√≤ che √® sottinteso.\n",
    "\n",
    "Infine, il modello viene caricato sulla GPU e accoppiato a un ottimizzatore **AdamW a 8-bit** (`bitsandbytes`) per massimizzare l'efficienza e prevenire l'esaurimento della memoria VRAM durante le epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19228275b9b4149a83ece3f0a53594b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODELLO MULTI-TASK PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base (Custom ACOS Extractor)\n",
      "Task: Token Classification + 2x Binary Classification (Impliciti)\n",
      "Numero di Classi Token: 5\n",
      "Optimizer: AdamW 8-bit (lr=5e-5)\n",
      "Loss Function: Loss Combinata (calcolata internamente)\n",
      "\n",
      "==================================================\n",
      "MODELLO PRONTO PER IL TRAINING\n",
      "==================================================\n",
      "Architettura: ModernBERT-base\n",
      "Task: Token Classification (Estrazione Aspetti & Opinioni)\n",
      "Numero di Classi: 5\n",
      "Optimizer: AdamW 8-bit (lr=5e-5)\n",
      "Loss Function: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# Definiamo le 5 etichette: 0=O, 1=B-ASP, 2=I-ASP, 3=B-OPI, 4=I-OPI\n",
    "NUM_LABELS = 5 \n",
    "class ModernBertACOS_Extractor(nn.Module):\n",
    "    def __init__(self, model_name=\"answerdotai/ModernBERT-base\", num_labels=5):\n",
    "        super().__init__()\n",
    "        # Carichiamo la \"schiena\" del modello (l'encoder base)\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Testolina 1: Trova le parole scritte (Token Classification)\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "        # Testoline 2 e 3: Indovinano se ci sono impliciti (Classificazione Binaria)\n",
    "        self.implicit_aspect_classifier = nn.Linear(hidden_size, 2)\n",
    "        self.implicit_opinion_classifier = nn.Linear(hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, implicit_aspect_labels=None, implicit_opinion_labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        \n",
    "        # Prendiamo il token [CLS] (posizione 0) per i classificatori binari\n",
    "        cls_output = sequence_output[:, 0, :] \n",
    "        \n",
    "        # Le tre testoline fanno le loro previsioni\n",
    "        token_logits = self.token_classifier(sequence_output)\n",
    "        imp_asp_logits = self.implicit_aspect_classifier(cls_output)\n",
    "        imp_opi_logits = self.implicit_opinion_classifier(cls_output)\n",
    "        \n",
    "        loss = None\n",
    "        # Calcolo della \"Loss Combinata\" durante il training\n",
    "        if labels is not None and implicit_aspect_labels is not None and implicit_opinion_labels is not None:\n",
    "            # Estraiamo il device (CUDA o CPU) per creare i tensori dei pesi nel posto giusto\n",
    "            device = input_ids.device\n",
    "            \n",
    "            # --- NOVIT√Ä 1: Pesi per i Token (Lotta allo sbilanciamento delle 'O') ---\n",
    "            # Indice 0 ('O') ha peso 1.0. \n",
    "            # Gli indici 1, 2, 3, 4 (Aspetti e Opinioni) hanno peso 10.0! Sbagliarli costa carissimo.\n",
    "            token_weights = torch.tensor([1.0, 10.0, 10.0, 10.0, 10.0], device=device)\n",
    "            loss_fct_token = nn.CrossEntropyLoss(weight=token_weights)\n",
    "            \n",
    "            # --- NOVIT√Ä 2: Pesi per gli Impliciti ---\n",
    "            # Classe 0 (Esplicito) ha peso 1.0. Classe 1 (Implicito) ha peso 5.0.\n",
    "            # Diciamo al modello: \"Non ignorare gli impliciti solo perch√© sono rari!\"\n",
    "            implicit_weights = torch.tensor([1.0, 5.0], device=device)\n",
    "            loss_fct_implicit = nn.CrossEntropyLoss(weight=implicit_weights)\n",
    "            \n",
    "            # Loss 1: Token (ignorando il padding -100)\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = token_logits.view(-1, 5)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss_token = loss_fct_token(active_logits, active_labels)\n",
    "            \n",
    "            # Loss 2 & 3: Impliciti\n",
    "            loss_asp = loss_fct_implicit(imp_asp_logits, implicit_aspect_labels)\n",
    "            loss_opi = loss_fct_implicit(imp_opi_logits, implicit_opinion_labels)\n",
    "            \n",
    "            # --- NOVIT√Ä 3: Moltiplicatori della Loss Multi-Task ---\n",
    "            # Visto che il Recall delle opinioni implicite era solo il 45%, \n",
    "            # diamo peso doppio alla sua Loss nella somma totale!\n",
    "            loss = loss_token + (1.5 * loss_asp) + (2.0 * loss_opi)\n",
    "            \n",
    "        return {\n",
    "            \"loss\": loss, \n",
    "            \"token_logits\": token_logits, \n",
    "            \"imp_asp_logits\": imp_asp_logits, \n",
    "            \"imp_opi_logits\": imp_opi_logits\n",
    "        }\n",
    "\n",
    "print(\"Scaricamento e configurazione della nuova architettura Multi-Task ModernBERT...\")\n",
    "\n",
    "# Inizializziamo il nostro modello custom invece di AutoModelForTokenClassification\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=NUM_LABELS)\n",
    "\n",
    "# Spostiamo il modello sul dispositivo di calcolo (GPU/MPS/CPU)\n",
    "model_step1.to(device)\n",
    "\n",
    "# --- 2. CONFIGURAZIONE DELL'OTTIMIZZATORE E DELLA LOSS ---\n",
    "\n",
    "# Manteniamo la tua ottima scelta di usare l'optimizer a 8-bit per non saturare la memoria!\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODELLO MULTI-TASK PRONTO PER IL TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Architettura: ModernBERT-base (Custom ACOS Extractor)\")\n",
    "print(f\"Task: Token Classification + 2x Binary Classification (Impliciti)\")\n",
    "print(f\"Numero di Classi Token: {NUM_LABELS}\")\n",
    "print(f\"Optimizer: AdamW 8-bit (lr=2e-5)\")\n",
    "print(f\"Loss Function: Loss Combinata (calcolata internamente)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e772824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attivazione Ottimizzazioni di Memoria...\n",
      "\n",
      "Training Multi-Task su RESTAURANT: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti: ogni 4 step | FP16: Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1:   3%|‚ñé         | 3/96 [00:00<00:24,  3.73it/s, loss=3.24]/home/al3th3ia/Scrivania/Cristina/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.82it/s, loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2520 | Valid Loss: 1.7552\n",
      "Miglior modello trovato (Loss: 1.7552)\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  5.03it/s, loss=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4441 | Valid Loss: 1.2890\n",
      "Miglior modello trovato (Loss: 1.2890)\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  5.02it/s, loss=1.03] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9493 | Valid Loss: 1.0853\n",
      "Miglior modello trovato (Loss: 1.0853)\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.99it/s, loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5663 | Valid Loss: 1.1235\n",
      "Nessun miglioramento. Patience: 1/2\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:19<00:00,  4.99it/s, loss=0.232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3204 | Valid Loss: 1.1719\n",
      "Nessun miglioramento. Patience: 2/2\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 5.\n",
      "\n",
      "Fine Addestramento Multi-Task.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>valid_loss_epoch</td><td>‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.23184</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss_epoch</td><td>0.32037</td></tr><tr><td>valid_loss_epoch</td><td>1.17191</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/2o15ivh3' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/2o15ivh3</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260222_171532-2o15ivh3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA E WANDB ---\n",
    "print(\"Attivazione Ottimizzazioni di Memoria...\")\n",
    "\n",
    "# Gradient Checkpointing: si applica all'encoder interno (BERT) della nostra classe custom\n",
    "model_step1.bert.gradient_checkpointing_enable()\n",
    "\n",
    "accumulation_steps = wandb.config.get('accumulation_steps', 4) \n",
    "patience = wandb.config.get('patience', 5)\n",
    "epochs = wandb.config.get('epochs', 40)\n",
    "lr = wandb.config.get('learning_rate', 2e-5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Optimizer a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(model_step1.parameters(), lr=lr)\n",
    "\n",
    "# Scaler per Mixed Precision (FP16)\n",
    "scaler = GradScaler() \n",
    "\n",
    "total_steps = (len(train_loader_rest) // accumulation_steps) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (MULTI-TASK) ---\n",
    "\n",
    "def evaluate_model_multitask(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "            imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "            \n",
    "            # Usiamo autocast in valutazione per risparmiare memoria\n",
    "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    labels=labels,\n",
    "                    implicit_aspect_labels=imp_asp_labels,\n",
    "                    implicit_opinion_labels=imp_opi_labels\n",
    "                )\n",
    "            \n",
    "            total_loss += outputs['loss'].item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_multitask(model, data_loader, optimizer, scheduler, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() # Reset iniziale\n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "        \n",
    "        # A. Mixed Precision Forward Pass (FP16)\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                labels=labels,\n",
    "                implicit_aspect_labels=imp_asp_labels,\n",
    "                implicit_opinion_labels=imp_opi_labels\n",
    "            )\n",
    "            # Normalizziamo la loss per l'accumulo dei gradienti\n",
    "            loss = outputs['loss'] / accumulation_steps \n",
    "        \n",
    "        # B. Backward Pass con Scaler (Evita underflow/overflow dell'FP16)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # C. Update Pesi ogni 'accumulation_steps'\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        wandb.log({\"batch_loss\": loss.item() * accumulation_steps})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"\\nTraining Multi-Task su RESTAURANT: {epochs} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti: ogni {accumulation_steps} step | FP16: Attivato\")\n",
    "\n",
    "best_valid_loss_rest = float('inf')\n",
    "output_dir = \"./best_multitask_extractor_restaurant\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{epochs} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_rest = train_epoch_multitask(model_step1, train_loader_rest, optimizer, scheduler, device, epoch, scaler, accumulation_steps)\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_rest = evaluate_model_multitask(model_step1, dev_loader_rest, device)\n",
    "    \n",
    "    # Pulizia spietata della cache della GPU a fine epoca!\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_rest:.4f} | Valid Loss: {valid_loss_rest:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_rest,\n",
    "        \"valid_loss_epoch\": valid_loss_rest\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_rest < best_valid_loss_rest:\n",
    "        best_valid_loss_rest = valid_loss_rest\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_rest:.4f})\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # Salvataggio Custom per l'architettura Multi-Task\n",
    "        torch.save(model_step1.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Multi-Task.\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6584a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello Multi-Task migliore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d39f33aa5f54bcab3e294c8b243133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inizio Test Multi-Task sul Dataset Restaurant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test RESTAURANT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:04<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\n",
      "============================================================\n",
      "Overall Precision: 0.5287\n",
      "Overall Recall:    0.6537\n",
      "Overall F1-Score:  0.5845\n",
      "\n",
      "Dettaglio per Classe (Quello che conta per il paper):\n",
      "--------------------------------------------------\n",
      "   ASP:\n",
      "   Precision: 0.4705\n",
      "   Recall:    0.6036\n",
      "   F1-Score:  0.5288\n",
      "   Support:   608\n",
      "   OPI:\n",
      "   Precision: 0.5873\n",
      "   Recall:    0.7006\n",
      "   F1-Score:  0.6390\n",
      "   Support:   648\n",
      "\n",
      "============================================================\n",
      "RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\n",
      "============================================================\n",
      "üîπ Accuratezza Aspetti Impliciti: 0.8285\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.84      0.92      0.88       386\n",
      "Implicito (1)       0.81      0.65      0.72       197\n",
      "\n",
      "     accuracy                           0.83       583\n",
      "    macro avg       0.82      0.78      0.80       583\n",
      " weighted avg       0.83      0.83      0.82       583\n",
      "\n",
      "--------------------------------------------------\n",
      "üîπ Accuratezza Opinioni Implicite: 0.8045\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Esplicito (0)       0.81      0.95      0.87       419\n",
      "Implicito (1)       0.76      0.45      0.56       164\n",
      "\n",
      "     accuracy                           0.80       583\n",
      "    macro avg       0.79      0.70      0.72       583\n",
      " weighted avg       0.80      0.80      0.79       583\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- A. CARICAMENTO DEL \"CAMPIONE\" MULTI-TASK ---\n",
    "print(\"Caricamento del modello Multi-Task migliore...\")\n",
    "\n",
    "# 1. Inizializziamo la nostra architettura custom\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5)\n",
    "\n",
    "# 2. Carichiamo i pesi salvati del miglior modello (solo i pesi, non l'intero oggetto)\n",
    "model_path = \"./best_multitask_extractor_restaurant/pytorch_model.bin\"\n",
    "model_step1.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "\n",
    "model_step1.to(device)\n",
    "model_step1.eval() # Modalit√† esame (spegne dropout)\n",
    "\n",
    "# --- B. PREPARAZIONE METRICHE ---\n",
    "# Carichiamo la metrica seqeval (standard per NER/ABSA)\n",
    "metric = load(\"seqeval\")\n",
    "\n",
    "# Mappa per decodificare i numeri in etichette\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"\\nInizio Test Multi-Task sul Dataset Restaurant...\")\n",
    "\n",
    "# --- C. CICLO DI PREVISIONE ---\n",
    "predictions_tokens = []\n",
    "true_labels_tokens = []\n",
    "\n",
    "# Liste per salvare le predizioni binarie (Impliciti)\n",
    "true_imp_asp, pred_imp_asp = [], []\n",
    "true_imp_opi, pred_imp_opi = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_rest, desc=\"Test RESTAURANT\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        imp_asp_labels = batch['implicit_aspect_labels'].to(device)\n",
    "        imp_opi_labels = batch['implicit_opinion_labels'].to(device)\n",
    "\n",
    "        # 1. Il modello fa le sue 3 predizioni contemporaneamente\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            outputs = model_step1(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 2. Estraiamo i risultati dalle 3 testoline\n",
    "        token_preds = torch.argmax(outputs['token_logits'], dim=-1)\n",
    "        asp_preds = torch.argmax(outputs['imp_asp_logits'], dim=-1)\n",
    "        opi_preds = torch.argmax(outputs['imp_opi_logits'], dim=-1)\n",
    "        \n",
    "        # Salviamo i risultati binari\n",
    "        true_imp_asp.extend(imp_asp_labels.cpu().tolist())\n",
    "        pred_imp_asp.extend(asp_preds.cpu().tolist())\n",
    "        \n",
    "        true_imp_opi.extend(imp_opi_labels.cpu().tolist())\n",
    "        pred_imp_opi.extend(opi_preds.cpu().tolist())\n",
    "\n",
    "        # 3. Convertiamo i numeri in etichette BIO (pulendo il padding)\n",
    "        for i in range(len(labels)):\n",
    "            true_label_row = []\n",
    "            pred_label_row = []\n",
    "            \n",
    "            for j in range(len(labels[i])):\n",
    "                # Ignoriamo i token di padding (dove attention_mask √® 0 o la label √® -100)\n",
    "                if labels[i][j] != -100 and attention_mask[i][j] == 1: \n",
    "                    true_label_row.append(id2label[labels[i][j].item()])\n",
    "                    pred_label_row.append(id2label[token_preds[i][j].item()])\n",
    "            \n",
    "            true_labels_tokens.append(true_label_row)\n",
    "            predictions_tokens.append(pred_label_row)\n",
    "\n",
    "# --- D. CALCOLO E STAMPA RISULTATI ---\n",
    "results_seq = metric.compute(predictions=predictions_tokens, references=true_labels_tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: TOKEN CLASSIFICATION (Parole Esplicite)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Overall Precision: {results_seq['overall_precision']:.4f}\")\n",
    "print(f\"Overall Recall:    {results_seq['overall_recall']:.4f}\")\n",
    "print(f\"Overall F1-Score:  {results_seq['overall_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nDettaglio per Classe (Quello che conta per il paper):\")\n",
    "print(\"-\" * 50)\n",
    "for key in results_seq.keys():\n",
    "    if key in ['ASP', 'OPI']: \n",
    "        print(f\"   {key}:\")\n",
    "        print(f\"   Precision: {results_seq[key]['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {results_seq[key]['recall']:.4f}\")\n",
    "        print(f\"   F1-Score:  {results_seq[key]['f1']:.4f}\")\n",
    "        print(f\"   Support:   {results_seq[key]['number']}\") \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISULTATI FINALI: IDENTIFICAZIONE IMPLICITI (NULL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metriche per Aspetti Impliciti\n",
    "acc_asp = accuracy_score(true_imp_asp, pred_imp_asp)\n",
    "print(f\"Accuratezza Aspetti Impliciti: {acc_asp:.4f}\")\n",
    "print(classification_report(true_imp_asp, pred_imp_asp, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Metriche per Opinioni Implicite\n",
    "acc_opi = accuracy_score(true_imp_opi, pred_imp_opi)\n",
    "print(f\"Accuratezza Opinioni Implicite: {acc_opi:.4f}\")\n",
    "print(classification_report(true_imp_opi, pred_imp_opi, target_names=[\"Esplicito (0)\", \"Implicito (1)\"], zero_division=0))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e4cbc",
   "metadata": {},
   "source": [
    "## Classificatore Category-Sentiment (Extract-Classify-ACOS)\n",
    "\n",
    "Implementiamo il **secondo stadio** dell'architettura proposta nel paper. Dopo aver estratto gli Aspetti e le Opinioni nello Step 1, ora dobbiamo capire a quale Categoria appartengono e qual √® il loro Sentiment.\n",
    "\n",
    "Il codice di preparazione √® diviso in tre componenti fondamentali:\n",
    "\n",
    " 1. Il Dataset PyTorch (`ACOSPairDataset`)\n",
    "\n",
    " 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "\n",
    " 3. Inizializzazione e DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b46c0",
   "metadata": {},
   "source": [
    "### 1. Il Dataset PyTorch (`ACOSPairDataset`)\n",
    "Questa classe si occupa di \"impacchettare\" i dati per la rete neurale. \n",
    "Legge le coppie salvate nei file `.pkl`, passa il testo nel Tokenizer di ModernBERT e trasforma tutto in **Tensori PyTorch**. Nota chiave: legge direttamente gli indici degli span (che ho gi√† corretto con l'offset `+1` per far spazio al token `[CLS]`) e carica l'array di 121 etichette (le \"soluzioni\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a13b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACOSPairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['review_text']\n",
    "        \n",
    "        # Gli span sono gi√† corretti con il +1 per il [CLS]!\n",
    "        a_span = row['aspect_span']\n",
    "        o_span = row['opinion_span']\n",
    "\n",
    "        # Tokenizzazione (ModernBERT usa il token [CLS] in automatico)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'aspect_span': torch.tensor(a_span),\n",
    "            'opinion_span': torch.tensor(o_span),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe6e74",
   "metadata": {},
   "source": [
    "### 2. L'Architettura Custom (`ModernBertACOSClassifier`)\n",
    "Questa √® la vera \"magia\" matematica del paper, tradotta in codice:\n",
    "* **Il Corpo (Backbone):** Invece di partire da zero, carichiamo il *corpo* del modello che hai gi√† addestrato nello Step 1 (`best_model_laptop`). In questo modo, la rete conosce gi√† il dominio tecnico dei computer!\n",
    "* **Span Pooling:** Il modello estrae i vettori (hidden states) corrispondenti alle parole dell'Aspetto e dell'Opinione e ne calcola la media. Se un elemento √® implicito (`-1`), pesca automaticamente il vettore globale del token `[CLS]`.\n",
    "* **Feature Fusion:** Concatena il vettore dell'aspetto ($u_a$) e dell'opinione ($u_o$) in un unico grande vettore di dimensione 1536.\n",
    "* **Le 121 Teste (Multiple Multi-class):** Passa questo vettore in 121 classificatori lineari paralleli. Ognuno di essi decider√† se per la *sua* categoria la coppia √® `Positive (0)`, `Negative (1)`, `Neutral (2)` o `Invalid (3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8a0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBertACOSClassifier(nn.Module):\n",
    "    def __init__(self, path_to_best_model, num_categories):\n",
    "        super(ModernBertACOSClassifier, self).__init__()\n",
    "        \n",
    "        # Carichiamo SOLO IL CORPO dal tuo modello dello Step 1\n",
    "        self.modernbert = AutoModel.from_pretrained(path_to_best_model)\n",
    "        hidden_size = self.modernbert.config.hidden_size # 768\n",
    "        \n",
    "        # Le 121 teste (ognuna prende il vettore concatenato 1536 e sputa 4 classi)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size * 2, 4) for _ in range(num_categories)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, aspect_spans, opinion_spans):\n",
    "        outputs = self.modernbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state # [Batch, Seq_Len, 768]\n",
    "        \n",
    "        batch_size = last_hidden_state.size(0)\n",
    "        u_a_list, u_o_list = [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Pooling Aspetto\n",
    "            a_start, a_end = aspect_spans[i]\n",
    "            if a_start == -1: \n",
    "                u_a = last_hidden_state[i, 0, :] # Token [CLS]\n",
    "            else:\n",
    "                u_a = last_hidden_state[i, a_start:a_end, :].mean(dim=0)\n",
    "            \n",
    "            # Pooling Opinione\n",
    "            o_start, o_end = opinion_spans[i]\n",
    "            if o_start == -1: \n",
    "                u_o = last_hidden_state[i, 0, :] # Token [CLS]\n",
    "            else:\n",
    "                u_o = last_hidden_state[i, o_start:o_end, :].mean(dim=0)\n",
    "            \n",
    "            u_a_list.append(u_a)\n",
    "            u_o_list.append(u_o)\n",
    "\n",
    "        # Concatenazione: [u_a ; u_o]\n",
    "        combined_features = torch.cat((torch.stack(u_a_list), torch.stack(u_o_list)), dim=-1)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "\n",
    "        # Passiamo il vettore nelle teste lineari\n",
    "        logits = [head(combined_features) for head in self.heads]\n",
    "        \n",
    "        # Output: [Batch, 121, 4]\n",
    "        return torch.stack(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebed1d",
   "metadata": {},
   "source": [
    "### 3. Inizializzazione e DataLoaders\n",
    "L'ultimo blocco carica fisicamente i file salvati dalla nostra \"Fabbrica dei Dati\", istanzia i `Dataset`, e crea i `DataLoader` (con batch size = 16) per \"nutrire\" la GPU in modo efficiente durante l'addestramento. Infine, sposta il modello sulla scheda video (CUDA) pronto per il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2509e976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee50c204c9042a2b30f8836dcd48be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_restaurant\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "classifier.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello inizializzato! Categorie: 13 | Device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. Carichiamo la lista delle categorie salvata prima\n",
    "with open(\"data_coppie/restaurant_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list) # 121\n",
    "\n",
    "# 2. Carichiamo i DataFrame di Train e Dev\n",
    "df_train = pd.read_pickle(\"data_coppie/train_restaurant_pairs.pkl\")\n",
    "df_dev = pd.read_pickle(\"data_coppie/dev_restaurant_pairs.pkl\")\n",
    "df_test = pd.read_pickle(\"data_coppie/test_restaurant_pairs.pkl\")\n",
    "\n",
    "# 3. Inizializziamo il Tokenizer e i Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "train_dataset = ACOSPairDataset(df_train, tokenizer)\n",
    "dev_dataset = ACOSPairDataset(df_dev, tokenizer)\n",
    "test_dataset = ACOSPairDataset(df_test, tokenizer) \n",
    "\n",
    "# 4. Creiamo i DataLoader (Batch size 16 √® un buon compromesso tra velocit√† e VRAM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dev_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16) \n",
    "# 5. Inizializziamo il Modello usando i pesi dello Step 1!\n",
    "model = ModernBertACOSClassifier(\"./best_model_restaurant\", num_categories)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Modello inizializzato! Categorie: {num_categories} | Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834f631",
   "metadata": {},
   "source": [
    "## WANDB per lo step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274d34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcristinatomaciello2001\u001b[0m (\u001b[33mcristinatextmining\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260222_190704-ls9tnias</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/ls9tnias' target=\"_blank\">Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/ls9tnias' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/ls9tnias</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W&B inizializzato per il progetto: BigData-TextMining-ACOS\n",
      "Nome della Run attuale: Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS\n"
     ]
    }
   ],
   "source": [
    "# Chiudiamo per sicurezza qualsiasi run precedente rimasta aperta nello stesso notebook\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "# 1. Definizione degli Hyperparameters per lo STEP 2\n",
    "config_step2 = {\n",
    "    \"learning_rate\": 2e-5, # Solitamente per lo Step 2 un LR leggermente pi√π basso √® meglio (es. 2e-5)\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 4, # Aggiunto per il tuo training loop ottimizzato!\n",
    "    \"model_name\": \"answerdotai/ModernBERT-base\",\n",
    "    \"dataset\": \"Restaurant-ACOS\", \n",
    "    \"seed\": 42,\n",
    "    \"patience\": 5  # Per Early Stopping\n",
    "}\n",
    "\n",
    "# 2. Inizializzazione del Run per lo Step 2\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    config=config_step2,\n",
    "    # Aggiungiamo \"Step2_Class\" al nome per distinguerlo dallo Step 1\n",
    "    name=f\"Step2_Class_{config_step2['model_name']}_{config_step2['dataset']}\" \n",
    ")\n",
    "\n",
    "print(f\" W&B inizializzato per il progetto: {wandb.run.project}\")\n",
    "print(f\"Nome della Run attuale: {wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f483c",
   "metadata": {},
   "source": [
    "## Train su Sentiment e Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ca2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training STEP 2 su RESTAURANT: 40 epoche | Device: cuda\n",
      "Accumulo Gradienti ogni 4 step | FP16 Attivato\n",
      "\n",
      "--- Epoca 1/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:49<00:00,  4.73it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8031 | Valid Loss: 0.6246\n",
      "Miglior modello trovato (Loss: 0.6246)! Salvataggio...\n",
      "\n",
      "--- Epoca 2/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:49<00:00,  4.71it/s, loss=1.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4954 | Valid Loss: 0.4818\n",
      "Miglior modello trovato (Loss: 0.4818)! Salvataggio...\n",
      "\n",
      "--- Epoca 3/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:49<00:00,  4.68it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3445 | Valid Loss: 0.4077\n",
      "Miglior modello trovato (Loss: 0.4077)! Salvataggio...\n",
      "\n",
      "--- Epoca 4/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:48<00:00,  4.80it/s, loss=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2425 | Valid Loss: 0.3993\n",
      "Miglior modello trovato (Loss: 0.3993)! Salvataggio...\n",
      "\n",
      "--- Epoca 5/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:48<00:00,  4.77it/s, loss=0.063] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1716 | Valid Loss: 0.3948\n",
      "Miglior modello trovato (Loss: 0.3948)! Salvataggio...\n",
      "\n",
      "--- Epoca 6/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:48<00:00,  4.79it/s, loss=0.0711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1318 | Valid Loss: 0.3924\n",
      "Miglior modello trovato (Loss: 0.3924)! Salvataggio...\n",
      "\n",
      "--- Epoca 7/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:48<00:00,  4.75it/s, loss=0.0321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0986 | Valid Loss: 0.4024\n",
      "Nessun miglioramento. Patience: 1/5\n",
      "\n",
      "--- Epoca 8/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:50<00:00,  4.63it/s, loss=0.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0826 | Valid Loss: 0.4263\n",
      "Nessun miglioramento. Patience: 2/5\n",
      "\n",
      "--- Epoca 9/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:51<00:00,  4.49it/s, loss=0.125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0688 | Valid Loss: 0.4441\n",
      "Nessun miglioramento. Patience: 3/5\n",
      "\n",
      "--- Epoca 10/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:50<00:00,  4.62it/s, loss=0.018] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0597 | Valid Loss: 0.4369\n",
      "Nessun miglioramento. Patience: 4/5\n",
      "\n",
      "--- Epoca 11/40 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232/232 [00:49<00:00,  4.65it/s, loss=0.0986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0550 | Valid Loss: 0.4442\n",
      "Nessun miglioramento. Patience: 5/5\n",
      "\n",
      "EARLY STOPPING ATTIVATO! Interruzione all'epoca 11.\n",
      "\n",
      "Fine Addestramento Step 2 (Restaurant).\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>train_loss_epoch</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>valid_loss_epoch</td><td>‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.09864</td></tr><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss_epoch</td><td>0.05499</td></tr><tr><td>valid_loss_epoch</td><td>0.44417</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Step2_Class_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/ls9tnias' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/ls9tnias</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260222_190704-ls9tnias/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. CONFIGURAZIONE AVANZATA MEMORIA ---\n",
    "# Gradient Checkpointing: abilitato SOLO sul corpo di ModernBERT\n",
    "model.modernbert.gradient_checkpointing_enable()\n",
    "\n",
    "# Parametri per simulare un batch size maggiore\n",
    "accumulation_steps = config_step2.get('accumulation_steps', 4) \n",
    "patience = config_step2.get('patience', 5)\n",
    "patience_counter = 0\n",
    "\n",
    "# Ottimizzatore AdamW a 8-bit\n",
    "optimizer = bnb.optim.AdamW8bit(\n",
    "    model.parameters(), \n",
    "    lr=config_step2['learning_rate']\n",
    ")\n",
    "\n",
    "# Scaler per Mixed Precision (fondamentale per evitare l'OOM)\n",
    "scaler = GradScaler() \n",
    "\n",
    "# Un bilanciamento pi√π \"umano\": i sentimenti contano 10 volte pi√π della classe Invalido.\n",
    "# Questo costringe il modello a essere molto pi√π cauto.\n",
    "weights = torch.tensor([10.0, 10.0, 10.0, 1.0]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Calcolo degli step totali per lo scheduler\n",
    "total_steps = (len(train_loader) // accumulation_steps) * config_step2['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO OTTIMIZZATE (STEP 2) ---\n",
    "\n",
    "def evaluate_model_step2(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            aspect_span = batch['aspect_span'].to(device)\n",
    "            opinion_span = batch['opinion_span'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast(device_type='cuda'):\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                               aspect_spans=aspect_span, opinion_spans=opinion_span)\n",
    "                \n",
    "                loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_epoch_step2(model, data_loader, optimizer, scheduler, criterion, device, epoch_idx, scaler, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        aspect_span = batch['aspect_span'].to(device)\n",
    "        opinion_span = batch['opinion_span'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                           aspect_spans=aspect_span, opinion_spans=opinion_span)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, 4), labels.view(-1))\n",
    "            loss = loss / accumulation_steps \n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        real_loss = loss.item() * accumulation_steps\n",
    "        total_loss += real_loss\n",
    "        \n",
    "        wandb.log({\"batch_loss\": real_loss})\n",
    "        loop.set_description(f\"Epoca {epoch_idx + 1}\")\n",
    "        loop.set_postfix(loss=real_loss)\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- 3. CICLO DI ADDESTRAMENTO ---\n",
    "\n",
    "print(f\"Training STEP 2 su RESTAURANT: {config_step2['epochs']} epoche | Device: {device}\")\n",
    "print(f\"Accumulo Gradienti ogni {accumulation_steps} step | FP16 Attivato\")\n",
    "\n",
    "best_valid_loss_restaurant = float('inf')\n",
    "\n",
    "for epoch in range(config_step2['epochs']):\n",
    "    print(f\"\\n--- Epoca {epoch+1}/{config_step2['epochs']} ---\")\n",
    "    \n",
    "    # 1. Training\n",
    "    train_loss_restaurant = train_epoch_step2(\n",
    "        model, train_loader, optimizer, scheduler, criterion, \n",
    "        device, epoch, scaler, accumulation_steps\n",
    "    )\n",
    "    \n",
    "    # 2. Validazione\n",
    "    valid_loss_restaurant = evaluate_model_step2(model, val_loader, criterion, device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss_restaurant:.4f} | Valid Loss: {valid_loss_restaurant:.4f}\")\n",
    "    \n",
    "    # 3. Log metriche epoca su W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_epoch\": train_loss_restaurant,\n",
    "        \"valid_loss_epoch\": valid_loss_restaurant\n",
    "    })\n",
    "    \n",
    "    # --- LOGICA EARLY STOPPING & CHECKPOINT ---\n",
    "    \n",
    "    if valid_loss_restaurant < best_valid_loss_restaurant:\n",
    "        best_valid_loss_restaurant = valid_loss_restaurant\n",
    "        patience_counter = 0  \n",
    "        \n",
    "        print(f\"Miglior modello trovato (Loss: {best_valid_loss_restaurant:.4f})! Salvataggio...\")\n",
    "        \n",
    "        # Salvataggio custom model nella cartella dei RESTAURANT\n",
    "        save_dir = \"./best_classifier_restaurant\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1  \n",
    "        print(f\"Nessun miglioramento. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEARLY STOPPING ATTIVATO! Interruzione all'epoca {epoch+1}.\")\n",
    "            break \n",
    "\n",
    "print(\"\\nFine Addestramento Step 2 (Restaurant).\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac5bb2",
   "metadata": {},
   "source": [
    "## Test su Sentiment e Categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd7ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/al3th3ia/Scrivania/Cristina/progetto-text-mining/wandb/run-20260222_191635-f0c6koho</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/f0c6koho' target=\"_blank\">TEST_Step2_answerdotai/ModernBERT-base_Restaurant-ACOS</a></strong> to <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/f0c6koho' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/f0c6koho</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a30cbbf96314384b16a9b9b0efa9c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_restaurant\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "classifier.weight | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CHECK CARICAMENTO PESI ---\n",
      "Chiavi Inaspettate (OK se sono dello Step 1): 0\n",
      "Chiavi Mancanti (PROBLEMA se sono 'heads'): 0\n",
      "SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\n",
      "‚öôÔ∏è Estrazione di tutte le probabilit√† dal modello in corso (attendere)...\n",
      "\n",
      "üîç Avvio Grid Search per la migliore Soglia di Confidenza (Macro F1)...\n",
      "IL MILGIOR THRESHOLD E'= 0.53 (53%)\n",
      "\n",
      "--- MIGLIOR CLASSIFICATION REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Positive (0)       0.46      0.57      0.51       205\n",
      "Negative (1)       1.00      0.02      0.04        44\n",
      " Neutral (2)       0.59      0.69      0.64       667\n",
      "\n",
      "   micro avg       0.56      0.63      0.59       916\n",
      "   macro avg       0.68      0.43      0.40       916\n",
      "weighted avg       0.58      0.63      0.58       916\n",
      "\n",
      "BEST MICRO F1-Score: 0.5915\n",
      "CORRISPONDENTE MACRO F1: 0.3958\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>‚ñÅ</td></tr><tr><td>best_test_micro_f1</td><td>‚ñÅ</td></tr><tr><td>optimal_threshold</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_macro_f1</td><td>0.39578</td></tr><tr><td>best_test_micro_f1</td><td>0.59149</td></tr><tr><td>optimal_threshold</td><td>0.53</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TEST_Step2_answerdotai/ModernBERT-base_Restaurant-ACOS</strong> at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/f0c6koho' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS/runs/f0c6koho</a><br> View project at: <a href='https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS' target=\"_blank\">https://wandb.ai/cristinatextmining/BigData-TextMining-ACOS</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260222_191635-f0c6koho/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chiudiamo run appese\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "WANDB_ENTITY = \"cristinatextmining\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"BigData-TextMining-ACOS\",\n",
    "    entity=WANDB_ENTITY,\n",
    "    name=f\"TEST_Step2_{config_step2['model_name']}_{config_step2['dataset']}\",\n",
    "    job_type=\"test\"\n",
    ")\n",
    "\n",
    "# Ricreiamo l'architettura del modello\n",
    "num_categories = len(category_list)\n",
    "model_test = ModernBertACOSClassifier(\"./best_model_restaurant\", num_categories)\n",
    "\n",
    "# Carichiamo i pesi dello Step 2\n",
    "model_path = \"./best_classifier_restaurant/pytorch_model.bin\"\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# USIAMO STRICT=FALSE\n",
    "missing_keys, unexpected_keys = model_test.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"\\n--- CHECK CARICAMENTO PESI ---\")\n",
    "print(f\"Chiavi Inaspettate (OK se sono dello Step 1): {len(unexpected_keys)}\")\n",
    "print(f\"Chiavi Mancanti (PROBLEMA se sono 'heads'): {len(missing_keys)}\")\n",
    "\n",
    "heads_missing = [k for k in missing_keys if \"heads\" in k]\n",
    "if heads_missing:\n",
    "    print(f\"ERRORE CRITICO: Le teste di classificazione non sono state caricate! {heads_missing[:5]}\")\n",
    "else:\n",
    "    print(\"SUCCESS: Le teste di classificazione (Sentiment) sono state caricate correttamente.\")\n",
    "    \n",
    "model_test.to(device)\n",
    "model_test.eval()\n",
    "\n",
    "# --- 1. ESTRAZIONE PROBABILITA' ---\n",
    "print(\"‚öôÔ∏è Estrazione di tutte le probabilit√† dal modello in corso (attendere)...\")\n",
    "\n",
    "all_probs_list = []\n",
    "all_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader: \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        aspect_span = batch['aspect_span'].to(device)\n",
    "        opinion_span = batch['opinion_span'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        # Gestione sicura dell'autocast basata sull'hardware\n",
    "        if device.type == 'cuda':\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                logits = model_test(input_ids, attention_mask, aspect_span, opinion_span)\n",
    "        else:\n",
    "            # Per Mac MPS o CPU facciamo il forward pass standard\n",
    "            logits = model_test(input_ids, attention_mask, aspect_span, opinion_span)\n",
    "            \n",
    "        probs = torch.softmax(logits, dim=-1).cpu() \n",
    "        \n",
    "        all_probs_list.append(probs)\n",
    "        all_true_list.append(labels.cpu())\n",
    "\n",
    "all_probs = torch.cat(all_probs_list, dim=0) \n",
    "all_true = torch.cat(all_true_list, dim=0).numpy().flatten()\n",
    "\n",
    "\n",
    "# --- 2. GRID SEARCH SUL THRESHOLD (OTTIMIZZATA PER MACRO F1) ---\n",
    "print(\"\\nüîç Avvio Grid Search per la migliore Soglia di Confidenza (Macro F1)...\")\n",
    "\n",
    "thresholds_to_test = np.arange(0.50, 1.00, 0.01) \n",
    "best_micro_f1 = 0.0\n",
    "best_macro_f1 = 0.0\n",
    "best_threshold = 0.0\n",
    "best_report = \"\"\n",
    "\n",
    "target_names = ['Positive (0)', 'Negative (1)', 'Neutral (2)']\n",
    "labels_to_eval = [0, 1, 2]\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    valid_class_probs, valid_class_preds = torch.max(all_probs[:, :, :3], dim=-1)\n",
    "    final_preds = torch.full_like(valid_class_preds, 3)\n",
    "    \n",
    "    mask = valid_class_probs > thresh\n",
    "    final_preds[mask] = valid_class_preds[mask]\n",
    "    \n",
    "    preds_flat = final_preds.numpy().flatten()\n",
    "    \n",
    "    current_micro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='micro')\n",
    "    current_macro_f1 = f1_score(all_true, preds_flat, labels=labels_to_eval, average='macro')\n",
    "    \n",
    "    if current_macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = current_macro_f1\n",
    "        best_micro_f1 = current_micro_f1 \n",
    "        best_threshold = thresh\n",
    "        best_report = classification_report(\n",
    "            all_true, preds_flat, labels=labels_to_eval, \n",
    "            target_names=target_names, zero_division=0\n",
    "        )\n",
    "\n",
    "\n",
    "# --- 3. STAMPA DEI RISULTATI VINCITORI E LOG W&B ---\n",
    "print(f\"IL MILGIOR THRESHOLD E'= {best_threshold:.2f} ({(best_threshold*100):.0f}%)\")\n",
    "\n",
    "print(\"\\n--- MIGLIOR CLASSIFICATION REPORT ---\")\n",
    "print(best_report)\n",
    "print(f\"BEST MICRO F1-Score: {best_micro_f1:.4f}\")\n",
    "print(f\"CORRISPONDENTE MACRO F1: {best_macro_f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "wandb.log({\n",
    "    \"best_test_micro_f1\": best_micro_f1,\n",
    "    \"best_test_macro_f1\": best_macro_f1,\n",
    "    \"optimal_threshold\": best_threshold\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0e040",
   "metadata": {},
   "source": [
    "### Test sulla quadruplice completa (Aspetto-Opinione-Categoria-Sentiment)\n",
    "In questa cella, eseguiamo il test finale del nostro modello su tutte e quattro le dimensioni del problema: Aspetto, Opinione, Categoria e Sentiment.\n",
    "Il processo √® il seguente:\n",
    "1. **Iterazione sui Dati di Test:** Per ogni batch di coppie (Aspetto-Opinione) estratte dallo Step 1, il modello predice la Categoria e il Sentiment.\n",
    "2. **Raccolta delle Predizioni:** Le predizioni vengono raccole in liste separate per le categorie e i sentimenti, insieme alle etichette vere.\n",
    "3. **Calcolo delle Metriche:** Utilizziamo `accuracy_score` e `classification_report` di `sklearn` per valutare le prestazioni del modello sia sulla Categoria che sul Sentiment, considerando solo le coppie che sono state correttamente estratte nello Step 1 (ovvero quelle con etichetta diversa da \"Invalid (3)\").\n",
    "4. **Stampa dei Risultati:** I risultati vengono stampati in modo chiaro, mostrando l'accuratezza e il report dettagliato per entrambe le dimensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa9ffa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento configurazioni...\n",
      "Caricamento Step 1 (L'Investigatore Multi-Task)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a12189be4994a9b9209ceefc2393aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento Step 2 (Lo Psicologo Classificatore)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009b83297463475ab3bb1981bd8c308c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mModernBertModel LOAD REPORT\u001b[0m from: ./best_model_restaurant\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "classifier.weight | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "classifier.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avvio Valutazione End-to-End sul Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analisi Frasi: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 583/583 [01:38<00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RISULTATI FINALI EXACT MATCH ACOS (RESTAURANT)\n",
      "==================================================\n",
      "Quadruple Reali totali:    916\n",
      "Quadruple Predette totali: 999\n",
      "Quadruple Esatte:          252\n",
      "--------------------------------------------------\n",
      "Precision: 0.2523\n",
      "Recall:    0.2751\n",
      "F1-Score:  0.2632\n",
      "==================================================\n",
      "Ottimo lavoro! Il modello √® ora bilanciato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CARICAMENTO DEI MODELLI E PREPARATIVI\n",
    "# ==========================================\n",
    "print(\"Caricamento configurazioni...\")\n",
    "with open(\"data_coppie/restaurant_categories.pkl\", \"rb\") as f:\n",
    "    category_list = pickle.load(f)\n",
    "num_categories = len(category_list)\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP', 3: 'B-OPI', 4: 'I-OPI'}\n",
    "\n",
    "print(\"Caricamento Step 1 (L'Investigatore Multi-Task)...\")\n",
    "model_step1 = ModernBertACOS_Extractor(num_labels=5).to(device)\n",
    "model_step1.load_state_dict(torch.load(\"./best_multitask_extractor_restaurant/pytorch_model.bin\", map_location=device, weights_only=True))\n",
    "model_step1.eval()\n",
    "\n",
    "print(\"Caricamento Step 2 (Lo Psicologo Classificatore)...\")\n",
    "model_step2 = ModernBertACOSClassifier(\"./best_model_restaurant\", num_categories).to(device)\n",
    "model_step2.load_state_dict(torch.load(\"./best_classifier_restaurant/pytorch_model.bin\", map_location=device, weights_only=True))\n",
    "model_step2.eval()\n",
    "\n",
    "# ==========================================\n",
    "# 2. FUNZIONI DI SUPPORTO PER L'ESTRAZIONE\n",
    "# ==========================================\n",
    "def get_spans(tags, b_tag, i_tag):\n",
    "    spans = []\n",
    "    start = -1\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == b_tag:\n",
    "            if start != -1: spans.append((start, i))\n",
    "            start = i\n",
    "        elif tag == i_tag and start != -1: continue\n",
    "        else:\n",
    "            if start != -1:\n",
    "                spans.append((start, i))\n",
    "                start = -1\n",
    "    if start != -1: spans.append((start, len(tags)))\n",
    "    return spans\n",
    "\n",
    "def predict_quadruples_e2e(text, model_1, model_2, tokenizer, cat_list):\n",
    "    \"\"\"La vera Pipeline End-to-End su una singola frase.\"\"\"\n",
    "    words = text.split()\n",
    "    inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True, max_length=128, padding='max_length').to(device)\n",
    "    \n",
    "    # --- FASE 1: L'Investigatore ---\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            out1 = model_1(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        \n",
    "    token_preds = torch.argmax(out1['token_logits'], dim=-1)[0].cpu().numpy()\n",
    "    # Al posto di usare argmax, leggiamo le probabilit√†\n",
    "    probs_imp_asp = torch.softmax(out1['imp_asp_logits'], dim=-1)[0]\n",
    "    probs_imp_opi = torch.softmax(out1['imp_opi_logits'], dim=-1)[0]\n",
    "    \n",
    "    # Se il modello √® sicuro almeno al 30% che c'√® un implicito, fidiamoci!\n",
    "    # Questo contrasta la sua \"timidezza\" dovuta all'overfitting\n",
    "    imp_asp = 1 if probs_imp_asp[1].item() > 0.30 else 0\n",
    "    imp_opi = 1 if probs_imp_opi[1].item() > 0.30 else 0\n",
    "    \n",
    "    word_ids = inputs.word_ids()\n",
    "    word_tags = [\"O\"] * len(words)\n",
    "    \n",
    "    for idx, w_id in enumerate(word_ids):\n",
    "        if w_id is not None and w_id < len(words) and word_tags[w_id] == \"O\":\n",
    "            word_tags[w_id] = id2label[token_preds[idx]]\n",
    "                \n",
    "    asp_spans = get_spans(word_tags, \"B-ASP\", \"I-ASP\")\n",
    "    opi_spans = get_spans(word_tags, \"B-OPI\", \"I-OPI\")\n",
    "    \n",
    "    # Se il modello segnala impliciti, aggiungiamo la coordinata speciale\n",
    "    if imp_asp == 1 or len(asp_spans) == 0: asp_spans.append((-1, -1))\n",
    "    if imp_opi == 1 or len(opi_spans) == 0: opi_spans.append((-1, -1))\n",
    "    \n",
    "    asp_spans = list(set(asp_spans))\n",
    "    opi_spans = list(set(opi_spans))\n",
    "    \n",
    "    quadruples = []\n",
    "    \n",
    "    # --- FASE 2: Lo Psicologo ---\n",
    "    for a in asp_spans:\n",
    "        for o in opi_spans:\n",
    "            fixed_a = (a[0]+1, a[1]+1) if a != (-1, -1) else (-1, -1)\n",
    "            fixed_o = (o[0]+1, o[1]+1) if o != (-1, -1) else (-1, -1)\n",
    "            \n",
    "            t_a = torch.tensor([fixed_a]).to(device)\n",
    "            t_o = torch.tensor([fixed_o]).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                    out2 = model_2(input_ids=inputs['input_ids'], \n",
    "                                   attention_mask=inputs['attention_mask'], \n",
    "                                   aspect_spans=t_a, \n",
    "                                   opinion_spans=t_o)\n",
    "            \n",
    "            logits = out2['logits'] if isinstance(out2, dict) else out2 \n",
    "            probs = torch.softmax(logits[0], dim=-1) \n",
    "            \n",
    "            for cat_idx, prob_dist in enumerate(probs):\n",
    "                best_class = torch.argmax(prob_dist).item()\n",
    "                \n",
    "                # --- FILTRO DI SICUREZZA ---\n",
    "                # 1. Se vince la classe 3 (Invalido), scartiamo\n",
    "                if best_class == 3:\n",
    "                    continue\n",
    "                \n",
    "                # 2. Soglia di confidenza: se il sentimento non ha almeno il 50%, scartiamo\n",
    "                if prob_dist[best_class] < 0.5:\n",
    "                    continue\n",
    "                \n",
    "                quadruples.append({\n",
    "                    'aspect_span': a,\n",
    "                    'opinion_span': o,\n",
    "                    'category': cat_list[cat_idx],\n",
    "                    'sentiment': best_class\n",
    "                })\n",
    "                    \n",
    "    return quadruples\n",
    "\n",
    "# ==========================================\n",
    "# 3. TEST SULL'INTERO DATASET RESTAURANT\n",
    "# ==========================================\n",
    "print(\"\\nAvvio Valutazione End-to-End sul Test Set...\")\n",
    "true_quads = []\n",
    "pred_quads = []\n",
    "\n",
    "percorso_file = os.path.join(\"data_parsing\", \"test_rest_parsed.pkl\")\n",
    "test_rest_parsed = pd.read_pickle(percorso_file)\n",
    "\n",
    "for idx, row in tqdm(test_rest_parsed.iterrows(), total=len(test_rest_parsed), desc=\"Analisi Frasi\"):\n",
    "    text = row['review_text']\n",
    "    preds = predict_quadruples_e2e(text, model_step1, model_step2, tokenizer, category_list)\n",
    "    \n",
    "    p_set = set()\n",
    "    for q in preds:\n",
    "        p_set.add((tuple(q['aspect_span']), q['category'], tuple(q['opinion_span']), q['sentiment']))\n",
    "    pred_quads.append(p_set)\n",
    "    \n",
    "    t_set = set()\n",
    "    for q in row['parsed_quadruples']:\n",
    "        a = tuple(q.get('span_A', [-1, -1]))\n",
    "        o = tuple(q.get('span_B', [-1, -1]))\n",
    "        c = q['category_aspect']\n",
    "        s = int(q['sentiment'])\n",
    "        t_set.add((a, c, o, s))\n",
    "    true_quads.append(t_set)\n",
    "\n",
    "# Metriche finali\n",
    "total_pred = sum(len(p) for p in pred_quads)\n",
    "total_true = sum(len(t) for t in true_quads)\n",
    "correct = sum(len(p_set.intersection(t_set)) for p_set, t_set in zip(pred_quads, true_quads))\n",
    "\n",
    "precision = correct / total_pred if total_pred > 0 else 0\n",
    "recall = correct / total_true if total_true > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RISULTATI FINALI EXACT MATCH ACOS (RESTAURANT)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Quadruple Reali totali:    {total_true}\")\n",
    "print(f\"Quadruple Predette totali: {total_pred}\")\n",
    "print(f\"Quadruple Esatte:          {correct}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Confronto con il benchmark del paper\n",
    "if f1 > 0.4461:\n",
    "    print(\"INCREDIBILE! Hai superato il paper originale (0.4461)!\")\n",
    "else:\n",
    "    print(\"Ottimo lavoro! Il modello √® ora bilanciato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d39340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù TESTO: not the biggest portions but adequate .\n",
      "\n",
      "‚úÖ QUADRUPLE VERE (Ground Truth):\n",
      "   Aspetto: (3, 4) | Opinione: (0, 3) | Categoria: FOOD#STYLE_OPTIONS | Sent: 1\n",
      "   Aspetto: (3, 4) | Opinione: (5, 6) | Categoria: FOOD#STYLE_OPTIONS | Sent: 1\n",
      "\n",
      "ü§ñ QUADRUPLE PREDETTE DAL MODELLO:\n",
      "   Aspetto: (-1, -1) | Opinione: (5, 6) | Categoria: RESTAURANT#GENERAL | Sent: 2\n",
      "   Aspetto: (-1, -1) | Opinione: (5, 6) | Categoria: SERVICE#GENERAL | Sent: 0\n"
     ]
    }
   ],
   "source": [
    "# --- DEBUG: GUARDIE E LADRI SULLA PRIMA FRASE ---\n",
    "test_row = test_rest_parsed.iloc[2] \n",
    "test_text = test_row['review_text']\n",
    "\n",
    "print(f\"üìù TESTO: {test_text}\")\n",
    "print(\"\\n‚úÖ QUADRUPLE VERE (Ground Truth):\")\n",
    "for q in test_row['parsed_quadruples']:\n",
    "    print(f\"   Aspetto: {q.get('span_A', [-1,-1])} | Opinione: {q.get('span_B', [-1,-1])} | Categoria: {q['category_aspect']} | Sent: {q['sentiment']}\")\n",
    "\n",
    "print(\"\\nü§ñ QUADRUPLE PREDETTE DAL MODELLO:\")\n",
    "preds = predict_quadruples_e2e(test_text, model_step1, model_step2, tokenizer, category_list)\n",
    "for p in preds:\n",
    "    print(f\"   Aspetto: {p['aspect_span']} | Opinione: {p['opinion_span']} | Categoria: {p['category']} | Sent: {p['sentiment']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
